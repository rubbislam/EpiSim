{"title":"Maximum likelihood estimation and iterated filtering","markdown":{"yaml":{"title":"Maximum likelihood estimation and iterated filtering","author":"Qianying (Ruby) Lin","toc":true,"engine":"knitr"},"headingText":"Summary","containsRefs":false,"markdown":"\n\n\n-   Model implementation in `pomp`\n\n-   Creating a `pomp` object with different componenets\n\n-   Computing likelihood using `pfilter`\n\n## Goal: estimating the parameters\n\nWe are interested in estimating the parameters $\\theta = (\\beta, \\gamma, \\rho, k, S_0, I_0, R_0)$ of the SIR model, given the weekly reported cases. \nThe likelihood-based inference is to find the set of parameters that maximize the likelihood of the data, i.e., the probability of observing the data given the parameters.\nNow we break down this problem into two steps:\n\n- compute the likelihood of the data given the parameters;\n\n- find the set of parameters that maximize the likelihood.\n\nThis session will focus on the second step and we will use the `mif2` function in `pomp` to compute the maximum likelihood estimate (MLE) of the parameters.\n\n## An iterated filtering algorithm (IF2)\n\nSince we already learn how to compute the likelihood of the data given the parameters using Sequential Monte Carlo (SMC) method, implemented in `pfilter` function in `pomp`, now we would like to explore the space of parameters to find the set of parameters that maximize the likelihood.\n\nThe iterated filtering algorithm (IF2) consists of multiple rounds of iterations, call it $M$. \nIn each round of iteration, we have $J$ particles, carrying out with the parameter vector.\nAssume that we have data $y^\\ast_{1:W}$.\n\nWe can consider this procedure as a two-layer process:\n\n1. **Outer loop**: run through $M$ rounds of iterations: \n\n- at the beginning of each round $m$, the particles resulted in the last time step from the previous round $m-1$ are recycled, with a random walk perturbation; \n\n- the particles run through the inner loop to update the weights and the parameters are updated based on the likelihood of the data given the parameters.\n\n2. **Inner loop**: run through $W$ time steps:\n\n- at the beginning of each time step $w$, the particles are sampled from those in the last time step $w-1$, with a random walk perturbation;\n\n- the current state at time $w$ are updated based on the stochastic compartmental model (e.g., SIR model);\n\n- the weights are updated based on the likelihood of the data given the parameters;\n\n- the particles are resampled based on the weights.\n\n![**Iterated filtering diagram**.](if2.png){width=\"500\"}\n\n### Analogy with evolutiion by natural selection\n\n- The parameters characterize the _genotype_.\n\n- The swarm of particles is a _population_.\n\n- The likelihood, a measure of the compatibility between the parameters and the data, is the analogue of _fitness_.\n\n- Each successive observation is a new _generation_.\n\n- Since particles reproduce in each generation in proportion to their likelihood, the particle filter acts like _natural selection_.\n\n- The artificial perturbations augment the \"genetic\" variance and therefore correspond to _mutation_.\n\n- IF2 increases the _fitness_ of the population of particles.\n\n- However, because our scientific interest focuses on the model without the artificial perturbations, we decrease the intensity of the latter with successive iterations.\n\n\n### IF2 pseudocode\n\n**Input**:\n\n- simulators for $f_{X_0}(x_0;\\theta)$ and $f_{X_w\\vert X_{w-1}}(x_w\\vert x_{w-1};\\theta)$;\n\n- evaluator for $f_{Y_w\\vert X_w}(y_w\\vert x_w;\\theta)$;\n\n- data, $y^\\ast_{1:W}$;\n\n**Algorithm parameters and `mif2` arguments**\n\n- number of iterations, `Nmif`=$M$\n\n- number of particles, `Np`=$J$\n\n- initial parameter vector swarm, `params`=$\\left\\{\\Theta_j^0, j=1,\\dots,J\\right\\}$\n\n- random walk standard deviation for each parameter within the vector, `rw.sd`, such that a diagonal variance matrix $V_w$ can be constructed\n\n- cooling fraction every 50 iterations, `cooling.fraction.50` = $a$\n\n**Output**\n\n- final parameter vector swarm after $M$ iterations, $\\left\\{\\Theta_j^M, j=1,\\dots,J\\right\\}$\n\n**Procedure**\n\n```pseudocode\n#| html-indent-size: \"1.2em\"\n#| html-comment-delimiter: \"//\"\n#| html-line-number: true\n#| html-line-number-punc: \":\"\n#| html-no-end: false\n#| pdf-placement: \"htb!\"\n#| pdf-line-number: true\n\n\\begin{algorithm}\n\\caption{Iterated filtering algorithm (IF2)}\n\\begin{algorithmic}\n\\For{$m=1$ to $M$}\n  \\State $\\Theta_{0,j}^{F,m} \\sim \\mathrm{Normal}(\\Theta_j^{m-1}, V_0\\,a^{2m/50})$ for $j=1$ to $J$\n  \\State $X_{0,j}^{F,m} \\sim f_{X_0}(x_0;\\Theta_{0,j}^{F,m})$ for $j=1$ to $J$\n  \\For{$w=1$ to $W$}\n    \\State $\\Theta_{w,j}^{P,m} \\sim \\mathrm{Normal}(\\Theta_{w-1,j}^{F,m}, V_n\\, a^{2m/50})$ for $j=1$ to $J$\n    \\State $X_{n,j}^{P,m} \\sim f_{X_w\\vert X_{w-1}}(x_w\\vert X_{w-1}^{F,m};\\Theta_{w,j}^{P,m})$ for $j=1$ to $J$\n    \\State $\\omega_{n,j}^{P,m} = f_{Y_w\\vert X_w}(y_w^\\ast\\vert X_{w,j}^{P,m};\\Theta_{w,j}^{P,m})$ for $j=1$ to $J$\n    \\State Draw $k_{1:J}$ with $P[k_j=i]=\\omega_{w,j}^m/\\sum_{u=1}^J\\,\\omega_{w,u}$\n    \\State $\\Theta_{w,j}^{F,m} = \\Theta_{w,k_j}^{P,m}$ and $X_{w,j}^{F,m}=X_{w,k_j}^{P,m}$ for $j=1$ to $J$\n  \\EndFor\n  \\State Set $\\Theta_j^m = \\Theta_{W,j}^{F,m}$ for $j=1$ to $J$\n\\EndFor\n\\end{algorithmic}\n\\end{algorithm}\n```\n\n### Particle filter in `pomp` for the Consett measles outbreak\n\n- Recap one wave of the Consett measles outbreak:\n\n```{r recap-data}\nsource(\"model.R\")\ndat_meas |>\n  ggplot(aes(x=week, y=reports)) +\n  geom_line() +\n  geom_point() +\n  theme_bw()\n```\n\n- Test the particle filter with pre-specified parameters:\n\n```{r test-pfilter}\n#| cache: true\nsird_measle |>\n  pfilter(\n    params =  c(\n      Beta = 15, gamma = .5, rho = .5, k = 10,\n      S0 = 2280, I0 = 1, R0 = 35720\n    ),\n    Np=1000\n  ) -> pf\n\nplot(pf)\n```\n\nThe above plot shows the data (`reports`), along with the _effective sample size_ (ESS) of the particle filter (`ess`) and the log-likelihood of each observation conditional on the preceding ones (`cond.logLik`).\nThe ESS is the equivalent number of independent particles. \nIn this case, the ESS appears to be everywhere adequate.\n\n- setting up the estimation problem\n\nAssume the initial population sizes are known, that is $S_0=2280, I_0 = 1, R_0 = 35720$.\nWe also assume that the infectious period is 2 weeks, i.e., $\\gamma = 0.5$.\nWe would also like to fix the over-dispersion parameter $k=10$.\nWe can relax these assumptions later.\nBy fixing these parameters, we can constrain the parameter space and try our best to avoid un-identifiability.\nWe can then proceed to estimate the transmission rate $\\beta$ and the reporting ratio $\\rho$, with initial guesses $\\beta=15$ and $\\rho=0.5$.\n\n```{r pf}\n#| cache: true\nfixed_params <- c(\n  S0 = 2280, I0 = 1, R0 = 35720,\n  gamma = .5, k = 10\n)\n\ncoef(sird_measle, names(fixed_params)) <- fixed_params\n\nlibrary(foreach)\nlibrary(doParallel)\nregisterDoParallel(cores=detectCores()-1)\nforeach(i=1:10,.combine=c, .options.future=list(seed=TRUE)) %dopar% {\n  sird_measle |> pfilter(Np=5000)\n} -> pf\npf |> logLik() |> logmeanexp(se=TRUE) -> L_pf\nL_pf\n```\n```{r store-results1}\n#| cache: true\npf[[1]] |> coef() |> bind_rows() |>\n  bind_cols(loglik=L_pf[1],loglik.se=L_pf[2]) |>\n  write_csv(\"measles_params.csv\")\n```\n\n\n## Local search of the likelihood surface\n\nA \"local search\" means that we are looking for the maximum likelihood estimate (MLE) starting from ONE initial guess. \nAfterwards, by applying \"local search\" to a number of different initial guesses covering the reasonable parameter space, we can explore the likelihood surface and find the global MLE.\n\nA few parameters for iterated filtering need to be specified:\n\n- the random walk standard deviation, `rw.sd` for each parameter to be estimated\n\n- the cooling fraction every 50 iterations, `cooling.fraction.50`\n\n- the transmission rate $\\beta$ will estimated in log scale (since $\\beta > 0$) and the reporting ratio $\\rho$ in logistic scale (since $0 < \\rho < 1$)\n\n- the perturbation for $\\beta$ and $\\rho$ are both $0.02$ (a very common and efficient choice from experience)\n\n- fix `cooling.fraction.50=0.5`, such that after 50 `mif2` iterations, the perturbation are reduced to half their original magnitudes\n\n\nNow we can repeat the `mif2` algorithm for 20 times with the same initial guesses:\n\n```{r local-mif2}\n#| cache: true\nforeach(i=1:20,.combine=c) %dopar% {\n  sird_measle |>\n    mif2(\n      Np=2000, Nmif=50,\n      cooling.fraction.50=0.5,\n      rw.sd=rw_sd(Beta=0.02, rho=0.02),\n      partrans=parameter_trans(log=\"Beta\",logit=\"rho\"),\n      paramnames=c(\"Beta\",\"rho\")\n    )\n} -> mifs_local\n```\n\n### Iterated filtering diagnostics\n\n```{r mif2-diagnostics}\nmifs_local |>\n  traces() |>\n  melt() |>\n  ggplot(aes(x=iteration,y=value,group=.L1,color=factor(.L1)))+\n    geom_line() +\n    guides(color=\"none\") +\n    facet_wrap(~name,scales=\"free_y\")\n```\n\nFrom the traces of the parameters, we can see that the iterated filtering algorithm has converged to a stable region after about 10 iterations.\nWe can conclude that:\n\n- our initial guesses are reasonable;\n\n- this Monte Carlo algorithm is stochastic and the results may vary from run to run.\n\n### Estimating the likelihood\n\nThe diagnostic plot shows that the 20 replications of the iterated filtering algorithm have converged to a stable region after 50 iterations, and now we can compute the likelihood of the data given these 20 converged parameter vectors.\n\n```{r mif2-likelihood}\nbake(\"results_local.rds\", {\n  foreach(mf=mifs_local,.combine=rbind) %dopar% {\n    evals <- replicate(10, logLik(pfilter(mf,Np=5000)))\n    ll <- logmeanexp(evals,se=TRUE)\n    mf |> coef() |> bind_rows() |>\n    bind_cols(loglik=ll[1],loglik.se=ll[2])\n  } -> results_local\n}) -> results_local\n\npairs(~loglik+Beta+rho,data=results_local,pch=16)\n```\n\nThe scatter plot shows a hint of a \"ridge\" in the likelihood surface. \nHowever, the sampling is as yet too sparse to give a clear picture.\nThere are things that we can try next:\n\n- Increase the number of iterations, `Nmif`, to confirm the convergence\n\n- Increase the number of particles, `Np`, to improve the accuracy of the estimates\n\n- Try different initial guesses to explore the likelihood surface\n\n```{r store-results2}\n#| cache: true\nread_csv(\"measles_params.csv\") |>\n  bind_rows(results_local) |>\n  arrange(-loglik) |>\n  write_csv(\"measles_params.csv\")\n```\n\n## Global Search and MLE\n\nTo avoid confusion, we have the following terminology:\n\n- initial guess: the initial parameter vector $\\Theta^0_j$\n\n- initial state: the initial state of population model $X_{0,j}$ at time $t_0$\n\nFor our measles model, a box containing reasonable parameter values might be $\\beta\\in(5,80)$ and $\\rho\\in(0.2,0.9)$.\nNow we can build a group of 400 initial guesses to cover this box:\n```{r initial-guess-group}\n#| cache: true\nset.seed(2062379496)\nrunif_design(\n  lower=c(Beta=5,rho=0.2),\n  upper=c(Beta=80,rho=0.9),\n  nseq=400\n) -> guesses\n\nmf1 <- mifs_local[[1]]  # retrieve the local search as the model template\n```\n\nWith the group of 400 initial guesses, we can now apply the iterated filtering algorithm to find the global MLE:\n\n- the codes run ONE local search for each of 400 initial guesses\n\n- each local search consists of two runs: the first run with 50 iterations, followed by the second with 100 iterations\n\n- a general `pomp` behavior:\n\n    - re-running a command on an object (i.e., `mif2` on `mf1`) created by the same command perserves the algorithmic arguments\n    \n    - running `mif2` on the results of a `mif2` computation re-runs IF2 from the endpoint of the first run\n    \n    - the second/subsequent run will preserve all the algorithmic parameters from the previous run by default; here we overrode the default choice of `Nmif` from 50 to 100\n    \n- following the local search, the particle filter is used to evaluate the likelihood as the previous session\n\n\n```{r global-mif2}\nbake(\"results_global.rds\", {\n  foreach(guess=iter(guesses,\"row\"), .combine=rbind) %dopar% {\n    # run the iterated filtering algorithm\n    mf1 |>\n      mif2(params=c(guess,fixed_params)) |>\n      mif2(Nmif=100) -> mf\n    # compute the likelihood and se from 10 replications\n    replicate(\n      10,\n      mf |> pfilter(Np=5000) |> logLik()\n    ) |>\n    logmeanexp(se=TRUE) -> ll\n    # store the results\n    mf |> coef() |> bind_rows() |>\n      bind_cols(loglik=ll[1],loglik.se=ll[2])\n  } -> results_global\n}) -> results_global\n\nresults_global |> filter(loglik == max(loglik))\n```\n\nThe best result of this search had a likelihood of -105.704 with a standard error of ~0.027. Now we can update the database:\n\n```{r store-results3}\nread_csv(\"measles_params.csv\") |>\n  bind_rows(results_global) |>\n  arrange(-loglik) |>\n  write_csv(\"measles_params.csv\")\n```\n\nWe attempt to visualize the global geometry of the likelihood surface using a scatterplot matrix. In particular, here we plot both the initial guesses (grey) and the IF2 estimates (red).\n\n```{r view-results_global}\nread_csv(\"measles_params.csv\") |>\n  filter(loglik>max(loglik)-50) |>\n  bind_rows(guesses) |>\n  mutate(type=if_else(is.na(loglik),\"guess\",\"result\")) |>\n  arrange(type) -> all\n  \npairs(~loglik+Beta+rho, data=all, pch=16, cex=0.3, col=ifelse(all$type==\"guess\",grey(.5),\"red\"))\n```\n### a poo rman's profile\n\n```{r poorman-profile}\nall |>\n  filter(type==\"result\") |>\n  filter(loglik>max(loglik)-10) |>\n  ggplot(aes(x=Beta,y=loglik))+\n  geom_point()+\n  labs(\n    x=expression(Beta),\n    title=\"poor man’s profile likelihood\"\n  )\n```\n","srcMarkdownNoYaml":"\n\n## Summary\n\n-   Model implementation in `pomp`\n\n-   Creating a `pomp` object with different componenets\n\n-   Computing likelihood using `pfilter`\n\n## Goal: estimating the parameters\n\nWe are interested in estimating the parameters $\\theta = (\\beta, \\gamma, \\rho, k, S_0, I_0, R_0)$ of the SIR model, given the weekly reported cases. \nThe likelihood-based inference is to find the set of parameters that maximize the likelihood of the data, i.e., the probability of observing the data given the parameters.\nNow we break down this problem into two steps:\n\n- compute the likelihood of the data given the parameters;\n\n- find the set of parameters that maximize the likelihood.\n\nThis session will focus on the second step and we will use the `mif2` function in `pomp` to compute the maximum likelihood estimate (MLE) of the parameters.\n\n## An iterated filtering algorithm (IF2)\n\nSince we already learn how to compute the likelihood of the data given the parameters using Sequential Monte Carlo (SMC) method, implemented in `pfilter` function in `pomp`, now we would like to explore the space of parameters to find the set of parameters that maximize the likelihood.\n\nThe iterated filtering algorithm (IF2) consists of multiple rounds of iterations, call it $M$. \nIn each round of iteration, we have $J$ particles, carrying out with the parameter vector.\nAssume that we have data $y^\\ast_{1:W}$.\n\nWe can consider this procedure as a two-layer process:\n\n1. **Outer loop**: run through $M$ rounds of iterations: \n\n- at the beginning of each round $m$, the particles resulted in the last time step from the previous round $m-1$ are recycled, with a random walk perturbation; \n\n- the particles run through the inner loop to update the weights and the parameters are updated based on the likelihood of the data given the parameters.\n\n2. **Inner loop**: run through $W$ time steps:\n\n- at the beginning of each time step $w$, the particles are sampled from those in the last time step $w-1$, with a random walk perturbation;\n\n- the current state at time $w$ are updated based on the stochastic compartmental model (e.g., SIR model);\n\n- the weights are updated based on the likelihood of the data given the parameters;\n\n- the particles are resampled based on the weights.\n\n![**Iterated filtering diagram**.](if2.png){width=\"500\"}\n\n### Analogy with evolutiion by natural selection\n\n- The parameters characterize the _genotype_.\n\n- The swarm of particles is a _population_.\n\n- The likelihood, a measure of the compatibility between the parameters and the data, is the analogue of _fitness_.\n\n- Each successive observation is a new _generation_.\n\n- Since particles reproduce in each generation in proportion to their likelihood, the particle filter acts like _natural selection_.\n\n- The artificial perturbations augment the \"genetic\" variance and therefore correspond to _mutation_.\n\n- IF2 increases the _fitness_ of the population of particles.\n\n- However, because our scientific interest focuses on the model without the artificial perturbations, we decrease the intensity of the latter with successive iterations.\n\n\n### IF2 pseudocode\n\n**Input**:\n\n- simulators for $f_{X_0}(x_0;\\theta)$ and $f_{X_w\\vert X_{w-1}}(x_w\\vert x_{w-1};\\theta)$;\n\n- evaluator for $f_{Y_w\\vert X_w}(y_w\\vert x_w;\\theta)$;\n\n- data, $y^\\ast_{1:W}$;\n\n**Algorithm parameters and `mif2` arguments**\n\n- number of iterations, `Nmif`=$M$\n\n- number of particles, `Np`=$J$\n\n- initial parameter vector swarm, `params`=$\\left\\{\\Theta_j^0, j=1,\\dots,J\\right\\}$\n\n- random walk standard deviation for each parameter within the vector, `rw.sd`, such that a diagonal variance matrix $V_w$ can be constructed\n\n- cooling fraction every 50 iterations, `cooling.fraction.50` = $a$\n\n**Output**\n\n- final parameter vector swarm after $M$ iterations, $\\left\\{\\Theta_j^M, j=1,\\dots,J\\right\\}$\n\n**Procedure**\n\n```pseudocode\n#| html-indent-size: \"1.2em\"\n#| html-comment-delimiter: \"//\"\n#| html-line-number: true\n#| html-line-number-punc: \":\"\n#| html-no-end: false\n#| pdf-placement: \"htb!\"\n#| pdf-line-number: true\n\n\\begin{algorithm}\n\\caption{Iterated filtering algorithm (IF2)}\n\\begin{algorithmic}\n\\For{$m=1$ to $M$}\n  \\State $\\Theta_{0,j}^{F,m} \\sim \\mathrm{Normal}(\\Theta_j^{m-1}, V_0\\,a^{2m/50})$ for $j=1$ to $J$\n  \\State $X_{0,j}^{F,m} \\sim f_{X_0}(x_0;\\Theta_{0,j}^{F,m})$ for $j=1$ to $J$\n  \\For{$w=1$ to $W$}\n    \\State $\\Theta_{w,j}^{P,m} \\sim \\mathrm{Normal}(\\Theta_{w-1,j}^{F,m}, V_n\\, a^{2m/50})$ for $j=1$ to $J$\n    \\State $X_{n,j}^{P,m} \\sim f_{X_w\\vert X_{w-1}}(x_w\\vert X_{w-1}^{F,m};\\Theta_{w,j}^{P,m})$ for $j=1$ to $J$\n    \\State $\\omega_{n,j}^{P,m} = f_{Y_w\\vert X_w}(y_w^\\ast\\vert X_{w,j}^{P,m};\\Theta_{w,j}^{P,m})$ for $j=1$ to $J$\n    \\State Draw $k_{1:J}$ with $P[k_j=i]=\\omega_{w,j}^m/\\sum_{u=1}^J\\,\\omega_{w,u}$\n    \\State $\\Theta_{w,j}^{F,m} = \\Theta_{w,k_j}^{P,m}$ and $X_{w,j}^{F,m}=X_{w,k_j}^{P,m}$ for $j=1$ to $J$\n  \\EndFor\n  \\State Set $\\Theta_j^m = \\Theta_{W,j}^{F,m}$ for $j=1$ to $J$\n\\EndFor\n\\end{algorithmic}\n\\end{algorithm}\n```\n\n### Particle filter in `pomp` for the Consett measles outbreak\n\n- Recap one wave of the Consett measles outbreak:\n\n```{r recap-data}\nsource(\"model.R\")\ndat_meas |>\n  ggplot(aes(x=week, y=reports)) +\n  geom_line() +\n  geom_point() +\n  theme_bw()\n```\n\n- Test the particle filter with pre-specified parameters:\n\n```{r test-pfilter}\n#| cache: true\nsird_measle |>\n  pfilter(\n    params =  c(\n      Beta = 15, gamma = .5, rho = .5, k = 10,\n      S0 = 2280, I0 = 1, R0 = 35720\n    ),\n    Np=1000\n  ) -> pf\n\nplot(pf)\n```\n\nThe above plot shows the data (`reports`), along with the _effective sample size_ (ESS) of the particle filter (`ess`) and the log-likelihood of each observation conditional on the preceding ones (`cond.logLik`).\nThe ESS is the equivalent number of independent particles. \nIn this case, the ESS appears to be everywhere adequate.\n\n- setting up the estimation problem\n\nAssume the initial population sizes are known, that is $S_0=2280, I_0 = 1, R_0 = 35720$.\nWe also assume that the infectious period is 2 weeks, i.e., $\\gamma = 0.5$.\nWe would also like to fix the over-dispersion parameter $k=10$.\nWe can relax these assumptions later.\nBy fixing these parameters, we can constrain the parameter space and try our best to avoid un-identifiability.\nWe can then proceed to estimate the transmission rate $\\beta$ and the reporting ratio $\\rho$, with initial guesses $\\beta=15$ and $\\rho=0.5$.\n\n```{r pf}\n#| cache: true\nfixed_params <- c(\n  S0 = 2280, I0 = 1, R0 = 35720,\n  gamma = .5, k = 10\n)\n\ncoef(sird_measle, names(fixed_params)) <- fixed_params\n\nlibrary(foreach)\nlibrary(doParallel)\nregisterDoParallel(cores=detectCores()-1)\nforeach(i=1:10,.combine=c, .options.future=list(seed=TRUE)) %dopar% {\n  sird_measle |> pfilter(Np=5000)\n} -> pf\npf |> logLik() |> logmeanexp(se=TRUE) -> L_pf\nL_pf\n```\n```{r store-results1}\n#| cache: true\npf[[1]] |> coef() |> bind_rows() |>\n  bind_cols(loglik=L_pf[1],loglik.se=L_pf[2]) |>\n  write_csv(\"measles_params.csv\")\n```\n\n\n## Local search of the likelihood surface\n\nA \"local search\" means that we are looking for the maximum likelihood estimate (MLE) starting from ONE initial guess. \nAfterwards, by applying \"local search\" to a number of different initial guesses covering the reasonable parameter space, we can explore the likelihood surface and find the global MLE.\n\nA few parameters for iterated filtering need to be specified:\n\n- the random walk standard deviation, `rw.sd` for each parameter to be estimated\n\n- the cooling fraction every 50 iterations, `cooling.fraction.50`\n\n- the transmission rate $\\beta$ will estimated in log scale (since $\\beta > 0$) and the reporting ratio $\\rho$ in logistic scale (since $0 < \\rho < 1$)\n\n- the perturbation for $\\beta$ and $\\rho$ are both $0.02$ (a very common and efficient choice from experience)\n\n- fix `cooling.fraction.50=0.5`, such that after 50 `mif2` iterations, the perturbation are reduced to half their original magnitudes\n\n\nNow we can repeat the `mif2` algorithm for 20 times with the same initial guesses:\n\n```{r local-mif2}\n#| cache: true\nforeach(i=1:20,.combine=c) %dopar% {\n  sird_measle |>\n    mif2(\n      Np=2000, Nmif=50,\n      cooling.fraction.50=0.5,\n      rw.sd=rw_sd(Beta=0.02, rho=0.02),\n      partrans=parameter_trans(log=\"Beta\",logit=\"rho\"),\n      paramnames=c(\"Beta\",\"rho\")\n    )\n} -> mifs_local\n```\n\n### Iterated filtering diagnostics\n\n```{r mif2-diagnostics}\nmifs_local |>\n  traces() |>\n  melt() |>\n  ggplot(aes(x=iteration,y=value,group=.L1,color=factor(.L1)))+\n    geom_line() +\n    guides(color=\"none\") +\n    facet_wrap(~name,scales=\"free_y\")\n```\n\nFrom the traces of the parameters, we can see that the iterated filtering algorithm has converged to a stable region after about 10 iterations.\nWe can conclude that:\n\n- our initial guesses are reasonable;\n\n- this Monte Carlo algorithm is stochastic and the results may vary from run to run.\n\n### Estimating the likelihood\n\nThe diagnostic plot shows that the 20 replications of the iterated filtering algorithm have converged to a stable region after 50 iterations, and now we can compute the likelihood of the data given these 20 converged parameter vectors.\n\n```{r mif2-likelihood}\nbake(\"results_local.rds\", {\n  foreach(mf=mifs_local,.combine=rbind) %dopar% {\n    evals <- replicate(10, logLik(pfilter(mf,Np=5000)))\n    ll <- logmeanexp(evals,se=TRUE)\n    mf |> coef() |> bind_rows() |>\n    bind_cols(loglik=ll[1],loglik.se=ll[2])\n  } -> results_local\n}) -> results_local\n\npairs(~loglik+Beta+rho,data=results_local,pch=16)\n```\n\nThe scatter plot shows a hint of a \"ridge\" in the likelihood surface. \nHowever, the sampling is as yet too sparse to give a clear picture.\nThere are things that we can try next:\n\n- Increase the number of iterations, `Nmif`, to confirm the convergence\n\n- Increase the number of particles, `Np`, to improve the accuracy of the estimates\n\n- Try different initial guesses to explore the likelihood surface\n\n```{r store-results2}\n#| cache: true\nread_csv(\"measles_params.csv\") |>\n  bind_rows(results_local) |>\n  arrange(-loglik) |>\n  write_csv(\"measles_params.csv\")\n```\n\n## Global Search and MLE\n\nTo avoid confusion, we have the following terminology:\n\n- initial guess: the initial parameter vector $\\Theta^0_j$\n\n- initial state: the initial state of population model $X_{0,j}$ at time $t_0$\n\nFor our measles model, a box containing reasonable parameter values might be $\\beta\\in(5,80)$ and $\\rho\\in(0.2,0.9)$.\nNow we can build a group of 400 initial guesses to cover this box:\n```{r initial-guess-group}\n#| cache: true\nset.seed(2062379496)\nrunif_design(\n  lower=c(Beta=5,rho=0.2),\n  upper=c(Beta=80,rho=0.9),\n  nseq=400\n) -> guesses\n\nmf1 <- mifs_local[[1]]  # retrieve the local search as the model template\n```\n\nWith the group of 400 initial guesses, we can now apply the iterated filtering algorithm to find the global MLE:\n\n- the codes run ONE local search for each of 400 initial guesses\n\n- each local search consists of two runs: the first run with 50 iterations, followed by the second with 100 iterations\n\n- a general `pomp` behavior:\n\n    - re-running a command on an object (i.e., `mif2` on `mf1`) created by the same command perserves the algorithmic arguments\n    \n    - running `mif2` on the results of a `mif2` computation re-runs IF2 from the endpoint of the first run\n    \n    - the second/subsequent run will preserve all the algorithmic parameters from the previous run by default; here we overrode the default choice of `Nmif` from 50 to 100\n    \n- following the local search, the particle filter is used to evaluate the likelihood as the previous session\n\n\n```{r global-mif2}\nbake(\"results_global.rds\", {\n  foreach(guess=iter(guesses,\"row\"), .combine=rbind) %dopar% {\n    # run the iterated filtering algorithm\n    mf1 |>\n      mif2(params=c(guess,fixed_params)) |>\n      mif2(Nmif=100) -> mf\n    # compute the likelihood and se from 10 replications\n    replicate(\n      10,\n      mf |> pfilter(Np=5000) |> logLik()\n    ) |>\n    logmeanexp(se=TRUE) -> ll\n    # store the results\n    mf |> coef() |> bind_rows() |>\n      bind_cols(loglik=ll[1],loglik.se=ll[2])\n  } -> results_global\n}) -> results_global\n\nresults_global |> filter(loglik == max(loglik))\n```\n\nThe best result of this search had a likelihood of -105.704 with a standard error of ~0.027. Now we can update the database:\n\n```{r store-results3}\nread_csv(\"measles_params.csv\") |>\n  bind_rows(results_global) |>\n  arrange(-loglik) |>\n  write_csv(\"measles_params.csv\")\n```\n\nWe attempt to visualize the global geometry of the likelihood surface using a scatterplot matrix. In particular, here we plot both the initial guesses (grey) and the IF2 estimates (red).\n\n```{r view-results_global}\nread_csv(\"measles_params.csv\") |>\n  filter(loglik>max(loglik)-50) |>\n  bind_rows(guesses) |>\n  mutate(type=if_else(is.na(loglik),\"guess\",\"result\")) |>\n  arrange(type) -> all\n  \npairs(~loglik+Beta+rho, data=all, pch=16, cex=0.3, col=ifelse(all$type==\"guess\",grey(.5),\"red\"))\n```\n### a poo rman's profile\n\n```{r poorman-profile}\nall |>\n  filter(type==\"result\") |>\n  filter(loglik>max(loglik)-10) |>\n  ggplot(aes(x=Beta,y=loglik))+\n  geom_point()+\n  labs(\n    x=expression(Beta),\n    title=\"poor man’s profile likelihood\"\n  )\n```\n"},"formats":{"html":{"identifier":{"display-name":"HTML","target-format":"html","base-format":"html"},"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":false,"echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"ipynb-shell-interactivity":null,"plotly-connected":true,"engine":"knitr"},"render":{"keep-tex":false,"keep-typ":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":"none","code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"inline-includes":false,"preserve-yaml":false,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-min-runs":1,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[],"notebook-links":true},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","filters":["pseudocode"],"css":["styles.css"],"toc":true,"include-in-header":{"text":"<script>\nMathJax = {\n  loader: {\n    load: ['[tex]/boldsymbol']\n  },\n  tex: {\n    tags: \"all\",\n    inlineMath: [['$','$'], ['\\\\(','\\\\)']],\n    displayMath: [['$$','$$'], ['\\\\[','\\\\]']],\n    processEscapes: true,\n    processEnvironments: true,\n    packages: {\n      '[+]': ['boldsymbol']\n    }\n  }\n};\n</script>\n<script src=\"https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js\" type=\"text/javascript\"></script>\n"},"output-file":"iterated.html"},"language":{"toc-title-document":"Table of contents","toc-title-website":"On this page","related-formats-title":"Other Formats","related-notebooks-title":"Notebooks","source-notebooks-prefix":"Source","other-links-title":"Other Links","code-links-title":"Code Links","launch-dev-container-title":"Launch Dev Container","launch-binder-title":"Launch Binder","article-notebook-label":"Article Notebook","notebook-preview-download":"Download Notebook","notebook-preview-download-src":"Download Source","notebook-preview-back":"Back to Article","manuscript-meca-bundle":"MECA Bundle","section-title-abstract":"Abstract","section-title-appendices":"Appendices","section-title-footnotes":"Footnotes","section-title-references":"References","section-title-reuse":"Reuse","section-title-copyright":"Copyright","section-title-citation":"Citation","appendix-attribution-cite-as":"For attribution, please cite this work as:","appendix-attribution-bibtex":"BibTeX citation:","appendix-view-license":"View License","title-block-author-single":"Author","title-block-author-plural":"Authors","title-block-affiliation-single":"Affiliation","title-block-affiliation-plural":"Affiliations","title-block-published":"Published","title-block-modified":"Modified","title-block-keywords":"Keywords","callout-tip-title":"Tip","callout-note-title":"Note","callout-warning-title":"Warning","callout-important-title":"Important","callout-caution-title":"Caution","code-summary":"Code","code-tools-menu-caption":"Code","code-tools-show-all-code":"Show All Code","code-tools-hide-all-code":"Hide All Code","code-tools-view-source":"View Source","code-tools-source-code":"Source Code","tools-share":"Share","tools-download":"Download","code-line":"Line","code-lines":"Lines","copy-button-tooltip":"Copy to Clipboard","copy-button-tooltip-success":"Copied!","repo-action-links-edit":"Edit this page","repo-action-links-source":"View source","repo-action-links-issue":"Report an issue","back-to-top":"Back to top","search-no-results-text":"No results","search-matching-documents-text":"matching documents","search-copy-link-title":"Copy link to search","search-hide-matches-text":"Hide additional matches","search-more-match-text":"more match in this document","search-more-matches-text":"more matches in this document","search-clear-button-title":"Clear","search-text-placeholder":"","search-detached-cancel-button-title":"Cancel","search-submit-button-title":"Submit","search-label":"Search","toggle-section":"Toggle section","toggle-sidebar":"Toggle sidebar navigation","toggle-dark-mode":"Toggle dark mode","toggle-reader-mode":"Toggle reader mode","toggle-navigation":"Toggle navigation","crossref-fig-title":"Figure","crossref-tbl-title":"Table","crossref-lst-title":"Listing","crossref-thm-title":"Theorem","crossref-lem-title":"Lemma","crossref-cor-title":"Corollary","crossref-prp-title":"Proposition","crossref-cnj-title":"Conjecture","crossref-def-title":"Definition","crossref-exm-title":"Example","crossref-exr-title":"Exercise","crossref-ch-prefix":"Chapter","crossref-apx-prefix":"Appendix","crossref-sec-prefix":"Section","crossref-eq-prefix":"Equation","crossref-lof-title":"List of Figures","crossref-lot-title":"List of Tables","crossref-lol-title":"List of Listings","environment-proof-title":"Proof","environment-remark-title":"Remark","environment-solution-title":"Solution","listing-page-order-by":"Order By","listing-page-order-by-default":"Default","listing-page-order-by-date-asc":"Oldest","listing-page-order-by-date-desc":"Newest","listing-page-order-by-number-desc":"High to Low","listing-page-order-by-number-asc":"Low to High","listing-page-field-date":"Date","listing-page-field-title":"Title","listing-page-field-description":"Description","listing-page-field-author":"Author","listing-page-field-filename":"File Name","listing-page-field-filemodified":"Modified","listing-page-field-subtitle":"Subtitle","listing-page-field-readingtime":"Reading Time","listing-page-field-wordcount":"Word Count","listing-page-field-categories":"Categories","listing-page-minutes-compact":"{0} min","listing-page-category-all":"All","listing-page-no-matches":"No matching items","listing-page-words":"{0} words","listing-page-filter":"Filter","draft":"Draft"},"metadata":{"lang":"en","fig-responsive":true,"quarto-version":"1.5.45","editor":"visual","pseudocode":{"caption-prefix":"Algorithm","reference-prefix":"Algorithm","caption-number":true},"theme":"cosmo","title":"Maximum likelihood estimation and iterated filtering","author":"Qianying (Ruby) Lin"},"extensions":{"book":{"multiFile":true}}}},"projectFormats":["html"]}