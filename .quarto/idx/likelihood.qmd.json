{"title":"Likelihood","markdown":{"yaml":{"title":"Likelihood","author":"Qianying (Ruby) Lin","toc":true,"format":{"html":{"embed-resources":true,"code-background":true}},"engine":"knitr"},"headingText":"Summary","containsRefs":false,"markdown":"\n\n\n-   Implementing in `pomp`\n\n-   Creating a `pomp` object\n\n-   Computing likelihood\n\n```{r loading}\noptions(warning = FALSE)\nlibrary(dplyr)\nlibrary(tidyr)\nlibrary(readr)\nlibrary(iterators)\nlibrary(pomp)\nlibrary(ggplot2)\nlibrary(foreach)\nlibrary(doParallel)\n```\n\n## Recap: A stochastic SIR model in the POMP framework\n\nWe define the **state process** as a stochastic SIR model, $X_t=(S_t, I_t, R_t, D_t)$ where $D_t$ is the weekly cumulative diagnoses, getting reset to 0 every 7 days. The number of infections $\\Delta N_{SI}$ and recoveries $\\Delta N_{IR}$ are random variables, following Binomial distributions:\n\n```{=tex}\n\\begin{aligned}\n  \\Delta N_{SI} &\\sim \\mathrm{Binomial}\\left(S, 1-e^{-\\beta\\frac{I}{N}\\Delta t}\\right) \\\\\n  \\Delta N_{IR} &\\sim \\mathrm{Binomial}\\left(I, 1-e^{-\\gamma \\Delta t}\\right)\n\\end{aligned}\n```\nwhere $\\beta$ is the transmission rate and $\\gamma$ is the recovery rate.\n\nWe define a weekly **measurement model** which follows a Negative Binomial distribution with mean $\\rho D_t$ and the dispersion parameter $k$:\n\n$$\n\\textrm{reports}_t \\sim \\textrm{NegBin}(\\rho D_t, k),\n$$\n\nwhere $\\rho$ is the reporting ratio and $0 \\leq \\rho \\leq 1$ .\n\n## Implementing the SIR model in `pomp` package\n\n### Example: the Consett measles outbreak\n\nWe first download the data and take a look at them:\n\n```{r dat_measle}\nread_csv(paste0(\"https://kingaa.github.io/sbied/stochsim/\", \n  \"Measles_Consett_1948.csv\")) |>\n  select(week,reports=cases) -> dat_meas\n\ndat_meas |> head()\n\ndat_meas |> summary()\n```\n\nand visualize:\n\n```{r dat_measle_viz}\ndat_meas |>\n  ggplot(aes(x=week, y=reports)) +\n  geom_line() +\n  geom_point() +\n  theme_bw()\n```\n\n### A stochastic SIR model for measles in `pomp`\n\nTo accelerate the computation, we would like to code the model in `C/C++` using `C snippets` in `pomp`. Now we can have our \\*\\*one-step transition\\*\\* function:\n\n```{r sird_step_c}\nsird_step <- Csnippet(\"\n  double N = S + I + R;\n  double dN_SI = rbinom(S, 1-exp(-Beta*I/N*dt));\n  double dN_IR = rbinom(I, 1-exp(-gamma*dt));\n  S -= dN_SI;\n  I += dN_SI - dN_IR;\n  R += dN_IR;\n  D += dN_IR;\n\")\n```\n\nand the initialization and measurement model:\n\n```{r sird_init_meas_c}\nsird_rinit <- Csnippet(\"\n  S = S0;\n  I = I0;\n  R = R0;\n  D = 0;\n\")\n\nsird_dmeas <- Csnippet(\"\n  lik = dnbinom_mu(reports, k, rho*D, give_log);\n\")\n\nsird_rmeas <- Csnippet(\"\n  reports = rnbinom_mu(k, rho*D);\n\")\n```\n\nNow we can put the model together in `pomp` and simulate the model dynamics:\n\n```{r sird_pomp}\ndat_meas |>\n  pomp(\n    times=\"week\",\n    t0=0,\n    rprocess = euler(sird_step, delta.t = 1/7),\n    rinit = sird_rinit,\n    rmeasure = sird_rmeas,\n    dmeasure = sird_dmeas,\n    accumvars = \"D\",\n    statenames = c(\"S\",\"I\",\"R\",\"D\"),\n    paramnames = c(\"Beta\",\"gamma\",\"rho\",\"k\",\"S0\",\"I0\",\"R0\")\n  ) -> sird_measle\n\n```\n\nAfter getting the components in place in `pomp`, we can simulate the dynamics with some parameters from literature or anything you want:\n\n```{r sird_pomp_sim}\nsird_measle |>\n  simulate(\n    params = c(\n      Beta = 7.5, gamma = .5, rho = .5, k = 10,\n      S0 = 1140, I0 = 1, R0 = 36860\n    ),\n    nsim = 100,\n    format = \"data.frame\",\n    include.data = TRUE\n  ) -> sims_measle\n\nsims_measle |> head()\n\nsummary(sims_measle)\n\nsims_measle |>\n  ggplot(aes(x=week, y=reports, group=.id,color=(.id==\"data\"))) +\n  geom_line() +\n  guides(color=\"none\") +\n  theme_bw()\n```\n\nObviously, the proposed set of parameters doesn't seem to be right. We can play around with the parameters to see whether we can obtain better simulations.\n\n### A SEIR model?\n\n## Likelihood-based inference for POMP models\n\n### Recap: the POMP structure\n\nThe POMP model consist of two components: the state process $X_t$ and the observations $Y_t$. The state process is Markovian, and transitions between states are driven by the **process model**, where we code it as `sird_step`; the observations are derived from the states, governed by a **measurement model** which we code in `sird_rmeas` for simulation and `sird_dmeas` for density computation.\n\n![**POMP schematic**. In previous example, we define the states as $X_t = (S_t, I_t, R_t, D_t)$ and the process model as the SIR dynamics; the observations are the weekly reported cases and the measurement model is defined as a negative binomial distribution.](POMP2.png){width=\"500\"}\n\n### Likelihood of the stochastic SIR model for measles\n\nIt will be straight-forward to compute the log-likelihood of $\\rho$ and $k$ of the data (i.e., weekly reports), given the underlying weekly cumulative diagnoses:\n\n$$\n\\ell(\\rho, k) = \\sum_{w=1}^W\\, \\log p(report_w \\vert D_w, \\rho, k),\n$$\n\nwhere $W$ is the total number of reporting weeks and $p$ is the probability mass function of Negative Binomial distribution. Unfortunately, we don't have any knowledge of $D_t$ because it is part of the unobserved underlying stochastic SIR dynamics, which we can simulate.\n\nNow, let's keep in mind the observations $Y_w$ is the weekly reported cases $\\mathrm{report}_w$, and the unobserved states $X_t = (S_t, I_t, R_t, D_t)$. We denote the density of the process model as $f_{X_w\\vert X_{w-1}}$, the density of the measurement model as $f_{Y_w\\vert X_w}$, the initialization density $f_{X_0}$, and the set of parameters $\\theta = (\\beta, \\gamma, \\rho, k, S_0, I_0, R_0)$. The likelihood for a POMP model takes the form of an integral:\n\n$$\n\\mathcal{L}(\\theta) = \\int f_{X_0}(x_0;\\theta)\\, \\prod_{w=1}^W f_{Y_w\\vert X_w} (y_w^\\ast\\vert x_w;\\theta)\\,f_{X_w\\vert X_{w-1}}(x_w \\vert x_w;\\theta) d x_{0:W},\n$$\n\nwhere $y_w^\\ast$ is the data (reported cases) in $w$-th week.\n\n### Monte Carlo likelihood by direct simulation\n\nThe intuitive idea is to simulate the sequence of states at each week, denoted as $X_{0:W}$, and then the likelihood is given by\n\n```{=tex}\n\\begin{aligned}\n  y &= \\int \\prod_{w=1}^W\\,f_{Y_w\\vert X_w}(y_w^\\ast\\vert x_w;\\theta) f_{X_{0:W}}(x_{0:w};\\theta) dx_{0:W} \\\\\n    &= \\mathbb{E}\\left[\\prod_{w=1}^W\\,f_{Y_w\\vert X_w}(y_w^\\ast\\vert x_w;\\theta)\\right],\n\\end{aligned}\n```\nwhere the expectation is taken with $X_{0:W}\\sim f_{X_{0:W}}(x_{0:W};\\theta)$.\n\nUsing a **law of large numbers**, we can simulate a large set ($J$) of sequences $\\{X_{0:W}^j, j=1,\\dots,J\\}$ and take the average of this Monte Carlo sample, which converges to the expectation: $$\n\\mathcal{L}(\\theta) \\approx \\frac{1}{J}\\sum_{j=1}^J\\,\\prod_{w=1}^W\\,f_{Y_w\\vert X_w}(y_w^\\ast\\vert x_w;\\theta).\n$$\n\nHowever, things are not that straight-forward and intuitive because this naive approach scales poorly with dimension, in other words, it requires a Monte Carlo effort ($J$) that scales exponentially with the length of the time series, and so is unfeasible on anything but a short data set. Another aspect to consider is that, the underlying process is stochastic, the simulations therefore diverge mostly, many of which don't align with the observed data thus are useless for parameter estimation.\n\n### Sequential Monte Carlo: the particle filter\n\nThe idea of Sequential Monte Carlo (SMC) is that, instead of computing and evaluating the whole sequence of states at once, we evaluate the state at every time step. At current state, we evaluate whether it aligns with the data: if yes, then we keep it and continue to simulate the next state; if no, we just drop it. This procedure is called \"importance sampling\".\n\nWe therefore can factorize the likelihood:\n\n```{=tex}\n\\begin{aligned}\n  \\mathcal{L}(\\theta) &= f_{Y_{1:W}}(y_{1:W}^\\ast;\\theta) = \\prod_{w=1}^W\\,f_{Y_w\\vert Y_{1:w-1}}(y_w^\\ast\\vert y^\\ast_{1:w-1};\\theta) \\\\\n  &= \\prod_{w=1}^W\\,\\int f_{Y_w\\vert X_w}(y_w^\\ast \\vert x_w;\\theta) f_{X_w\\vert Y_{1:w-1}}(x_w\\vert y_{1:w-1}^\\ast) d x_w,\n\\end{aligned}\n```\nwhere obviously $f_{X_1\\vert Y_{1:0}} = f_{X_1}$. With the Markov property and the Baye's theorem, we can break the factorization into two steps, **predition** and **filtering**:\n\n-   the prediction formula, gives the prediction at time $t_w$ conditioned on the filtering distribution at time $t_{w-1}$:\n\n```{=tex}\n\\begin{aligned}\n  & f_{X_w\\vert Y_{1:w-1}}(x_w\\vert y_{1:w-1}^\\ast;\\theta) \\\\\n  & \\qquad = \\int f_{X_w\\vert X_{w-1}}(x_w\\vert x_{w-1};\\theta) f_{X_{w-1}\\vert Y_{1:w-1}} (x_{w-1}\\vert y_{1:w-1}^\\ast) d x_{w-1};\n\\end{aligned}\n```\n-   the filtering formula, gives the filtering distribution at time $t_w$ using the prediction distribution at time $t_w$:\n\n```{=tex}\n\\begin{aligned}\n  & f_{X_w\\vert Y_{1:w}} (x_w\\vert y_{1:w}^\\ast;\\theta) \\\\\n  & \\qquad = f_{X_w\\vert Y_w, Y_{1:w-1}} (x_w \\vert y_w^\\ast, y_{1:w-1}^\\ast;\\theta) \\\\\n  & \\qquad = \\dfrac{f_{Y_w\\vert X_w}(y_w^\\ast\\vert x_w;\\theta)\\,f_{X_w\\vert Y_{1:w-1}}(x_w\\vert y_{1:w-1}^\\ast;\\theta)}{\\int f_{Y_w\\vert X_w} (y_{w}^\\ast\\vert u_w;\\theta)\\,f_{X_w\\vert Y_{1:w-1}}(u_w\\vert y_{1:w-1}^\\ast;\\theta)} d u_w.\n\\end{aligned}\n```\nWe now can have a one-step particle filtering as follows:\n\n1.  Suppose we have a set of $J$ samples for the state $X_{w-1}^F, j=1,\\dots,J$ drawn from the filtering distribution at time $t_{w-1}$;\n\n2.  We then can obtain a set of samples $X_{w, j}^P$ drawn from the prediction distribution at time $t_w$ by simulating the process model: $$\n    X_{w,j}^P \\sim \\mathrm{process}(X_{w-1}^F,\\theta), j=1,\\dots,J;\n    $$\n\n3.  Given the set of samples for the state at time $t_w$, we can obtain the filtering distribution by *resampling* from this set $\\{X_{w,j}^P, j=1,\\dots,J\\}$ with weights (the density of the measurement model): $$\n    \\nu_{w,j} = f_{Y_w\\vert X_w}(y_w^\\ast\\vert X_{w,j}^P;\\theta);\n    $$\n\n4.  The Monte Carlo principle (law of large numbers) gives the approximated likelihood: $$\n    \\hat{\\mathcal{L}}_w(\\theta) \\approx \\frac{1}{J} \\sum_j f_{Y_w\\vert X_w} (y_{w}^\\ast \\vert X_{w,j}^P;\\theta)\n    $$ since $X_{w,j}^P$ is approximately a draw from $f_{X_w\\vert Y_{1:w-1}} (x_w \\vert y_{1:w-1}^\\ast;\\theta)$;\n\nThen we can iterate through the end of the time and the approximated full log-likelihood is given by: $$\n\\ell(\\theta) = \\log \\mathcal{L}(\\theta) = \\sum_w \\log \\mathcal{L}_{w}(\\theta) \\approx \\sum_w \\hat{\\mathcal{L}}_{w}(\\theta).\n$$\n\n## Paticle filtering in `pomp`\n\nWe can now try to compute the log-likelihood in `pomp` using the function `pfilter` with $J=5000$:\n\n```{r pfilter}\nsird_measle |>\n  pfilter(\n    params = c(\n      Beta = 7.5, gamma = .5, rho = .5, k = 10,\n      S0 = 1140, I0 = 1, R0 = 36860\n    ),\n    Np=5000\n  ) -> pf\nlogLik(pf)\n```\n\nWe can try another set of parameters:\n\n```{r pf-2}\nsird_measle |>\n  pfilter(\n    params = c(\n      Beta = 15, gamma = .5, rho = .5, k = 10,\n      S0 = 2280, I0 = 1, R0 = 35720\n    ),\n    Np=5000\n  ) |>\n  logLik()\n```\n\nUsing parallel computation, we can see the average log-likelihood and the variation in log-likelihoods:\n\n```{r parallel-pf}\nregisterDoParallel(cores=detectCores()-1)\nforeach (\n  i=1:10, .combine=c, .options.future=list(seed=652643293)\n  ) %dopar% {\n  sird_measle |>\n  pfilter(\n    params = c(\n      Beta = 15, gamma = .5, rho = .5, k = 10,\n      S0 = 2280, I0 = 1, R0 = 35720\n    ),\n    Np=5000\n  )\n} -> pf\nlogLik(pf) -> ll\nlogmeanexp(ll,se=TRUE)\n```\n\n## Slice likelihood: changing one specific parameter\n\n```{r slice-llk}\nsird_measle |>\n  pomp(\n    params =  c(\n      Beta = 15, gamma = .5, rho = .5, k = 10,\n      S0 = 2280, I0 = 1, R0 = 35720\n    )\n  ) -> sird_measle\n\nslice_design(\n  center=coef(sird_measle),\n  Beta = rep(seq(from=5,to=30,length=40),each=3),\n  gamma = rep(seq(from=0.2,to=2,length=40),each=3)\n) -> params_slice\n\nhead(params_slice)\n\nsummary(params_slice)\n```\n\nNow we can run the computations of likelihood at different combinations of parameters and visualize:\n\n```{r slice-llk-viz}\nforeach (\n  theta=iter(params_slice,\"row\"), .combine=rbind, .options.future=list(seed=108028909)\n  ) %dopar% {\n    sird_measle |> pfilter(params=theta,Np=5000) -> pf\n    theta$loglik <- logLik(pf)\n    theta\n} -> llks_slice\n\nsummary(llks_slice)\n\nllks_slice |>\n  pivot_longer(c(Beta,gamma)) |>\n  filter(name==slice) |>\n  ggplot(aes(x=value,y=loglik,color=name))+\n  geom_point()+\n  facet_grid(~name,scales=\"free_x\")+\n  guides(color=\"none\")+\n  labs(x=\"parameter value\",color=\"\")\n```\n","srcMarkdownNoYaml":"\n\n## Summary\n\n-   Implementing in `pomp`\n\n-   Creating a `pomp` object\n\n-   Computing likelihood\n\n```{r loading}\noptions(warning = FALSE)\nlibrary(dplyr)\nlibrary(tidyr)\nlibrary(readr)\nlibrary(iterators)\nlibrary(pomp)\nlibrary(ggplot2)\nlibrary(foreach)\nlibrary(doParallel)\n```\n\n## Recap: A stochastic SIR model in the POMP framework\n\nWe define the **state process** as a stochastic SIR model, $X_t=(S_t, I_t, R_t, D_t)$ where $D_t$ is the weekly cumulative diagnoses, getting reset to 0 every 7 days. The number of infections $\\Delta N_{SI}$ and recoveries $\\Delta N_{IR}$ are random variables, following Binomial distributions:\n\n```{=tex}\n\\begin{aligned}\n  \\Delta N_{SI} &\\sim \\mathrm{Binomial}\\left(S, 1-e^{-\\beta\\frac{I}{N}\\Delta t}\\right) \\\\\n  \\Delta N_{IR} &\\sim \\mathrm{Binomial}\\left(I, 1-e^{-\\gamma \\Delta t}\\right)\n\\end{aligned}\n```\nwhere $\\beta$ is the transmission rate and $\\gamma$ is the recovery rate.\n\nWe define a weekly **measurement model** which follows a Negative Binomial distribution with mean $\\rho D_t$ and the dispersion parameter $k$:\n\n$$\n\\textrm{reports}_t \\sim \\textrm{NegBin}(\\rho D_t, k),\n$$\n\nwhere $\\rho$ is the reporting ratio and $0 \\leq \\rho \\leq 1$ .\n\n## Implementing the SIR model in `pomp` package\n\n### Example: the Consett measles outbreak\n\nWe first download the data and take a look at them:\n\n```{r dat_measle}\nread_csv(paste0(\"https://kingaa.github.io/sbied/stochsim/\", \n  \"Measles_Consett_1948.csv\")) |>\n  select(week,reports=cases) -> dat_meas\n\ndat_meas |> head()\n\ndat_meas |> summary()\n```\n\nand visualize:\n\n```{r dat_measle_viz}\ndat_meas |>\n  ggplot(aes(x=week, y=reports)) +\n  geom_line() +\n  geom_point() +\n  theme_bw()\n```\n\n### A stochastic SIR model for measles in `pomp`\n\nTo accelerate the computation, we would like to code the model in `C/C++` using `C snippets` in `pomp`. Now we can have our \\*\\*one-step transition\\*\\* function:\n\n```{r sird_step_c}\nsird_step <- Csnippet(\"\n  double N = S + I + R;\n  double dN_SI = rbinom(S, 1-exp(-Beta*I/N*dt));\n  double dN_IR = rbinom(I, 1-exp(-gamma*dt));\n  S -= dN_SI;\n  I += dN_SI - dN_IR;\n  R += dN_IR;\n  D += dN_IR;\n\")\n```\n\nand the initialization and measurement model:\n\n```{r sird_init_meas_c}\nsird_rinit <- Csnippet(\"\n  S = S0;\n  I = I0;\n  R = R0;\n  D = 0;\n\")\n\nsird_dmeas <- Csnippet(\"\n  lik = dnbinom_mu(reports, k, rho*D, give_log);\n\")\n\nsird_rmeas <- Csnippet(\"\n  reports = rnbinom_mu(k, rho*D);\n\")\n```\n\nNow we can put the model together in `pomp` and simulate the model dynamics:\n\n```{r sird_pomp}\ndat_meas |>\n  pomp(\n    times=\"week\",\n    t0=0,\n    rprocess = euler(sird_step, delta.t = 1/7),\n    rinit = sird_rinit,\n    rmeasure = sird_rmeas,\n    dmeasure = sird_dmeas,\n    accumvars = \"D\",\n    statenames = c(\"S\",\"I\",\"R\",\"D\"),\n    paramnames = c(\"Beta\",\"gamma\",\"rho\",\"k\",\"S0\",\"I0\",\"R0\")\n  ) -> sird_measle\n\n```\n\nAfter getting the components in place in `pomp`, we can simulate the dynamics with some parameters from literature or anything you want:\n\n```{r sird_pomp_sim}\nsird_measle |>\n  simulate(\n    params = c(\n      Beta = 7.5, gamma = .5, rho = .5, k = 10,\n      S0 = 1140, I0 = 1, R0 = 36860\n    ),\n    nsim = 100,\n    format = \"data.frame\",\n    include.data = TRUE\n  ) -> sims_measle\n\nsims_measle |> head()\n\nsummary(sims_measle)\n\nsims_measle |>\n  ggplot(aes(x=week, y=reports, group=.id,color=(.id==\"data\"))) +\n  geom_line() +\n  guides(color=\"none\") +\n  theme_bw()\n```\n\nObviously, the proposed set of parameters doesn't seem to be right. We can play around with the parameters to see whether we can obtain better simulations.\n\n### A SEIR model?\n\n## Likelihood-based inference for POMP models\n\n### Recap: the POMP structure\n\nThe POMP model consist of two components: the state process $X_t$ and the observations $Y_t$. The state process is Markovian, and transitions between states are driven by the **process model**, where we code it as `sird_step`; the observations are derived from the states, governed by a **measurement model** which we code in `sird_rmeas` for simulation and `sird_dmeas` for density computation.\n\n![**POMP schematic**. In previous example, we define the states as $X_t = (S_t, I_t, R_t, D_t)$ and the process model as the SIR dynamics; the observations are the weekly reported cases and the measurement model is defined as a negative binomial distribution.](POMP2.png){width=\"500\"}\n\n### Likelihood of the stochastic SIR model for measles\n\nIt will be straight-forward to compute the log-likelihood of $\\rho$ and $k$ of the data (i.e., weekly reports), given the underlying weekly cumulative diagnoses:\n\n$$\n\\ell(\\rho, k) = \\sum_{w=1}^W\\, \\log p(report_w \\vert D_w, \\rho, k),\n$$\n\nwhere $W$ is the total number of reporting weeks and $p$ is the probability mass function of Negative Binomial distribution. Unfortunately, we don't have any knowledge of $D_t$ because it is part of the unobserved underlying stochastic SIR dynamics, which we can simulate.\n\nNow, let's keep in mind the observations $Y_w$ is the weekly reported cases $\\mathrm{report}_w$, and the unobserved states $X_t = (S_t, I_t, R_t, D_t)$. We denote the density of the process model as $f_{X_w\\vert X_{w-1}}$, the density of the measurement model as $f_{Y_w\\vert X_w}$, the initialization density $f_{X_0}$, and the set of parameters $\\theta = (\\beta, \\gamma, \\rho, k, S_0, I_0, R_0)$. The likelihood for a POMP model takes the form of an integral:\n\n$$\n\\mathcal{L}(\\theta) = \\int f_{X_0}(x_0;\\theta)\\, \\prod_{w=1}^W f_{Y_w\\vert X_w} (y_w^\\ast\\vert x_w;\\theta)\\,f_{X_w\\vert X_{w-1}}(x_w \\vert x_w;\\theta) d x_{0:W},\n$$\n\nwhere $y_w^\\ast$ is the data (reported cases) in $w$-th week.\n\n### Monte Carlo likelihood by direct simulation\n\nThe intuitive idea is to simulate the sequence of states at each week, denoted as $X_{0:W}$, and then the likelihood is given by\n\n```{=tex}\n\\begin{aligned}\n  y &= \\int \\prod_{w=1}^W\\,f_{Y_w\\vert X_w}(y_w^\\ast\\vert x_w;\\theta) f_{X_{0:W}}(x_{0:w};\\theta) dx_{0:W} \\\\\n    &= \\mathbb{E}\\left[\\prod_{w=1}^W\\,f_{Y_w\\vert X_w}(y_w^\\ast\\vert x_w;\\theta)\\right],\n\\end{aligned}\n```\nwhere the expectation is taken with $X_{0:W}\\sim f_{X_{0:W}}(x_{0:W};\\theta)$.\n\nUsing a **law of large numbers**, we can simulate a large set ($J$) of sequences $\\{X_{0:W}^j, j=1,\\dots,J\\}$ and take the average of this Monte Carlo sample, which converges to the expectation: $$\n\\mathcal{L}(\\theta) \\approx \\frac{1}{J}\\sum_{j=1}^J\\,\\prod_{w=1}^W\\,f_{Y_w\\vert X_w}(y_w^\\ast\\vert x_w;\\theta).\n$$\n\nHowever, things are not that straight-forward and intuitive because this naive approach scales poorly with dimension, in other words, it requires a Monte Carlo effort ($J$) that scales exponentially with the length of the time series, and so is unfeasible on anything but a short data set. Another aspect to consider is that, the underlying process is stochastic, the simulations therefore diverge mostly, many of which don't align with the observed data thus are useless for parameter estimation.\n\n### Sequential Monte Carlo: the particle filter\n\nThe idea of Sequential Monte Carlo (SMC) is that, instead of computing and evaluating the whole sequence of states at once, we evaluate the state at every time step. At current state, we evaluate whether it aligns with the data: if yes, then we keep it and continue to simulate the next state; if no, we just drop it. This procedure is called \"importance sampling\".\n\nWe therefore can factorize the likelihood:\n\n```{=tex}\n\\begin{aligned}\n  \\mathcal{L}(\\theta) &= f_{Y_{1:W}}(y_{1:W}^\\ast;\\theta) = \\prod_{w=1}^W\\,f_{Y_w\\vert Y_{1:w-1}}(y_w^\\ast\\vert y^\\ast_{1:w-1};\\theta) \\\\\n  &= \\prod_{w=1}^W\\,\\int f_{Y_w\\vert X_w}(y_w^\\ast \\vert x_w;\\theta) f_{X_w\\vert Y_{1:w-1}}(x_w\\vert y_{1:w-1}^\\ast) d x_w,\n\\end{aligned}\n```\nwhere obviously $f_{X_1\\vert Y_{1:0}} = f_{X_1}$. With the Markov property and the Baye's theorem, we can break the factorization into two steps, **predition** and **filtering**:\n\n-   the prediction formula, gives the prediction at time $t_w$ conditioned on the filtering distribution at time $t_{w-1}$:\n\n```{=tex}\n\\begin{aligned}\n  & f_{X_w\\vert Y_{1:w-1}}(x_w\\vert y_{1:w-1}^\\ast;\\theta) \\\\\n  & \\qquad = \\int f_{X_w\\vert X_{w-1}}(x_w\\vert x_{w-1};\\theta) f_{X_{w-1}\\vert Y_{1:w-1}} (x_{w-1}\\vert y_{1:w-1}^\\ast) d x_{w-1};\n\\end{aligned}\n```\n-   the filtering formula, gives the filtering distribution at time $t_w$ using the prediction distribution at time $t_w$:\n\n```{=tex}\n\\begin{aligned}\n  & f_{X_w\\vert Y_{1:w}} (x_w\\vert y_{1:w}^\\ast;\\theta) \\\\\n  & \\qquad = f_{X_w\\vert Y_w, Y_{1:w-1}} (x_w \\vert y_w^\\ast, y_{1:w-1}^\\ast;\\theta) \\\\\n  & \\qquad = \\dfrac{f_{Y_w\\vert X_w}(y_w^\\ast\\vert x_w;\\theta)\\,f_{X_w\\vert Y_{1:w-1}}(x_w\\vert y_{1:w-1}^\\ast;\\theta)}{\\int f_{Y_w\\vert X_w} (y_{w}^\\ast\\vert u_w;\\theta)\\,f_{X_w\\vert Y_{1:w-1}}(u_w\\vert y_{1:w-1}^\\ast;\\theta)} d u_w.\n\\end{aligned}\n```\nWe now can have a one-step particle filtering as follows:\n\n1.  Suppose we have a set of $J$ samples for the state $X_{w-1}^F, j=1,\\dots,J$ drawn from the filtering distribution at time $t_{w-1}$;\n\n2.  We then can obtain a set of samples $X_{w, j}^P$ drawn from the prediction distribution at time $t_w$ by simulating the process model: $$\n    X_{w,j}^P \\sim \\mathrm{process}(X_{w-1}^F,\\theta), j=1,\\dots,J;\n    $$\n\n3.  Given the set of samples for the state at time $t_w$, we can obtain the filtering distribution by *resampling* from this set $\\{X_{w,j}^P, j=1,\\dots,J\\}$ with weights (the density of the measurement model): $$\n    \\nu_{w,j} = f_{Y_w\\vert X_w}(y_w^\\ast\\vert X_{w,j}^P;\\theta);\n    $$\n\n4.  The Monte Carlo principle (law of large numbers) gives the approximated likelihood: $$\n    \\hat{\\mathcal{L}}_w(\\theta) \\approx \\frac{1}{J} \\sum_j f_{Y_w\\vert X_w} (y_{w}^\\ast \\vert X_{w,j}^P;\\theta)\n    $$ since $X_{w,j}^P$ is approximately a draw from $f_{X_w\\vert Y_{1:w-1}} (x_w \\vert y_{1:w-1}^\\ast;\\theta)$;\n\nThen we can iterate through the end of the time and the approximated full log-likelihood is given by: $$\n\\ell(\\theta) = \\log \\mathcal{L}(\\theta) = \\sum_w \\log \\mathcal{L}_{w}(\\theta) \\approx \\sum_w \\hat{\\mathcal{L}}_{w}(\\theta).\n$$\n\n## Paticle filtering in `pomp`\n\nWe can now try to compute the log-likelihood in `pomp` using the function `pfilter` with $J=5000$:\n\n```{r pfilter}\nsird_measle |>\n  pfilter(\n    params = c(\n      Beta = 7.5, gamma = .5, rho = .5, k = 10,\n      S0 = 1140, I0 = 1, R0 = 36860\n    ),\n    Np=5000\n  ) -> pf\nlogLik(pf)\n```\n\nWe can try another set of parameters:\n\n```{r pf-2}\nsird_measle |>\n  pfilter(\n    params = c(\n      Beta = 15, gamma = .5, rho = .5, k = 10,\n      S0 = 2280, I0 = 1, R0 = 35720\n    ),\n    Np=5000\n  ) |>\n  logLik()\n```\n\nUsing parallel computation, we can see the average log-likelihood and the variation in log-likelihoods:\n\n```{r parallel-pf}\nregisterDoParallel(cores=detectCores()-1)\nforeach (\n  i=1:10, .combine=c, .options.future=list(seed=652643293)\n  ) %dopar% {\n  sird_measle |>\n  pfilter(\n    params = c(\n      Beta = 15, gamma = .5, rho = .5, k = 10,\n      S0 = 2280, I0 = 1, R0 = 35720\n    ),\n    Np=5000\n  )\n} -> pf\nlogLik(pf) -> ll\nlogmeanexp(ll,se=TRUE)\n```\n\n## Slice likelihood: changing one specific parameter\n\n```{r slice-llk}\nsird_measle |>\n  pomp(\n    params =  c(\n      Beta = 15, gamma = .5, rho = .5, k = 10,\n      S0 = 2280, I0 = 1, R0 = 35720\n    )\n  ) -> sird_measle\n\nslice_design(\n  center=coef(sird_measle),\n  Beta = rep(seq(from=5,to=30,length=40),each=3),\n  gamma = rep(seq(from=0.2,to=2,length=40),each=3)\n) -> params_slice\n\nhead(params_slice)\n\nsummary(params_slice)\n```\n\nNow we can run the computations of likelihood at different combinations of parameters and visualize:\n\n```{r slice-llk-viz}\nforeach (\n  theta=iter(params_slice,\"row\"), .combine=rbind, .options.future=list(seed=108028909)\n  ) %dopar% {\n    sird_measle |> pfilter(params=theta,Np=5000) -> pf\n    theta$loglik <- logLik(pf)\n    theta\n} -> llks_slice\n\nsummary(llks_slice)\n\nllks_slice |>\n  pivot_longer(c(Beta,gamma)) |>\n  filter(name==slice) |>\n  ggplot(aes(x=value,y=loglik,color=name))+\n  geom_point()+\n  facet_grid(~name,scales=\"free_x\")+\n  guides(color=\"none\")+\n  labs(x=\"parameter value\",color=\"\")\n```\n"},"formats":{"html":{"identifier":{"display-name":"HTML","target-format":"html","base-format":"html"},"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":false,"echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"engine":"knitr"},"render":{"keep-tex":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":"none","code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"inline-includes":false,"preserve-yaml":false,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[],"notebook-links":true,"format-links":true},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","css":["styles.css"],"toc":true,"include-in-header":{"text":"<script>\nMathJax = {\n  loader: {\n    load: ['[tex]/boldsymbol']\n  },\n  tex: {\n    tags: \"all\",\n    inlineMath: [['$','$'], ['\\\\(','\\\\)']],\n    displayMath: [['$$','$$'], ['\\\\[','\\\\]']],\n    processEscapes: true,\n    processEnvironments: true,\n    packages: {\n      '[+]': ['boldsymbol']\n    }\n  }\n};\n</script>\n<script src=\"https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js\" type=\"text/javascript\"></script>\n"},"embed-resources":true,"output-file":"likelihood.html"},"language":{"toc-title-document":"Table of contents","toc-title-website":"On this page","related-formats-title":"Other Formats","related-notebooks-title":"Notebooks","source-notebooks-prefix":"Source","section-title-abstract":"Abstract","section-title-appendices":"Appendices","section-title-footnotes":"Footnotes","section-title-references":"References","section-title-reuse":"Reuse","section-title-copyright":"Copyright","section-title-citation":"Citation","appendix-attribution-cite-as":"For attribution, please cite this work as:","appendix-attribution-bibtex":"BibTeX citation:","title-block-author-single":"Author","title-block-author-plural":"Authors","title-block-affiliation-single":"Affiliation","title-block-affiliation-plural":"Affiliations","title-block-published":"Published","title-block-modified":"Modified","callout-tip-title":"Tip","callout-note-title":"Note","callout-warning-title":"Warning","callout-important-title":"Important","callout-caution-title":"Caution","code-summary":"Code","code-tools-menu-caption":"Code","code-tools-show-all-code":"Show All Code","code-tools-hide-all-code":"Hide All Code","code-tools-view-source":"View Source","code-tools-source-code":"Source Code","code-line":"Line","code-lines":"Lines","copy-button-tooltip":"Copy to Clipboard","copy-button-tooltip-success":"Copied!","repo-action-links-edit":"Edit this page","repo-action-links-source":"View source","repo-action-links-issue":"Report an issue","back-to-top":"Back to top","search-no-results-text":"No results","search-matching-documents-text":"matching documents","search-copy-link-title":"Copy link to search","search-hide-matches-text":"Hide additional matches","search-more-match-text":"more match in this document","search-more-matches-text":"more matches in this document","search-clear-button-title":"Clear","search-detached-cancel-button-title":"Cancel","search-submit-button-title":"Submit","search-label":"Search","toggle-section":"Toggle section","toggle-sidebar":"Toggle sidebar navigation","toggle-dark-mode":"Toggle dark mode","toggle-reader-mode":"Toggle reader mode","toggle-navigation":"Toggle navigation","crossref-fig-title":"Figure","crossref-tbl-title":"Table","crossref-lst-title":"Listing","crossref-thm-title":"Theorem","crossref-lem-title":"Lemma","crossref-cor-title":"Corollary","crossref-prp-title":"Proposition","crossref-cnj-title":"Conjecture","crossref-def-title":"Definition","crossref-exm-title":"Example","crossref-exr-title":"Exercise","crossref-ch-prefix":"Chapter","crossref-apx-prefix":"Appendix","crossref-sec-prefix":"Section","crossref-eq-prefix":"Equation","crossref-lof-title":"List of Figures","crossref-lot-title":"List of Tables","crossref-lol-title":"List of Listings","environment-proof-title":"Proof","environment-remark-title":"Remark","environment-solution-title":"Solution","listing-page-order-by":"Order By","listing-page-order-by-default":"Default","listing-page-order-by-date-asc":"Oldest","listing-page-order-by-date-desc":"Newest","listing-page-order-by-number-desc":"High to Low","listing-page-order-by-number-asc":"Low to High","listing-page-field-date":"Date","listing-page-field-title":"Title","listing-page-field-description":"Description","listing-page-field-author":"Author","listing-page-field-filename":"File Name","listing-page-field-filemodified":"Modified","listing-page-field-subtitle":"Subtitle","listing-page-field-readingtime":"Reading Time","listing-page-field-categories":"Categories","listing-page-minutes-compact":"{0} min","listing-page-category-all":"All","listing-page-no-matches":"No matching items"},"metadata":{"lang":"en","fig-responsive":true,"quarto-version":"1.3.450","editor":"visual","theme":"cosmo","title":"Likelihood","author":"Qianying (Ruby) Lin","code-background":true},"extensions":{"book":{"multiFile":true}}}},"projectFormats":["html","beamer"]}