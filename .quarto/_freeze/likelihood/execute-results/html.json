{
  "hash": "6c847587392d362c1882b858585cbfa7",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Simulation and Likelihood\"\nauthor: \"Qianying (Ruby) Lin\"\ntoc: true\nformat:\n  html:\n    embed-resources: true\n    code-background: true\nengine: knitr\n---\n\n\n\n\n## Summary\n\n-   Model implementation in `pomp`\n\n-   Creating a `pomp` object with different components\n\n-   Computing likelihood using `pfilter`\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\noptions(warning = FALSE)\nlibrary(dplyr)\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\n\nAttaching package: 'dplyr'\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n```\n\n\n:::\n\n```{.r .cell-code}\nlibrary(tidyr)\nlibrary(readr)\nlibrary(iterators)\nlibrary(pomp)\nlibrary(ggplot2)\nlibrary(foreach)\nlibrary(doParallel)\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nLoading required package: parallel\n```\n\n\n:::\n:::\n\n\n\n\n## Recap: A stochastic SIR model in the POMP framework\n\nWe define the **state process** as a stochastic SIR model, $X_t=(S_t, I_t, R_t, D_t)$ where $D_t$ is the weekly cumulative diagnoses, getting reset to 0 every 7 days. The number of infections $\\Delta N_{SI}$ and recoveries $\\Delta N_{IR}$ are random variables, following Binomial distributions:\n\n\n\n\n```{=tex}\n\\begin{aligned}\n  \\Delta N_{SI} &\\sim \\mathrm{Binomial}\\left(S, 1-e^{-\\beta\\frac{I}{N}\\Delta t}\\right) \\\\\n  \\Delta N_{IR} &\\sim \\mathrm{Binomial}\\left(I, 1-e^{-\\gamma \\Delta t}\\right)\n\\end{aligned}\n```\n\n\n\nwhere $\\beta$ is the transmission rate and $\\gamma$ is the recovery rate.\n\nWe define a weekly **measurement model** which follows a Negative Binomial distribution with mean $\\rho D_t$ and the dispersion parameter $k$:\n\n$$\n\\textrm{reports}_t \\sim \\textrm{NegBin}(\\rho D_t, k),\n$$\n\nwhere $\\rho$ is the reporting ratio and $0 \\leq \\rho \\leq 1$ .\n\n## Implementing the SIR model in `pomp` package\n\n### Example: the Consett measles outbreak\n\nWe first download the data and take a look at them:\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nread_csv(paste0(\"https://kingaa.github.io/sbied/stochsim/\", \n  \"Measles_Consett_1948.csv\")) |>\n  select(week,reports=cases) -> dat_meas\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nRows: 53 Columns: 2\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\ndbl (2): week, cases\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n```\n\n\n:::\n\n```{.r .cell-code}\ndat_meas |> head()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 6 × 2\n   week reports\n  <dbl>   <dbl>\n1     1       0\n2     2       0\n3     3       2\n4     4       0\n5     5       3\n6     6       0\n```\n\n\n:::\n\n```{.r .cell-code}\ndat_meas |> summary()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n      week       reports     \n Min.   : 1   Min.   : 0.00  \n 1st Qu.:14   1st Qu.: 0.00  \n Median :27   Median : 1.00  \n Mean   :27   Mean   : 9.83  \n 3rd Qu.:40   3rd Qu.: 7.00  \n Max.   :53   Max.   :75.00  \n```\n\n\n:::\n:::\n\n\n\n\nand visualize:\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndat_meas |>\n  ggplot(aes(x=week, y=reports)) +\n  geom_line() +\n  geom_point() +\n  theme_bw()\n```\n\n::: {.cell-output-display}\n![](likelihood_files/figure-html/dat_measle_viz-1.png){width=672}\n:::\n:::\n\n\n\n\n### A stochastic SIR model for measles in `pomp`\n\nTo accelerate the computation, we would like to code the model in `C/C++` using `C snippets` in `pomp`. Now we can have our \\*\\*one-step transition\\*\\* function:\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsird_step <- Csnippet(\"\n  double N = S + I + R;\n  double dN_SI = rbinom(S, 1-exp(-Beta*I/N*dt));\n  double dN_IR = rbinom(I, 1-exp(-gamma*dt));\n  S -= dN_SI;\n  I += dN_SI - dN_IR;\n  R += dN_IR;\n  D += dN_IR;\n\")\n```\n:::\n\n\n\n\nand the initialization and measurement model:\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsird_rinit <- Csnippet(\"\n  S = S0;\n  I = I0;\n  R = R0;\n  D = 0;\n\")\n\nsird_dmeas <- Csnippet(\"\n  lik = dnbinom_mu(reports, k, rho*D, give_log);\n\")\n\nsird_rmeas <- Csnippet(\"\n  reports = rnbinom_mu(k, rho*D);\n\")\n```\n:::\n\n\n\n\nNow we can put the model together in `pomp` and simulate the model dynamics:\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndat_meas |>\n  pomp(\n    times=\"week\",\n    t0=0,\n    rprocess = euler(sird_step, delta.t = 1/7),\n    rinit = sird_rinit,\n    rmeasure = sird_rmeas,\n    dmeasure = sird_dmeas,\n    accumvars = \"D\",\n    statenames = c(\"S\",\"I\",\"R\",\"D\"),\n    paramnames = c(\"Beta\",\"gamma\",\"rho\",\"k\",\"S0\",\"I0\",\"R0\")\n  ) -> sird_measle\n```\n:::\n\n\n\n\nAfter getting the components in place in `pomp`, we can simulate the dynamics with some parameters from literature or anything you want:\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsird_measle |>\n  simulate(\n    params = c(\n      Beta = 7.5, gamma = .5, rho = .5, k = 10,\n      S0 = 1140, I0 = 1, R0 = 36860\n    ),\n    nsim = 100,\n    format = \"data.frame\",\n    include.data = TRUE\n  ) -> sims_measle\n\nsims_measle |> head()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n  week  .id reports  S  I  R  D\n1    1 data       0 NA NA NA NA\n2    2 data       0 NA NA NA NA\n3    3 data       2 NA NA NA NA\n4    4 data       0 NA NA NA NA\n5    5 data       3 NA NA NA NA\n6    6 data       0 NA NA NA NA\n```\n\n\n:::\n\n```{.r .cell-code}\nsummary(sims_measle)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n      week         .id          reports              S              I          \n Min.   : 1   data   :  53   Min.   : 0.0000   Min.   :1132   Min.   :0.00000  \n 1st Qu.:14   1      :  53   1st Qu.: 0.0000   1st Qu.:1139   1st Qu.:0.00000  \n Median :27   2      :  53   Median : 0.0000   Median :1140   Median :0.00000  \n Mean   :27   3      :  53   Mean   : 0.1151   Mean   :1139   Mean   :0.06415  \n 3rd Qu.:40   4      :  53   3rd Qu.: 0.0000   3rd Qu.:1140   3rd Qu.:0.00000  \n Max.   :53   5      :  53   Max.   :75.0000   Max.   :1140   Max.   :5.00000  \n              (Other):5035                     NA's   :53     NA's   :53       \n       R               D          \n Min.   :36860   Min.   :0.00000  \n 1st Qu.:36861   1st Qu.:0.00000  \n Median :36861   Median :0.00000  \n Mean   :36862   Mean   :0.03396  \n 3rd Qu.:36862   3rd Qu.:0.00000  \n Max.   :36869   Max.   :3.00000  \n NA's   :53      NA's   :53       \n```\n\n\n:::\n\n```{.r .cell-code}\nsims_measle |>\n  ggplot(aes(x=week, y=reports, group=.id,color=(.id==\"data\"))) +\n  geom_line() +\n  guides(color=\"none\") +\n  theme_bw()\n```\n\n::: {.cell-output-display}\n![](likelihood_files/figure-html/sird_pomp_sim-1.png){width=672}\n:::\n:::\n\n\n\n\nObviously, the proposed set of parameters doesn't seem to be right. We can play around with the parameters to see whether we can obtain better simulations.\n\n### A SEIR model?\n\n## Likelihood-based inference for POMP models\n\n### Goal: estimating the parameters\n\nWe are interested in estimating the parameters $\\theta = (\\beta, \\gamma, \\rho, k, S_0, I_0, R_0)$ of the SIR model, given the weekly reported cases. \nThe likelihood-based inference is to find the set of parameters that maximize the likelihood of the data, i.e., the probability of observing the data given the parameters.\nNow we break down this problem into two steps:\n\n- compute the likelihood of the data given the parameters;\n\n- find the set of parameters that maximize the likelihood.\n\nThis session will focus on the first step and we will use the `pfilter` function in `pomp` to compute the likelihood of the data given the model and parameters.\n\n### Recap: the POMP structure\n\nThe POMP model consist of two components: the state process $X_t$ and the observations $Y_t$. The state process is Markovian, and transitions between states are driven by the **process model**, where we code it as `sird_step`; the observations are derived from the states, governed by a **measurement model** which we code in `sird_rmeas` for simulation and `sird_dmeas` for density computation.\n\n![**POMP schematic**. In previous example, we define the states as $X_t = (S_t, I_t, R_t, D_t)$ and the process model as the SIR dynamics; the observations are the weekly reported cases and the measurement model is defined as a negative binomial distribution.](POMP2.png){width=\"500\"}\n\n### Likelihood of the stochastic SIR model for measles\n\nIt will be straight-forward to compute the log-likelihood of $\\rho$ and $k$ of the data (i.e., weekly reports), given the underlying weekly cumulative diagnoses:\n\n$$\n\\ell(\\rho, k) = \\sum_{w=1}^W\\, \\log p(report_w \\vert D_w, \\rho, k),\n$$\n\nwhere $W$ is the total number of reporting weeks and $p$ is the probability mass function of Negative Binomial distribution. Unfortunately, we don't have any knowledge of $D_t$ because it is part of the unobserved underlying stochastic SIR dynamics, which we can simulate.\n\nNow, let's keep in mind the observations $Y_w$ is the weekly reported cases $\\mathrm{report}_w$, and the unobserved states $X_t = (S_t, I_t, R_t, D_t)$. We denote the density of the process model as $f_{X_w\\vert X_{w-1}}$, the density of the measurement model as $f_{Y_w\\vert X_w}$, the initialization density $f_{X_0}$, and the set of parameters $\\theta = (\\beta, \\gamma, \\rho, k, S_0, I_0, R_0)$. The likelihood for a POMP model takes the form of an integral:\n\n$$\n\\mathcal{L}(\\theta) = \\int f_{X_0}(x_0;\\theta)\\, \\prod_{w=1}^W f_{Y_w\\vert X_w} (y_w^\\ast\\vert x_w;\\theta)\\,f_{X_w\\vert X_{w-1}}(x_w \\vert x_w;\\theta) d x_{0:W},\n$$\n\nwhere $y_w^\\ast$ is the data (reported cases) in $w$-th week.\n\n### Monte Carlo likelihood by direct simulation\n\nThe intuitive idea is to simulate the sequence of states at each week, denoted as $X_{0:W}$, and then the likelihood is given by\n\n\n\n\n```{=tex}\n\\begin{aligned}\n  \\mathcal{L}(\\theta) &= \\int \\left\\{\\prod_{w=1}^W\\,f_{Y_w\\vert X_w}(y_w^\\ast\\vert x_w;\\theta)\\right\\} f_{X_{0:W}}(x_{0:W};\\theta) dx_{0:W} \\\\\n    &= \\mathbb{E}\\left[\\prod_{w=1}^W\\,f_{Y_w\\vert X_w}(y_w^\\ast\\vert x_w;\\theta)\\right],\n\\end{aligned}\n```\n\n\n\nwhere the expectation is taken with $X_{0:W}\\sim f_{X_{0:W}}(x_{0:W};\\theta)$.\n\nUsing a **law of large numbers**, we can simulate a large set ($J$) of sequences $\\{X_{0:W}^j, j=1,\\dots,J\\}$ and take the average of this Monte Carlo sample, which converges to the expectation: $$\n\\mathcal{L}(\\theta) \\approx \\frac{1}{J}\\sum_{j=1}^J\\,\\prod_{w=1}^W\\,f_{Y_w\\vert X_w}(y_w^\\ast\\vert x_w;\\theta).\n$$\n\nHowever, things are not that straight-forward and intuitive because this naive approach scales poorly with dimension, in other words, it requires a Monte Carlo effort ($J$) that scales exponentially with the length of the time series, and so is unfeasible on anything but a short data set. Another aspect to consider is that, the underlying process is stochastic, the simulations therefore diverge mostly, many of which don't align with the observed data thus are useless for parameter estimation.\n\n### Sequential Monte Carlo: the particle filter\n\nThe idea of Sequential Monte Carlo (SMC) is that, instead of computing and evaluating the whole sequence of states at once, we evaluate the state at every time step. At current state, we evaluate whether it aligns with the data: if yes, then we keep it and continue to simulate the next state; if no, we just drop it. This procedure is called \"importance sampling\".\n\nWe therefore can factorize the likelihood:\n\n\n\n\n```{=tex}\n\\begin{aligned}\n  \\mathcal{L}(\\theta) &= f_{Y_{1:W}}(y_{1:W}^\\ast;\\theta) = \\prod_{w=1}^W\\,f_{Y_w\\vert Y_{1:w-1}}(y_w^\\ast\\vert y^\\ast_{1:w-1};\\theta) \\\\\n  &= \\prod_{w=1}^W\\,\\int f_{Y_w\\vert X_w}(y_w^\\ast \\vert x_w;\\theta) f_{X_w\\vert Y_{1:w-1}}(x_w\\vert y_{1:w-1}^\\ast) d x_w,\n\\end{aligned}\n```\n\n\n\nwhere obviously $f_{X_1\\vert Y_{1:0}} = f_{X_1}$. With the Markov property and the Baye's theorem, we can break the factorization into two steps, **predition** and **filtering**:\n\n-   the prediction formula, gives the prediction at time $t_w$ conditioned on the filtering distribution at time $t_{w-1}$:\n\n\n\n\n```{=tex}\n\\begin{aligned}\n  & f_{X_w\\vert Y_{1:w-1}}(x_w\\vert y_{1:w-1}^\\ast;\\theta) \\\\\n  & \\qquad = \\int f_{X_w\\vert X_{w-1}}(x_w\\vert x_{w-1};\\theta) f_{X_{w-1}\\vert Y_{1:w-1}} (x_{w-1}\\vert y_{1:w-1}^\\ast) d x_{w-1};\n\\end{aligned}\n```\n\n\n\n-   the filtering formula, gives the filtering distribution at time $t_w$ using the prediction distribution at time $t_w$:\n\n\n\n\n```{=tex}\n\\begin{aligned}\n  & f_{X_w\\vert Y_{1:w}} (x_w\\vert y_{1:w}^\\ast;\\theta) \\\\\n  & \\qquad = f_{X_w\\vert Y_w, Y_{1:w-1}} (x_w \\vert y_w^\\ast, y_{1:w-1}^\\ast;\\theta) \\\\\n  & \\qquad = \\dfrac{f_{Y_w\\vert X_w}(y_w^\\ast\\vert x_w;\\theta)\\,f_{X_w\\vert Y_{1:w-1}}(x_w\\vert y_{1:w-1}^\\ast;\\theta)}{\\int f_{Y_w\\vert X_w} (y_{w}^\\ast\\vert u_w;\\theta)\\,f_{X_w\\vert Y_{1:w-1}}(u_w\\vert y_{1:w-1}^\\ast;\\theta)} d u_w.\n\\end{aligned}\n```\n\n\n\nWe now can have a one-step particle filtering as follows:\n\n1.  Suppose we have a set of $J$ samples for the state $X_{w-1}^F, j=1,\\dots,J$ drawn from the filtering distribution at time $t_{w-1}$;\n\n2.  We then can obtain a set of samples $X_{w, j}^P$ drawn from the prediction distribution at time $t_w$ by simulating the process model: $$\n    X_{w,j}^P \\sim \\mathrm{process}(X_{w-1}^F,\\theta), j=1,\\dots,J;\n    $$\n\n3.  Given the set of samples for the state at time $t_w$, we can obtain the filtering distribution by *resampling* from this set $\\{X_{w,j}^P, j=1,\\dots,J\\}$ with weights (the density of the measurement model): $$\n    \\nu_{w,j} = f_{Y_w\\vert X_w}(y_w^\\ast\\vert X_{w,j}^P;\\theta);\n    $$\n\n4.  The Monte Carlo principle (law of large numbers) gives the approximated likelihood: $$\n    \\hat{\\mathcal{L}}_w(\\theta) \\approx \\frac{1}{J} \\sum_j f_{Y_w\\vert X_w} (y_{w}^\\ast \\vert X_{w,j}^P;\\theta)\n    $$ since $X_{w,j}^P$ is approximately a draw from $f_{X_w\\vert Y_{1:w-1}} (x_w \\vert y_{1:w-1}^\\ast;\\theta)$;\n\nThen we can iterate through the end of the time and the approximated full log-likelihood is given by: $$\n\\ell(\\theta) = \\log \\mathcal{L}(\\theta) = \\sum_w \\log \\mathcal{L}_{w}(\\theta) \\approx \\sum_w \\hat{\\mathcal{L}}_{w}(\\theta).\n$$\n\n## Paticle filtering in `pomp`\n\nWe can now try to compute the log-likelihood in `pomp` using the function `pfilter` with $J=5000$:\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsird_measle |>\n  pfilter(\n    params = c(\n      Beta = 7.5, gamma = .5, rho = .5, k = 10,\n      S0 = 1140, I0 = 1, R0 = 36860\n    ),\n    Np=5000\n  ) -> pf\nlogLik(pf)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] -Inf\n```\n\n\n:::\n:::\n\n\n\n\nWe can try another set of parameters:\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsird_measle |>\n  pfilter(\n    params = c(\n      Beta = 15, gamma = .5, rho = .5, k = 10,\n      S0 = 2280, I0 = 1, R0 = 35720\n    ),\n    Np=5000\n  ) |>\n  logLik()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] -136.1367\n```\n\n\n:::\n:::\n\n\n\n\nUsing parallel computation, we can see the average log-likelihood and the variation in log-likelihoods:\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nregisterDoParallel(cores=detectCores()-1)\nforeach (\n  i=1:10, .combine=c, .options.future=list(seed=652643293)\n  ) %dopar% {\n  sird_measle |>\n  pfilter(\n    params = c(\n      Beta = 15, gamma = .5, rho = .5, k = 10,\n      S0 = 2280, I0 = 1, R0 = 35720\n    ),\n    Np=5000\n  )\n} -> pf\nlogLik(pf) -> ll\nlogmeanexp(ll,se=TRUE)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n       est         se \n-130.43782    2.57487 \n```\n\n\n:::\n:::\n\n\n\n\n## Slice likelihood: changing one specific parameter\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsird_measle |>\n  pomp(\n    params =  c(\n      Beta = 15, gamma = .5, rho = .5, k = 10,\n      S0 = 2280, I0 = 1, R0 = 35720\n    )\n  ) -> sird_measle\n\nslice_design(\n  center=coef(sird_measle),\n  Beta = rep(seq(from=5,to=30,length=40),each=3),\n  gamma = rep(seq(from=0.2,to=2,length=40),each=3)\n) -> params_slice\n\nhead(params_slice)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n      Beta gamma rho  k   S0 I0    R0 slice\n1 5.000000   0.5 0.5 10 2280  1 35720  Beta\n2 5.000000   0.5 0.5 10 2280  1 35720  Beta\n3 5.000000   0.5 0.5 10 2280  1 35720  Beta\n4 5.641026   0.5 0.5 10 2280  1 35720  Beta\n5 5.641026   0.5 0.5 10 2280  1 35720  Beta\n6 5.641026   0.5 0.5 10 2280  1 35720  Beta\n```\n\n\n:::\n\n```{.r .cell-code}\nsummary(params_slice)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n      Beta           gamma            rho            k            S0      \n Min.   : 5.00   Min.   :0.200   Min.   :0.5   Min.   :10   Min.   :2280  \n 1st Qu.:15.00   1st Qu.:0.500   1st Qu.:0.5   1st Qu.:10   1st Qu.:2280  \n Median :15.00   Median :0.500   Median :0.5   Median :10   Median :2280  \n Mean   :16.25   Mean   :0.800   Mean   :0.5   Mean   :10   Mean   :2280  \n 3rd Qu.:17.34   3rd Qu.:1.088   3rd Qu.:0.5   3rd Qu.:10   3rd Qu.:2280  \n Max.   :30.00   Max.   :2.000   Max.   :0.5   Max.   :10   Max.   :2280  \n       I0          R0          slice    \n Min.   :1   Min.   :35720   Beta :120  \n 1st Qu.:1   1st Qu.:35720   gamma:120  \n Median :1   Median :35720              \n Mean   :1   Mean   :35720              \n 3rd Qu.:1   3rd Qu.:35720              \n Max.   :1   Max.   :35720              \n```\n\n\n:::\n:::\n\n\n\n\nNow we can run the computations of likelihood at different combinations of parameters and visualize:\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nforeach (\n  theta=iter(params_slice,\"row\"), .combine=rbind, .options.future=list(seed=108028909)\n  ) %dopar% {\n    sird_measle |> pfilter(params=theta,Np=5000) -> pf\n    theta$loglik <- logLik(pf)\n    theta\n} -> llks_slice\n\nsummary(llks_slice)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n      Beta           gamma            rho            k            S0      \n Min.   : 5.00   Min.   :0.200   Min.   :0.5   Min.   :10   Min.   :2280  \n 1st Qu.:15.00   1st Qu.:0.500   1st Qu.:0.5   1st Qu.:10   1st Qu.:2280  \n Median :15.00   Median :0.500   Median :0.5   Median :10   Median :2280  \n Mean   :16.25   Mean   :0.800   Mean   :0.5   Mean   :10   Mean   :2280  \n 3rd Qu.:17.34   3rd Qu.:1.088   3rd Qu.:0.5   3rd Qu.:10   3rd Qu.:2280  \n Max.   :30.00   Max.   :2.000   Max.   :0.5   Max.   :10   Max.   :2280  \n       I0          R0          slice         loglik      \n Min.   :1   Min.   :35720   Beta :120   Min.   :-462.5  \n 1st Qu.:1   1st Qu.:35720   gamma:120   1st Qu.:-176.8  \n Median :1   Median :35720               Median :-136.4  \n Mean   :1   Mean   :35720               Mean   :-164.0  \n 3rd Qu.:1   3rd Qu.:35720               3rd Qu.:-129.7  \n Max.   :1   Max.   :35720               Max.   :-111.8  \n```\n\n\n:::\n\n```{.r .cell-code}\nllks_slice |>\n  pivot_longer(c(Beta,gamma)) |>\n  filter(name==slice) |>\n  ggplot(aes(x=value,y=loglik,color=name))+\n  geom_point()+\n  facet_grid(~name,scales=\"free_x\")+\n  guides(color=\"none\")+\n  labs(x=\"parameter value\",color=\"\")\n```\n\n::: {.cell-output-display}\n![](likelihood_files/figure-html/slice-llk-viz-1.png){width=672}\n:::\n:::\n",
    "supporting": [
      "likelihood_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}