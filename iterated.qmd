---
title: "Maximum likelihood estimation and iterated filtering"
author: "Qianying (Ruby) Lin"
toc: true
engine: knitr
filters:
  - pseudocode

pseudocode:
  caption-prefix: "Algorithm"
  reference-prefix: "Algorithm"
  caption-number: true
---

## Summary

-   Model implementation in `pomp`

-   Creating a `pomp` object with different componenets

-   Computing likelihood using `pfilter`

## Goal: estimating the parameters

We are interested in estimating the parameters $\theta = (\beta, \gamma, \rho, k, S_0, I_0, R_0)$ of the SIR model, given the weekly reported cases. 
The likelihood-based inference is to find the set of parameters that maximize the likelihood of the data, i.e., the probability of observing the data given the parameters.
Now we break down this problem into two steps:

- compute the likelihood of the data given the parameters;

- find the set of parameters that maximize the likelihood.

This session will focus on the second step and we will use the `mif2` function in `pomp` to compute the maximum likelihood estimate (MLE) of the parameters.

## An iterated filtering algorithm (IF2)

Since we already learn how to compute the likelihood of the data given the parameters using Sequential Monte Carlo (SMC) method, implemented in `pfilter` function in `pomp`, now we would like to explore the space of parameters to find the set of parameters that maximize the likelihood.

The iterated filtering algorithm (IF2) consists of multiple rounds of iterations, call it $M$. 
In each round of iteration, we have $J$ particles, carrying out with the parameter vector.
Assume that we have data $y^\ast_{1:W}$.

We can consider this procedure as a two-layer process:

1. **Outer loop**: run through $M$ rounds of iterations: 

- at the beginning of each round $m$, the particles resulted in the last time step from the previous round $m-1$ are recycled, with a random walk perturbation; 

- the particles run through the inner loop to update the weights and the parameters are updated based on the likelihood of the data given the parameters.

2. **Inner loop**: run through $W$ time steps:

- at the beginning of each time step $w$, the particles are sampled from those in the last time step $w-1$, with a random walk perturbation;

- the current state at time $w$ are updated based on the stochastic compartmental model (e.g., SIR model);

- the weights are updated based on the likelihood of the data given the parameters;

- the particles are resampled based on the weights.

![**Iterated filtering diagram**.](if2.png){width="500"}

### Analogy with evolutiion by natural selection

- The parameters characterize the _genotype_.

- The swarm of particles is a _population_.

- The likelihood, a measure of the compatibility between the parameters and the data, is the analogue of _fitness_.

- Each successive observation is a new _generation_.

- Since particles reproduce in each generation in proportion to their likelihood, the particle filter acts like _natural selection_.

- The artificial perturbations augment the "genetic" variance and therefore correspond to _mutation_.

- IF2 increases the _fitness_ of the population of particles.

- However, because our scientific interest focuses on the model without the artificial perturbations, we decrease the intensity of the latter with successive iterations.


### IF2 pseudocode

**Input**:

- simulators for $f_{X_0}(x_0;\theta)$ and $f_{X_w\vert X_{w-1}}(x_w\vert x_{w-1};\theta)$;

- evaluator for $f_{Y_w\vert X_w}(y_w\vert x_w;\theta)$;

- data, $y^\ast_{1:W}$;

**Algorithm parameters and `mif2` arguments**

- number of iterations, `Nmif`=$M$

- number of particles, `Np`=$J$

- initial parameter vector swarm, `params`=$\left\{\Theta_j^0, j=1,\dots,J\right\}$

- random walk standard deviation for each parameter within the vector, `rw.sd`, such that a diagonal variance matrix $V_w$ can be constructed

- cooling fraction every 50 iterations, `cooling.fraction.50` = $a$

**Output**

- final parameter vector swarm after $M$ iterations, $\left\{\Theta_j^M, j=1,\dots,J\right\}$

**Procedure**

```pseudocode
#| html-indent-size: "1.2em"
#| html-comment-delimiter: "//"
#| html-line-number: true
#| html-line-number-punc: ":"
#| html-no-end: false
#| pdf-placement: "htb!"
#| pdf-line-number: true

\begin{algorithm}
\caption{Iterated filtering algorithm (IF2)}
\begin{algorithmic}
\For{$m=1$ to $M$}
  \State $\Theta_{0,j}^{F,m} \sim \mathrm{Normal}(\Theta_j^{m-1}, V_0\,a^{2m/50})$ for $j=1$ to $J$
  \State $X_{0,j}^{F,m} \sim f_{X_0}(x_0;\Theta_{0,j}^{F,m})$ for $j=1$ to $J$
  \For{$w=1$ to $W$}
    \State $\Theta_{w,j}^{P,m} \sim \mathrm{Normal}(\Theta_{w-1,j}^{F,m}, V_n\, a^{2m/50})$ for $j=1$ to $J$
    \State $X_{n,j}^{P,m} \sim f_{X_w\vert X_{w-1}}(x_w\vert X_{w-1}^{F,m};\Theta_{w,j}^{P,m})$ for $j=1$ to $J$
    \State $\omega_{n,j}^{P,m} = f_{Y_w\vert X_w}(y_w^\ast\vert X_{w,j}^{P,m};\Theta_{w,j}^{P,m})$ for $j=1$ to $J$
    \State Draw $k_{1:J}$ with $P[k_j=i]=\omega_{w,j}^m/\sum_{u=1}^J\,\omega_{w,u}$
    \State $\Theta_{w,j}^{F,m} = \Theta_{w,k_j}^{P,m}$ and $X_{w,j}^{F,m}=X_{w,k_j}^{P,m}$ for $j=1$ to $J$
  \EndFor
  \State Set $\Theta_j^m = \Theta_{W,j}^{F,m}$ for $j=1$ to $J$
\EndFor
\end{algorithmic}
\end{algorithm}
```

### Particle filter in `pomp` for the Consett measles outbreak

- Recap one wave of the Consett measles outbreak:

```{r recap-data}
source("model.R")
dat_meas |>
  ggplot(aes(x=week, y=reports)) +
  geom_line() +
  geom_point() +
  theme_bw()
```

- Test the particle filter with pre-specified parameters:

```{r test-pfilter}
#| cache: true
sird_measle |>
  pfilter(
    params =  c(
      Beta = 15, gamma = .5, rho = .5, k = 10,
      S0 = 2280, I0 = 1, R0 = 35720
    ),
    Np=1000
  ) -> pf

plot(pf)
```

The above plot shows the data (`reports`), along with the _effective sample size_ (ESS) of the particle filter (`ess`) and the log-likelihood of each observation conditional on the preceding ones (`cond.logLik`).
The ESS is the equivalent number of independent particles. 
In this case, the ESS appears to be everywhere adequate.

- setting up the estimation problem

Assume the initial population sizes are known, that is $S_0=2280, I_0 = 1, R_0 = 35720$.
We also assume that the infectious period is 2 weeks, i.e., $\gamma = 0.5$.
We would also like to fix the over-dispersion parameter $k=10$.
We can relax these assumptions later.
By fixing these parameters, we can constrain the parameter space and try our best to avoid un-identifiability.
We can then proceed to estimate the transmission rate $\beta$ and the reporting ratio $\rho$, with initial guesses $\beta=15$ and $\rho=0.5$.

```{r pf}
#| cache: true
fixed_params <- c(
  S0 = 2280, I0 = 1, R0 = 35720,
  gamma = .5, k = 10
)

coef(sird_measle, names(fixed_params)) <- fixed_params

library(foreach)
library(doParallel)
registerDoParallel(cores=detectCores()-1)
foreach(i=1:10,.combine=c, .options.future=list(seed=TRUE)) %dopar% {
  sird_measle |> pfilter(Np=5000)
} -> pf
pf |> logLik() |> logmeanexp(se=TRUE) -> L_pf
L_pf
```
```{r store-results1}
#| cache: true
pf[[1]] |> coef() |> bind_rows() |>
  bind_cols(loglik=L_pf[1],loglik.se=L_pf[2]) |>
  write_csv("measles_params.csv")
```


## Local search of the likelihood surface

A "local search" means that we are looking for the maximum likelihood estimate (MLE) starting from ONE initial guess. 
Afterwards, by applying "local search" to a number of different initial guesses covering the reasonable parameter space, we can explore the likelihood surface and find the global MLE.

A few parameters for iterated filtering need to be specified:

- the random walk standard deviation, `rw.sd` for each parameter to be estimated

- the cooling fraction every 50 iterations, `cooling.fraction.50`

- the transmission rate $\beta$ will estimated in log scale (since $\beta > 0$) and the reporting ratio $\rho$ in logistic scale (since $0 < \rho < 1$)

- the perturbation for $\beta$ and $\rho$ are both $0.02$ (a very common and efficient choice from experience)

- fix `cooling.fraction.50=0.5`, such that after 50 `mif2` iterations, the perturbation are reduced to half their original magnitudes


Now we can repeat the `mif2` algorithm for 20 times with the same initial guesses:

```{r local-mif2}
#| cache: true
foreach(i=1:20,.combine=c) %dopar% {
  sird_measle |>
    mif2(
      Np=2000, Nmif=50,
      cooling.fraction.50=0.5,
      rw.sd=rw_sd(Beta=0.02, rho=0.02),
      partrans=parameter_trans(log="Beta",logit="rho"),
      paramnames=c("Beta","rho")
    )
} -> mifs_local
```

### Iterated filtering diagnostics

```{r mif2-diagnostics}
mifs_local |>
  traces() |>
  melt() |>
  ggplot(aes(x=iteration,y=value,group=.L1,color=factor(.L1)))+
    geom_line() +
    guides(color="none") +
    facet_wrap(~name,scales="free_y")
```

From the traces of the parameters, we can see that the iterated filtering algorithm has converged to a stable region after about 10 iterations.
We can conclude that:

- our initial guesses are reasonable;

- this Monte Carlo algorithm is stochastic and the results may vary from run to run.

### Estimating the likelihood

The diagnostic plot shows that the 20 replications of the iterated filtering algorithm have converged to a stable region after 50 iterations, and now we can compute the likelihood of the data given these 20 converged parameter vectors.

```{r mif2-likelihood}
bake("results_local.rds", {
  foreach(mf=mifs_local,.combine=rbind) %dopar% {
    evals <- replicate(10, logLik(pfilter(mf,Np=5000)))
    ll <- logmeanexp(evals,se=TRUE)
    mf |> coef() |> bind_rows() |>
    bind_cols(loglik=ll[1],loglik.se=ll[2])
  } -> results_local
}) -> results_local

pairs(~loglik+Beta+rho,data=results_local,pch=16)
```

The scatter plot shows a hint of a "ridge" in the likelihood surface. 
However, the sampling is as yet too sparse to give a clear picture.
There are things that we can try next:

- Increase the number of iterations, `Nmif`, to confirm the convergence

- Increase the number of particles, `Np`, to improve the accuracy of the estimates

- Try different initial guesses to explore the likelihood surface

```{r store-results2}
#| cache: true
read_csv("measles_params.csv") |>
  bind_rows(results_local) |>
  arrange(-loglik) |>
  write_csv("measles_params.csv")
```

## Global Search and MLE

To avoid confusion, we have the following terminology:

- initial guess: the initial parameter vector $\Theta^0_j$

- initial state: the initial state of population model $X_{0,j}$ at time $t_0$

For our measles model, a box containing reasonable parameter values might be $\beta\in(5,80)$ and $\rho\in(0.2,0.9)$.
Now we can build a group of 400 initial guesses to cover this box:
```{r initial-guess-group}
#| cache: true
set.seed(2062379496)
runif_design(
  lower=c(Beta=5,rho=0.2),
  upper=c(Beta=80,rho=0.9),
  nseq=400
) -> guesses

mf1 <- mifs_local[[1]]  # retrieve the local search as the model template
```

With the group of 400 initial guesses, we can now apply the iterated filtering algorithm to find the global MLE:

- the codes run ONE local search for each of 400 initial guesses

- each local search consists of two runs: the first run with 50 iterations, followed by the second with 100 iterations

- a general `pomp` behavior:

    - re-running a command on an object (i.e., `mif2` on `mf1`) created by the same command perserves the algorithmic arguments
    
    - running `mif2` on the results of a `mif2` computation re-runs IF2 from the endpoint of the first run
    
    - the second/subsequent run will preserve all the algorithmic parameters from the previous run by default; here we overrode the default choice of `Nmif` from 50 to 100
    
- following the local search, the particle filter is used to evaluate the likelihood as the previous session


```{r global-mif2}
bake("results_global.rds", {
  foreach(guess=iter(guesses,"row"), .combine=rbind) %dopar% {
    # run the iterated filtering algorithm
    mf1 |>
      mif2(params=c(guess,fixed_params)) |>
      mif2(Nmif=100) -> mf
    # compute the likelihood and se from 10 replications
    replicate(
      10,
      mf |> pfilter(Np=5000) |> logLik()
    ) |>
    logmeanexp(se=TRUE) -> ll
    # store the results
    mf |> coef() |> bind_rows() |>
      bind_cols(loglik=ll[1],loglik.se=ll[2])
  } -> results_global
}) -> results_global

results_global |> filter(loglik == max(loglik))
```

The best result of this search had a likelihood of -105.704 with a standard error of ~0.027. Now we can update the database:

```{r store-results3}
read_csv("measles_params.csv") |>
  bind_rows(results_global) |>
  arrange(-loglik) |>
  write_csv("measles_params.csv")
```

We attempt to visualize the global geometry of the likelihood surface using a scatterplot matrix. In particular, here we plot both the initial guesses (grey) and the IF2 estimates (red).

```{r view-results_global}
read_csv("measles_params.csv") |>
  filter(loglik>max(loglik)-50) |>
  bind_rows(guesses) |>
  mutate(type=if_else(is.na(loglik),"guess","result")) |>
  arrange(type) -> all
  
pairs(~loglik+Beta+rho, data=all, pch=16, cex=0.3, col=ifelse(all$type=="guess",grey(.5),"red"))
```
### a poo rman's profile

```{r poorman-profile}
all |>
  filter(type=="result") |>
  filter(loglik>max(loglik)-10) |>
  ggplot(aes(x=Beta,y=loglik))+
  geom_point()+
  labs(
    x=expression(Beta),
    title="poor man’s profile likelihood"
  )
```
