---
title: "Likelihood"
author: "Qianying (Ruby) Lin"
toc: true
format:
  html:
    embed-resources: true
    code-background: true
engine: knitr
---

## Summary

-   Implementing in `pomp`

-   Creating a `pomp` object

-   Computing likelihood

```{r loading}
options(warning = FALSE)
library(dplyr)
library(tidyr)
library(readr)
library(iterators)
library(pomp)
library(ggplot2)
library(foreach)
library(doParallel)
```

## Recap: A stochastic SIR model in the POMP framework

We define the **state process** as a stochastic SIR model, $X_t=(S_t, I_t, R_t, D_t)$ where $D_t$ is the weekly cumulative diagnoses, getting reset to 0 every 7 days. The number of infections $\Delta N_{SI}$ and recoveries $\Delta N_{IR}$ are random variables, following Binomial distributions:

```{=tex}
\begin{aligned}
  \Delta N_{SI} &\sim \mathrm{Binomial}\left(S, 1-e^{-\beta\frac{I}{N}\Delta t}\right) \\
  \Delta N_{IR} &\sim \mathrm{Binomial}\left(I, 1-e^{-\gamma \Delta t}\right)
\end{aligned}
```
where $\beta$ is the transmission rate and $\gamma$ is the recovery rate.

We define a weekly **measurement model** which follows a Negative Binomial distribution with mean $\rho D_t$ and the dispersion parameter $k$:

$$
\textrm{reports}_t \sim \textrm{NegBin}(\rho D_t, k),
$$

where $\rho$ is the reporting ratio and $0 \leq \rho \leq 1$ .

## Implementing the SIR model in `pomp` package

### Example: the Consett measles outbreak

We first download the data and take a look at them:

```{r dat_measle}
read_csv(paste0("https://kingaa.github.io/sbied/stochsim/", 
  "Measles_Consett_1948.csv")) |>
  select(week,reports=cases) -> dat_meas

dat_meas |> head()

dat_meas |> summary()
```

and visualize:

```{r dat_measle_viz}
dat_meas |>
  ggplot(aes(x=week, y=reports)) +
  geom_line() +
  geom_point() +
  theme_bw()
```

### A stochastic SIR model for measles in `pomp`

To accelerate the computation, we would like to code the model in `C/C++` using `C snippets` in `pomp`. Now we can have our \*\*one-step transition\*\* function:

```{r sird_step_c}
sird_step <- Csnippet("
  double N = S + I + R;
  double dN_SI = rbinom(S, 1-exp(-Beta*I/N*dt));
  double dN_IR = rbinom(I, 1-exp(-gamma*dt));
  S -= dN_SI;
  I += dN_SI - dN_IR;
  R += dN_IR;
  D += dN_IR;
")
```

and the initialization and measurement model:

```{r sird_init_meas_c}
sird_rinit <- Csnippet("
  S = S0;
  I = I0;
  R = R0;
  D = 0;
")

sird_dmeas <- Csnippet("
  lik = dnbinom_mu(reports, k, rho*D, give_log);
")

sird_rmeas <- Csnippet("
  reports = rnbinom_mu(k, rho*D);
")
```

Now we can put the model together in `pomp` and simulate the model dynamics:

```{r sird_pomp}
dat_meas |>
  pomp(
    times="week",
    t0=0,
    rprocess = euler(sird_step, delta.t = 1/7),
    rinit = sird_rinit,
    rmeasure = sird_rmeas,
    dmeasure = sird_dmeas,
    accumvars = "D",
    statenames = c("S","I","R","D"),
    paramnames = c("Beta","gamma","rho","k","S0","I0","R0")
  ) -> sird_measle

```

After getting the components in place in `pomp`, we can simulate the dynamics with some parameters from literature or anything you want:

```{r sird_pomp_sim}
sird_measle |>
  simulate(
    params = c(
      Beta = 7.5, gamma = .5, rho = .5, k = 10,
      S0 = 1140, I0 = 1, R0 = 36860
    ),
    nsim = 100,
    format = "data.frame",
    include.data = TRUE
  ) -> sims_measle

sims_measle |> head()

summary(sims_measle)

sims_measle |>
  ggplot(aes(x=week, y=reports, group=.id,color=(.id=="data"))) +
  geom_line() +
  guides(color="none") +
  theme_bw()
```

Obviously, the proposed set of parameters doesn't seem to be right. We can play around with the parameters to see whether we can obtain better simulations.

### A SEIR model?

## Likelihood-based inference for POMP models

### Recap: the POMP structure

The POMP model consist of two components: the state process $X_t$ and the observations $Y_t$. The state process is Markovian, and transitions between states are driven by the **process model**, where we code it as `sird_step`; the observations are derived from the states, governed by a **measurement model** which we code in `sird_rmeas` for simulation and `sird_dmeas` for density computation.

![**POMP schematic**. In previous example, we define the states as $X_t = (S_t, I_t, R_t, D_t)$ and the process model as the SIR dynamics; the observations are the weekly reported cases and the measurement model is defined as a negative binomial distribution.](POMP2.png){width="500"}

### Likelihood of the stochastic SIR model for measles

It will be straight-forward to compute the log-likelihood of $\rho$ and $k$ of the data (i.e., weekly reports), given the underlying weekly cumulative diagnoses:

$$
\ell(\rho, k) = \sum_{w=1}^W\, \log p(report_w \vert D_w, \rho, k),
$$

where $W$ is the total number of reporting weeks and $p$ is the probability mass function of Negative Binomial distribution. Unfortunately, we don't have any knowledge of $D_t$ because it is part of the unobserved underlying stochastic SIR dynamics, which we can simulate.

Now, let's keep in mind the observations $Y_w$ is the weekly reported cases $\mathrm{report}_w$, and the unobserved states $X_t = (S_t, I_t, R_t, D_t)$. We denote the density of the process model as $f_{X_w\vert X_{w-1}}$, the density of the measurement model as $f_{Y_w\vert X_w}$, the initialization density $f_{X_0}$, and the set of parameters $\theta = (\beta, \gamma, \rho, k, S_0, I_0, R_0)$. The likelihood for a POMP model takes the form of an integral:

$$
\mathcal{L}(\theta) = \int f_{X_0}(x_0;\theta)\, \prod_{w=1}^W f_{Y_w\vert X_w} (y_w^\ast\vert x_w;\theta)\,f_{X_w\vert X_{w-1}}(x_w \vert x_w;\theta) d x_{0:W},
$$

where $y_w^\ast$ is the data (reported cases) in $w$-th week.

### Monte Carlo likelihood by direct simulation

The intuitive idea is to simulate the sequence of states at each week, denoted as $X_{0:W}$, and then the likelihood is given by

```{=tex}
\begin{aligned}
  y &= \int \prod_{w=1}^W\,f_{Y_w\vert X_w}(y_w^\ast\vert x_w;\theta) f_{X_{0:W}}(x_{0:w};\theta) dx_{0:W} \\
    &= \mathbb{E}\left[\prod_{w=1}^W\,f_{Y_w\vert X_w}(y_w^\ast\vert x_w;\theta)\right],
\end{aligned}
```
where the expectation is taken with $X_{0:W}\sim f_{X_{0:W}}(x_{0:W};\theta)$.

Using a **law of large numbers**, we can simulate a large set ($J$) of sequences $\{X_{0:W}^j, j=1,\dots,J\}$ and take the average of this Monte Carlo sample, which converges to the expectation: $$
\mathcal{L}(\theta) \approx \frac{1}{J}\sum_{j=1}^J\,\prod_{w=1}^W\,f_{Y_w\vert X_w}(y_w^\ast\vert x_w;\theta).
$$

However, things are not that straight-forward and intuitive because this naive approach scales poorly with dimension, in other words, it requires a Monte Carlo effort ($J$) that scales exponentially with the length of the time series, and so is unfeasible on anything but a short data set. Another aspect to consider is that, the underlying process is stochastic, the simulations therefore diverge mostly, many of which don't align with the observed data thus are useless for parameter estimation.

### Sequential Monte Carlo: the particle filter

The idea of Sequential Monte Carlo (SMC) is that, instead of computing and evaluating the whole sequence of states at once, we evaluate the state at every time step. At current state, we evaluate whether it aligns with the data: if yes, then we keep it and continue to simulate the next state; if no, we just drop it. This procedure is called "importance sampling".

We therefore can factorize the likelihood:

```{=tex}
\begin{aligned}
  \mathcal{L}(\theta) &= f_{Y_{1:W}}(y_{1:W}^\ast;\theta) = \prod_{w=1}^W\,f_{Y_w\vert Y_{1:w-1}}(y_w^\ast\vert y^\ast_{1:w-1};\theta) \\
  &= \prod_{w=1}^W\,\int f_{Y_w\vert X_w}(y_w^\ast \vert x_w;\theta) f_{X_w\vert Y_{1:w-1}}(x_w\vert y_{1:w-1}^\ast) d x_w,
\end{aligned}
```
where obviously $f_{X_1\vert Y_{1:0}} = f_{X_1}$. With the Markov property and the Baye's theorem, we can break the factorization into two steps, **predition** and **filtering**:

-   the prediction formula, gives the prediction at time $t_w$ conditioned on the filtering distribution at time $t_{w-1}$:

```{=tex}
\begin{aligned}
  & f_{X_w\vert Y_{1:w-1}}(x_w\vert y_{1:w-1}^\ast;\theta) \\
  & \qquad = \int f_{X_w\vert X_{w-1}}(x_w\vert x_{w-1};\theta) f_{X_{w-1}\vert Y_{1:w-1}} (x_{w-1}\vert y_{1:w-1}^\ast) d x_{w-1};
\end{aligned}
```
-   the filtering formula, gives the filtering distribution at time $t_w$ using the prediction distribution at time $t_w$:

```{=tex}
\begin{aligned}
  & f_{X_w\vert Y_{1:w}} (x_w\vert y_{1:w}^\ast;\theta) \\
  & \qquad = f_{X_w\vert Y_w, Y_{1:w-1}} (x_w \vert y_w^\ast, y_{1:w-1}^\ast;\theta) \\
  & \qquad = \dfrac{f_{Y_w\vert X_w}(y_w^\ast\vert x_w;\theta)\,f_{X_w\vert Y_{1:w-1}}(x_w\vert y_{1:w-1}^\ast;\theta)}{\int f_{Y_w\vert X_w} (y_{w}^\ast\vert u_w;\theta)\,f_{X_w\vert Y_{1:w-1}}(u_w\vert y_{1:w-1}^\ast;\theta)} d u_w.
\end{aligned}
```
We now can have a one-step particle filtering as follows:

1.  Suppose we have a set of $J$ samples for the state $X_{w-1}^F, j=1,\dots,J$ drawn from the filtering distribution at time $t_{w-1}$;

2.  We then can obtain a set of samples $X_{w, j}^P$ drawn from the prediction distribution at time $t_w$ by simulating the process model: $$
    X_{w,j}^P \sim \mathrm{process}(X_{w-1}^F,\theta), j=1,\dots,J;
    $$

3.  Given the set of samples for the state at time $t_w$, we can obtain the filtering distribution by *resampling* from this set $\{X_{w,j}^P, j=1,\dots,J\}$ with weights (the density of the measurement model): $$
    \nu_{w,j} = f_{Y_w\vert X_w}(y_w^\ast\vert X_{w,j}^P;\theta);
    $$

4.  The Monte Carlo principle (law of large numbers) gives the approximated likelihood: $$
    \hat{\mathcal{L}}_w(\theta) \approx \frac{1}{J} \sum_j f_{Y_w\vert X_w} (y_{w}^\ast \vert X_{w,j}^P;\theta)
    $$ since $X_{w,j}^P$ is approximately a draw from $f_{X_w\vert Y_{1:w-1}} (x_w \vert y_{1:w-1}^\ast;\theta)$;

Then we can iterate through the end of the time and the approximated full log-likelihood is given by: $$
\ell(\theta) = \log \mathcal{L}(\theta) = \sum_w \log \mathcal{L}_{w}(\theta) \approx \sum_w \hat{\mathcal{L}}_{w}(\theta).
$$

## Paticle filtering in `pomp`

We can now try to compute the log-likelihood in `pomp` using the function `pfilter` with $J=5000$:

```{r pfilter}
sird_measle |>
  pfilter(
    params = c(
      Beta = 7.5, gamma = .5, rho = .5, k = 10,
      S0 = 1140, I0 = 1, R0 = 36860
    ),
    Np=5000
  ) -> pf
logLik(pf)
```

We can try another set of parameters:

```{r pf-2}
sird_measle |>
  pfilter(
    params = c(
      Beta = 15, gamma = .5, rho = .5, k = 10,
      S0 = 2280, I0 = 1, R0 = 35720
    ),
    Np=5000
  ) |>
  logLik()
```

Using parallel computation, we can see the average log-likelihood and the variation in log-likelihoods:

```{r parallel-pf}
registerDoParallel(cores=detectCores()-1)
foreach (
  i=1:10, .combine=c, .options.future=list(seed=652643293)
  ) %dopar% {
  sird_measle |>
  pfilter(
    params = c(
      Beta = 15, gamma = .5, rho = .5, k = 10,
      S0 = 2280, I0 = 1, R0 = 35720
    ),
    Np=5000
  )
} -> pf
logLik(pf) -> ll
logmeanexp(ll,se=TRUE)
```

## Slice likelihood: changing one specific parameter

```{r slice-llk}
sird_measle |>
  pomp(
    params =  c(
      Beta = 15, gamma = .5, rho = .5, k = 10,
      S0 = 2280, I0 = 1, R0 = 35720
    )
  ) -> sird_measle

slice_design(
  center=coef(sird_measle),
  Beta = rep(seq(from=5,to=30,length=40),each=3),
  gamma = rep(seq(from=0.2,to=2,length=40),each=3)
) -> params_slice

head(params_slice)

summary(params_slice)
```

Now we can run the computations of likelihood at different combinations of parameters and visualize:

```{r slice-llk-viz}
foreach (
  theta=iter(params_slice,"row"), .combine=rbind, .options.future=list(seed=108028909)
  ) %dopar% {
    sird_measle |> pfilter(params=theta,Np=5000) -> pf
    theta$loglik <- logLik(pf)
    theta
} -> llks_slice

summary(llks_slice)

llks_slice |>
  pivot_longer(c(Beta,gamma)) |>
  filter(name==slice) |>
  ggplot(aes(x=value,y=loglik,color=name))+
  geom_point()+
  facet_grid(~name,scales="free_x")+
  guides(color="none")+
  labs(x="parameter value",color="")
```
