{
  "hash": "1bc214185692e1bcbe793d5ac48f7cd5",
  "result": {
    "markdown": "---\ntitle: \"Lesson 4: Iterated filtering: principles and practice\"\nauthor:\n  - Qianying (Ruby) Lin\n  - Spencer J. Fox\n  - Zian (Larry) Zhuang\nformat: \n  beamer:\n    classoption: \"t\"\n    # fontsize: \"11pt\"\n    link-citations: true\n    keep_tex: true\n    slide_level: 2\n    section-titles: false\n    aspectratio: 169\n    include-in-header: \"../_includes/header.tex\"\n    # beameroptions:\n    #   - setbeamertemplate: \"footline[frame number]\"\n    #   - setbeamertemplate: \"navigation symbols{}\"\n    header-includes: |\n       \\setbeamertemplate{navigation symbols}{}\n       \\setbeamertemplate{footline}[page number]\n    hyperrefoptions:\n      - linktoc=all\n  html:\n    toc: true\n  pdf:\n    toc: true\neditor_options: \n  chunk_output_type: console\nbibliography: [\"../sbied.bib\"]\n---\n\n\n\n::: {.cell layout-align=\"center\"}\n\n:::\n\n\n\n# Introduction\n\n## Introduction\n\n-   This tutorial covers likelihood estimation via the method of iterated filtering.\n-   It presupposes familiarity with building partially observed Markov process (POMP) objects in the `R` package pomp [@King2016].\n-   This tutorial follows on from the [topic of particle filtering](../pfilter/main.pdf) (also known as sequential Monte Carlo) via `pfilter` in pomp.\n\n## Objectives\n\n1.  To review the available options for inference on POMP models, to put iterated filtering in context.\n2.  To understand how iterated filtering algorithms carry out repeated particle filtering operations, with randomly perturbed parameter values, in order to maximize the likelihood.\n3.  To gain experience carrying out statistical investigations using iterated filtering in a relatively simple situation: fitting an SIR model to data from a measles outbreak.\n\n# Classification of statistical methods for POMP models\n\n## Classification of statistical methods for POMP models {.allowframebreaks}\n\n-   Many, many statistical methods have been proposed for inference on POMP models [@He2010,@King2016].\n\n-   The volume of research indicates both the importance and the difficulty of the problem.\n\n-   Let's start by considering three criteria to categorize inference methods:\n\n    -   the plug-and-play property\n    -   full-information or feature-based\n    -   frequentist or Bayesian\n\n# The plug-and-play property\n\n## Plug-and-play (also called simulation-based) methods {.allowframebreaks}\n\n-   Inference methodology that calls `rprocess` but not `dprocess` is said to be *plug-and-play*. All popular modern Monte Carlo methods for POMP models are in this category.\n-   \"Simulation-based\" is equivalent to \"plug-and-play\".\n-   Historically, simulation-based meant simulating forward from initial conditions to the end of the time series.\n-   However, particle filtering methods instead consider each observation interval sequentially. They carry out multiple, carefully selected, simulations over each interval.\n\n\\framebreak\n\n-   Plug-and-play methods can call `dmeasure`. A method that uses only `rprocess` and `rmeasure` is called \"doubly plug-and-play\".\n-   Two *non-plug-and-play* methods---expectation-maximization (EM) and Markov chain Monte Carlo (MCMC)---have theoretical convergence problems for nonlinear POMP models. The failures of these two workhorses of statistical computation have prompted development of alternative methodologies.\n\n# Full information vs.\\~feature-based methods\n\n## Full-information and feature-based methods {.allowframebreaks}\n\n-   *Full-information* methods are defined to be those based on the likelihood function for the full data (i.e., likelihood-based frequentist inference and Bayesian inference).\n-   *Feature-based* methods either consider a summary statistic (a function of the data) or work with an an alternative to the likelihood.\n-   Asymptotically, full-information methods are statistically efficient and feature-based methods are not.\n-   In some cases, loss of statistical efficiency might be an acceptable tradeoff for advantages in computational efficiency.\n\n\\framebreak\n\n-   However:\n\n    -   Good low-dimensional summary statistics can be hard to find.\n    -   When using statistically inefficient methods, it can be hard to know how much information you are losing.\n    -   Intuition and scientific reasoning can be inadequate tools to derive informative low-dimensional summary statistics [@Shrestha2011,Ionides2011a].\n\n# Bayesian vs.\\~frequentist approaches\n\n## Bayesian and frequentist methods {.allowframebreaks}\n\n-   Recently, plug-and-play Bayesian methods have been discovered:\n\n    -   particle Markov chain Monte Carlo (PMCMC) [@Andrieu2010].\n    -   approximate Bayesian computation (ABC) [@Toni2009].\n\n-   Prior belief specification is both the strength and weakness of Bayesian methodology:\n\n-   The likelihood surface for nonlinear POMP models often contains nonlinear ridges and variations in curvature.\n\n\\framebreak\n\n-   These situations bring into question the appropriateness of independent priors derived from expert opinion on marginal distributions of parameters.\n-   They also are problematic for specification of \"flat\" or \"uninformative\" prior beliefs.\n-   Expert opinion can be treated as data for non-Bayesian analysis. However, our primary task is to identify the information in the data under investigation, so it can be helpful to use methods that do not force us to make our conclusions dependent on quantification of prior beliefs.\n\n## Summary\n\n\n\n```{=tex}\n\\begin{center}\n  \\includegraphics[height=6cm]{../graphics/lec4 table.png}\n\\end{center}\n```\n\n\n# Iterated filtering in theory\n\n## Full-information, plug-and-play, frequentist methods {.allowframebreaks}\n\n-   Iterated filtering methods [@Ionides2006,@Ionides2015] are the only currently available, full-information, plug-and-play, frequentist methods for POMP models.\n-   Iterated filtering methods have been shown to solve likelihood-based inference problems for epidemiological situations which are computationally intractable for available Bayesian methodology [@Ionides2015].\n\n## An iterated filtering algorithm (IF2) {.allowframebreaks}\n\nWe focus on the IF2 algorithm of @Ionides2015. In this algorithm:\n\n-   Each iteration consists of a particle filter, carried out with the parameter vector, for each particle, doing a random walk.\n-   At the end of the time series, the collection of parameter vectors is recycled as starting parameters for the next iteration.\n-   The random-walk variance decreases at each iteration.\n\nIn theory, this procedure converges toward the region of parameter space maximizing the maximum likelihood.\\\nIn practice, we can test this claim on examples.\n\n\\framebreak\n\n\n\n```{=tex}\n\\begin{center}\n  \\includegraphics[height=8cm]{../graphics/lec4 pf.png}\n\\end{center}\n```\n\n\n## IF2 algorithm pseudocode {.allowframebreaks}\n\n**Input:**\n\n-   simulators for $f_{X_0}(x_0;\\theta)$ and $f_{X_n|X_{n-1}}(x_n| x_{n-1}; \\theta)$\n-   evaluator for $f_{Y_n|X_n}(y_n| x_n;\\theta)$\n-   data, $y^*_{1:N}$\n\n\\textbf{Algorithmic parameters and corresponding \\texttt{mif2} arguments:}\n\n-   number of iterations, \\texttt{Nmif} = $M$\n-   number of particles, \\texttt{Np} = $J$\n-   initial parameter swarm, \\texttt{params} = $\\{\\Theta^0_j, j=1,\\dots,J\\}$\n-   random walk standard deviation for each parameter, \\texttt{rw.sd}, squared to construct a diagonal variance matrix, $V_n$\n-   cooling fraction in 50 iterations, \\texttt{cooling.fraction.50} = $a$\n\n**Output:**\n\n-   final parameter swarm, $\\{\\Theta^M_j, j=1,\\dots,J\\}$\n\n\\framebreak\n\n**Procedure:**\n\n1.  For $m$ in $1{:}M$\n2.  $\\qquad$ $\\Theta^{F,m}_{0,j}\\sim \\normal\\big(\\Theta^{m-1}_{j},V_0 \\, a^{2m/50}\\big)$ for $j$ in $1{:} J$\n3.  $\\qquad$ $X_{0,j}^{F,m}\\sim f_{X_0}(x_0 ; \\Theta^{F,m}_{0,j})$ for $j$ in $1{:} J$\n4.  $\\qquad$ For $n$ in $1{:} N$\n5.  $\\qquad\\qquad$ $\\Theta^{P,m}_{n,j}\\sim \\normal\\big(\\Theta^{F,m}_{n-1,j},V_n \\, a^{2m/50}\\big)$ for $j$ in $1{:} J$\n6.  $\\qquad\\qquad$ $X_{n,j}^{P,m}\\sim f_{X_n|X_{n-1}}(x_n | X^{F,m}_{n-1,j}; \\Theta^{P,m}_{n,j})$ for $j$ in $1{:} J$\n7.  $\\qquad\\qquad$ $w_{n,j}^m = f_{Y_n|X_n}(y^*_n| X_{n,j}^{P,m} ; \\Theta^{P,m}_{n,j})$ for $j$ in $1{:} J$\n8.  $\\qquad\\qquad$ Draw $k_{1{:}J}$ with $P[k_j=i]= w_{n,i}^m\\Big/\\sum_{u=1}^J w_{n,u}^m$\n9.  $\\qquad\\qquad$ $\\Theta^{F,m}_{n,j}=\\Theta^{P,m}_{n,k_j}$ and $X^{F,m}_{n,j}=X^{P,m}_{n,k_j}$ for $j$ in $1{:} J$\n10. $\\qquad$ End For\n11. $\\qquad$ Set $\\Theta^{m}_{j}=\\Theta^{F,m}_{N,j}$ for $j$ in $1{:} J$\n12. End For\n\n\\framebreak\n\n**Remarks:**\n\n-   The $N$ loop (lines 4 through 10) is a basic particle filter applied to a model with stochastic perturbations to the parameters.\n-   The $M$ loop repeats this particle filter with decreasing perturbations.\n-   The superscript $F$ in $\\Theta^{F,m}_{n,j}$ and $X^{F,m}_{n,j}$ denote solutions to the *filtering problem*, with the particles $j=1,\\dots,J$ providing a Monte Carlo representation of the conditional distribution at time $n$ given data $y^*_{1:n}$ for filtering iteration $m$.\n-   The superscript $P$ in $\\Theta^{P,m}_{n,j}$ and $X^{P,m}_{n,j}$ denote solutions to the *prediction problem*, with the particles $j=1,\\dots,J$ providing a Monte Carlo representation of the conditional distribution at time $n$ given data $y^*_{1:n-1}$ for filtering iteration $m$.\n-   The *weight* $w^m_{n,j}$ gives the likelihood of the data at time $n$ for particle $j$ in filtering iteration $m$.\n\n## Analogy with evolution by natural selection {.allowframebreaks}\n\n-   The parameters characterize the *genotype*.\n-   The swarm of particles is a *population*.\n-   The likelihood, a measure of the compatibility between the parameters and the data, is the analogue of *fitness*.\n-   Each successive observation is a new *generation*.\n-   Since particles reproduce in each generation in proportion to their likelihood, the particle filter acts like *natural selection*.\n-   The artificial perturbations augment the \"genetic\" variance and therefore correspond to *mutation*.\n-   IF2 increases the *fitness* of the population of particles.\n-   However, because our scientific interest focuses on the model without the artificial perturbations, we decrease the intensity of the latter with successive iterations.\n\n# Iterated filtering in practice\n\n# An example problem\n\n## Applying IF2 to the Consett measles outbreak\n\nLet us apply IF2 to our analysis of the Consett measles outbreak we began to examine in Lessons 2 and 3.\n\nThe following loads the data, pomp, and the stochastic SIR model we constructed there.\n\n\n\n::: {.cell layout-align=\"center\" hash='tmp//cache/model-construct_ae260fe823363a805f7bd0e8c665c3b6'}\n\n```{.r .cell-code}\nsource(\"model_measSIR.R\")\n```\n:::\n\n\n\n\\vspace{-20mm}\n\n\\hspace{-20mm}\n\n\n\n::: {.cell layout-align=\"center\" hash='tmp//cache/dataplot_b0c7cde8090f16fbf7d303e063e5a89f'}\n::: {.cell-output-display}\n![](tmp//figure/dataplot-1.png){fig-align='center' fig-pos='h!' width=1800 height=70%}\n:::\n:::\n\n\n\n<!-- ```{r dataplot,purl=FALSE,echo=FALSE,dpi=200,out.width=\"0.7\\\\textwidth\"} -->\n\n<!-- op <- par(mar=c(3,3,0.5,0.5)) -->\n\n<!-- measSIR |> plot() -->\n\n<!-- par(op) -->\n\n<!-- ``` -->\n\n<!-- In the earlier lessons, we demonstrated how to test the codes via simulation. -->\n\n<!-- ## Testing the codes: filtering  {.allowframebreaks} -->\n\n<!-- Before engaging in iterated filtering, it is a good idea to check that the basic particle filter is working since we can't iterate something unless we can run it once! -->\n\n<!-- The simulations above check the `rprocess` and `rmeasure` codes; -->\n\n<!-- the particle filter depends on the `rprocess` and `dmeasure` codes and so is a check of the latter. -->\n\n<!-- ```{r init_pfilter} -->\n\n<!-- measSIR |> -->\n\n<!--   pfilter(Np=1000) -> pf -->\n\n<!-- ``` -->\n\n<!-- ```{r init_pfilter_plot,purl=F,dpi=200,out.width=\"0.7\\\\textwidth\"} -->\n\n<!-- plot(pf) -->\n\n<!-- ``` -->\n\n<!-- \\vspace{-5mm} -->\n\n<!-- The above plot shows the data (`reports`), along with the *effective sample size* (ESS) of the particle filter (`ess`) and the log-likelihood of each observation conditional on the preceding ones (`cond.logLik`). -->\n\n<!-- The ESS is the equivalent number of independent particles. -->\n\n<!-- In this case, the ESS appears to be everywhere adequate. -->\n\n# Setting up the estimation problem\n\n## Setting up the estimation problem\n\nLet's assume that the population size, $N$, is known accurately. We'll fix that parameter.\n\nLet's revisit the assumption that the infectious period is 2 weeks, imagining that we have access to the results of household and clinical studies that have concluded that infected patients shed the virus for 3--4\\~da. We'll use these results to constrain the infectious period in our model to 3.5\\~da, i.e., $\\gamma=2~\\mathrm{wk}^{-1}$. We also fix $k=10$. Later, we can relax our assumptions.\n\n\n\n::: {.cell layout-align=\"center\" hash='tmp//cache/fixed_params_e7b756c0ef840b488de489fbca8e2420'}\n\n```{.r .cell-code}\nfixed_params <- c(N=38000, Gamma=2, k=10)\ncoef(measSIR,names(fixed_params)) <- fixed_params\ncoef(measSIR)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n   Beta   Gamma     Rho       k     Eta       N \n1.5e+01 2.0e+00 5.0e-01 1.0e+01 6.0e-02 3.8e+04 \n```\n:::\n:::\n\n\n\nWe proceed to estimate $\\beta$, $\\eta$, and $\\rho$.\n\n## Sanity check\n\nIn Lesson 3, we have introduced how to test the codes and the particle filter from three aspects. Now we can compare the simulations with the raw data using the proposed parameters:\n\n\n\n::: {.cell layout-align=\"center\" hash='tmp//cache/sanity_0ded17f681fa619c6e049b567512ea1a'}\n::: {.cell-output-display}\n![](tmp//figure/sanity-1.png){fig-align='center' fig-pos='h!' width=1800 height=70%}\n:::\n:::\n\n\n\n## Parallel computing\n\nIt will be helpful to parallelize most of the computations. [Lesson 3](https://kingaa.github.io/sbied/pfilter/) discusses how to accomplish this using foreach.\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nlibrary(foreach)\nlibrary(doFuture)\nplan(multisession)\n```\n:::\n\n\n\n## Running a particle filter {.allowframebreaks}\n\nWe proceed to carry out replicated particle filters at an initial guess of $\\beta=15$, $\\eta=0.06$, and $\\rho=0.5$.\n\n\n\n::: {.cell layout-align=\"center\" hash='tmp//cache/pf2_5d6969224095aacd4ea1a1304fb2c0dd'}\n\n```{.r .cell-code}\nforeach(i=1:10,.combine=c,\n  .options.future=list(seed=TRUE)) %dofuture% {\n  measSIR |> pfilter(Np=5000)\n} -> pf\npf |> logLik() |> logmeanexp(se=TRUE) -> L_pf\nL_pf\n```\n:::\n\n::: {.cell layout-align=\"center\" hash='tmp//cache/pf_5d8156810740e2f587c5644a9d743c77'}\n::: {.cell-output .cell-output-stdout}\n```\n         est           se \n-274.2571683    0.8639223 \n```\n:::\n:::\n\n\n\nIn 4.32 seconds, using 10 cores, we obtain an unbiased likelihood estimate of -274.3 with a Monte Carlo standard error of 0.86.\n\n## Building up a picture of the likelihood surface {.allowframebreaks}\n\n-   Given a model and a set of data, the likelihood surface is well defined, though it may be difficult to visualize.\n-   We can develop a progressively more complete picture of this surface by storing likelihood estimates whenever we compute them.\n-   It is a very good idea to set up a database within which to store the likelihood of every point for which we have an estimated likelihood.\n-   This will become larger and more complete as our parameter-space search goes on and will be a basis for a variety of explorations.\n\nAt this point, we've computed the likelihood at a single point. Let's store this point, together with the estimated likelihood and our estimate of the standard error on that likelihood, in a CSV file:\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\npf[[1]] |> coef() |> bind_rows() |>\n  bind_cols(loglik=L_pf[1],loglik.se=L_pf[2]) |>\n  write_csv(\"measles_params.csv\")\n```\n:::\n\n\n\n# A local search of the likelihood surface\n\n## A local search of the likelihood surface {.allowframebreaks}\n\nLet's carry out a local search using `mif2` around this point in parameter space.\n\n-   We need to choose the `rw.sd` and `cooling.fraction.50` algorithmic parameters.\n-   Since $\\beta$ and $\\gamma$ will be estimated on the log scale, and we expect that multiplicative perturbations of these parameters will have roughly similar effects on the likelihood, we'll use a perturbation size of $0.02$, which we imagine will have a small but non-negligible effect.\n-   For simplicity, we'll use the same perturbation size on $\\rho$.\n-   We fix `cooling.fraction.50=0.5`, so that after 50 `mif2` iterations, the perturbations are reduced to half their original magnitudes.\n\n\n\n::: {.cell layout-align=\"center\" hash='tmp//cache/local_search_07ab3d655632d9bc7a755ba2c2a3113a'}\n\n```{.r .cell-code}\nforeach(i=1:20,.combine=c,\n  .options.future=list(seed=482947940)\n) %dofuture% {\n  measSIR |>\n    mif2(\n      Np=2000, Nmif=50, cooling.fraction.50=0.5,\n      rw.sd=rw_sd(Beta=0.02, Rho=0.02, Eta=ivp(0.02)),\n      partrans=parameter_trans(\n        log=\"Beta\",logit=c(\"Rho\",\"Eta\")\n      ),\n      paramnames=c(\"Beta\",\"Rho\",\"Eta\")\n    )\n} -> mifs_local\n```\n:::\n\n::: {.cell layout-align=\"center\" hash='tmp//cache/local_search_eval_83f2384f8aed5333c6012cedf33c9090'}\n\n:::\n\n\n\n## Windows issues\n\nSome Windows users have reported trouble with the above code. This appears to be due to certain Windows security features that make it impossible to compile codes inside a parallel block. We have found a workaround.\n\n-   [This document describes the problem and the workaround.](../misc/windows.html)\n-   [A Windows-safe version of the code for this document is available here.](./main_win.R)\n\n## Iterated filtering diagnostics {.allowframebreaks}\n\nWe obtain some diagnostic plots with the `plot` command applied to `mifs_local`. Here is a way to get a prettier version:\n\n\n\n::: {.cell layout-align=\"center\" hash='tmp//cache/local_search_plot_e8e15f30dd944ba892e4609a3bb57f04'}\n\n```{.r .cell-code}\nmifs_local |>\n  traces() |>\n  melt() |>\n  ggplot(aes(x=iteration,y=value,group=.L1,color=factor(.L1)))+\n  geom_line()+\n  guides(color=\"none\")+\n  facet_wrap(~name,scales=\"free_y\")\n```\n\n::: {.cell-output-display}\n![](tmp//figure/local_search_plot-1.png){fig-align='center' fig-pos='h!' width=1800 height=80%}\n:::\n:::\n\n\n\n\\framebreak\n\n-   We see that the likelihood increases as the iterations proceed, though there is considerable variability due to\n\n    1.  the poorness of our starting guess and\n    2.  the stochastic nature of this Monte Carlo algorithm.\n\n-   We see movement in the parameters, though variability remains.\n\n## Estimating the likelihood {.allowframebreaks}\n\nAlthough the filtering carried out by `mif2` in the final filtering iteration generates an approximation to the likelihood at the resulting point estimate, this is not good enough for reliable inference.\n\n-   Partly, this is because parameter perturbations are applied in the last filtering iteration, so that the likelihood reported by `mif2` is not identical to that of the model of interest.\n-   Partly, this is because `mif2` is usually carried out with fewer particles than are needed for a good likelihood evaluation.\n\n\\framebreak\n\nTherefore, we evaluate the likelihood, together with a standard error, using replicated particle filters at each point estimate.\n\n\n\n::: {.cell layout-align=\"center\" hash='tmp//cache/lik_local_b2496e4cf0134026a00b854eaecfaa6d'}\n\n```{.r .cell-code}\nforeach(mf=mifs_local,.combine=rbind,\n  .options.future=list(seed=900242057)\n) %dofuture% {\n  evals <- replicate(10, logLik(pfilter(mf,Np=5000)))\n  ll <- logmeanexp(evals,se=TRUE)\n  mf |> coef() |> bind_rows() |>\n    bind_cols(loglik=ll[1],loglik.se=ll[2])\n} -> results\n```\n:::\n\n\n\n\n\nOn 10 processors, this local investigation took 14\\~sec for the maximization and 5\\~sec for the likelihood evaluation.\n\n\\framebreak\n\n\n\n::: {.cell layout-align=\"center\" hash='tmp//cache/mle_local_2be203242bc1fbd41ca1e3bf257e206e'}\n\n```{.r .cell-code}\nresults |> filter(loglik==max(loglik))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 1 × 8\n   Beta Gamma   Rho     k    Eta     N loglik loglik.se\n  <dbl> <dbl> <dbl> <dbl>  <dbl> <dbl>  <dbl>     <dbl>\n1  26.0     2 0.304    10 0.0904 38000  -112.    0.0630\n```\n:::\n:::\n\n\n\nThese repeated stochastic maximizations can also show us the geometry of the likelihood surface in a neighborhood of this point estimate:\n\n\n\n::: {.cell layout-align=\"center\" hash='tmp//cache/pairs_local_cd7b5daccc1682a960ab75b171689573'}\n\n```{.r .cell-code}\npairs(~loglik+Beta+Eta+Rho,data=results,pch=16)\n```\n\n::: {.cell-output-display}\n![](tmp//figure/pairs_local-1.png){fig-align='center' fig-pos='h!' width=2400 height=80%}\n:::\n:::\n\n\n\n## Building up a picture of the likelihood surface\n\nThis plot shows a hint of a ridge in the likelihood surface (cf.\\~the $\\beta$-$\\eta$ panel). However, the sampling is as yet too sparse to give a clear picture.\n\nWe add these newly explored points to our database,\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nread_csv(\"measles_params.csv\") |>\n  bind_rows(results) |>\n  arrange(-loglik) |>\n  write_csv(\"measles_params.csv\")\n```\n:::\n\n\n\nand move on to a more thorough exploration of the likelihood surface.\n\n# Searching for the MLE\n\n# A global search\n\n## A global search of the likelihood surface {.allowframebreaks}\n\n-   When carrying out parameter estimation for dynamic systems, we need to specify beginning values for both the dynamic system (in the state space) and the parameters (in the parameter space).\n\n-   To avoid confusion, we use the term \"initial values\" to refer to the state of the system at $t_0$ and \"starting values\" to refer to the point in parameter space at which a search is initialized.\n\n-   Practical parameter estimation involves trying many starting values for the parameters.\n\n-   One way to approach this is to choose a large box in parameter space that contains all remotely sensible parameter vectors.\n\n-   If an estimation method gives stable conclusions with starting values drawn randomly from this box, this gives some confidence that an adequate global search has been carried out.\n\n\\framebreak\n\nFor our measles model, a box containing reasonable parameter values might be $\\beta\\in (5,80)$, $\\rho\\in (0.2,0.9)$, $\\eta\\in (0,1)$.\n\nWe are now ready to carry out likelihood maximizations from diverse starting points.\n\n\n\n\n\n::: {.cell layout-align=\"center\" hash='tmp//cache/global_search1_35299af033333a03ce6fc1c26cdb9291'}\n\n```{.r .cell-code}\nset.seed(2062379496)\n\nrunif_design(\n  lower=c(Beta=5,Rho=0.2,Eta=0),\n  upper=c(Beta=80,Rho=0.9,Eta=1),\n  nseq=400\n) -> guesses\n\nmf1 <- mifs_local[[1]]\n```\n:::\n\n\n\n\\framebreak\n\n\n\n::: {.cell layout-align=\"center\" hash='tmp//cache/global_search2_a06c59bfbd1bd0e0fd9e1b249cd5f6a8'}\n\n```{.r .cell-code}\nforeach(guess=iter(guesses,\"row\"), .combine=rbind,\n  .options.future=list(seed=1270401374)\n) %dofuture% {\n  mf1 |>\n    mif2(params=c(guess,fixed_params)) |>\n    mif2(Nmif=100) -> mf\n  replicate(\n    10,\n    mf |> pfilter(Np=5000) |> logLik()\n  ) |>\n    logmeanexp(se=TRUE) -> ll\n  mf |> coef() |> bind_rows() |>\n    bind_cols(loglik=ll[1],loglik.se=ll[2])\n} -> results\n```\n:::\n\n\n\n\n\n\n\n\\framebreak\n\n-   The above codes run one search from each of 400 starting values.\n\n-   Each search consists of an initial run of 50 IF2 iterations, followed by another 100 iterations.\n\n-   These codes exhibit a general pomp behavior:\n\n    -   Re-running a command on an object (i.e., `mif2` on `mf1`) created by the same command preserves the algorithmic arguments.\n    -   In particular, running `mif2` on the result of a `mif2` computation re-runs IF2 from the endpoint of the first run.\n    -   In the second computation, by default, all algorithmic parameters are preserved; here we overrode the default choice of `Nmif`.\n\n-   Following the `mif2` computations, the particle filter is used to evaluate the likelihood, as before.\n\n\\framebreak\n\n\n\n::: {.cell layout-align=\"center\" hash='tmp//cache/mle_global_9e292da2f01d982361a75bb99cbae547'}\n\n```{.r .cell-code}\nresults |> filter(loglik==max(loglik))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 1 × 8\n   Beta    Rho   Eta     N Gamma     k loglik loglik.se\n  <dbl>  <dbl> <dbl> <dbl> <dbl> <dbl>  <dbl>     <dbl>\n1  3.96 0.0602 0.562 38000     2    10  -104.    0.0273\n```\n:::\n:::\n\n\n\n-   In contrast to the local-search codes above, here we return only the endpoint of the search, together with the likelihood estimate and its standard error in a named vector.\n-   The best result of this search had a likelihood of -104.3 with a standard error of 0.03.\n-   This took 13.7 minutes altogether using 10 processors.\n\n\\framebreak\n\nAgain, we attempt to visualize the global geometry of the likelihood surface using a scatterplot matrix. In particular, here we plot both the starting values (grey) and the IF2 estimates (red).\n\n\n\n::: {.cell layout-align=\"center\" hash='tmp//cache/pairs_global1_c938d719f58521804e99a759485a372d'}\n\n```{.r .cell-code}\nread_csv(\"measles_params.csv\") |>\n  filter(loglik>max(loglik)-50) |>\n  bind_rows(guesses) |>\n  mutate(type=if_else(is.na(loglik),\"guess\",\"result\")) |>\n  arrange(type) -> all\n\npairs(~loglik+Beta+Eta+Rho, data=all, pch=16, cex=0.3,\n  col=ifelse(all$type==\"guess\",grey(0.5),\"red\"))\n```\n\n::: {.cell-output-display}\n![](tmp//figure/pairs_global1-1.png){fig-align='center' fig-pos='h!' width=2400 height=80%}\n:::\n:::\n\n\n\n\\framebreak\n\n-   We see that optimization attempts from diverse remote starting points converge on a particular region in parameter space.\n-   The estimates have comparable likelihoods, despite their considerable variability.\n-   This gives us some confidence in our maximization procedure.\n\n\\framebreak\n\nThe projections of the estimates give us \\``poor man`s profiles'':\n\n\n\n::: {.cell layout-align=\"center\" hash='tmp//cache/pairs_global2_cb70905b99fe00f62b90e5b311615ff2'}\n\n```{.r .cell-code}\nall |>\n  filter(type==\"result\") |>\n  filter(loglik>max(loglik)-10) |>\n  ggplot(aes(x=Eta,y=loglik))+\n  geom_point()+\n  labs(\n    x=expression(Eta),\n    title=\"poor man's profile likelihood\"\n  )\n```\n\n::: {.cell-output-display}\n![](tmp//figure/pairs_global2-1.png){fig-align='center' fig-pos='h!' width=2400 height=80%}\n:::\n:::\n\n\n\n# Profile likelihood\n\n## Profile likelihood over $\\eta$ {.allowframebreaks}\n\n-   The curvature displayed in the upper envelope of the above plot suggests that there is indeed information in the data with respect to the susceptible fraction, $\\eta$.\n-   To solidify this evidence, let's compute a profile likelihood over this parameter.\n-   Recall that this means determining, for each value of $\\eta$, the best likelihood that the model can achieve.\n-   To do this, we'll first bound the uncertainty by putting a box around the highest-likelihood estimates we've found so far.\n-   Within this box, we'll choose some random starting points, for each of several values of $\\eta$.\n\n\\framebreak\n\n\n\n::: {.cell layout-align=\"center\" hash='tmp//cache/eta_profile1a_b3a7d5a935d2f473dc4692a1c1fb76e1'}\n\n```{.r .cell-code}\nread_csv(\"measles_params.csv\") |>\n  filter(loglik>max(loglik)-20,loglik.se<2) |>\n  sapply(range) -> box\nbox\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n          Beta Gamma        Rho  k        Eta     N\n[1,]  1.824688     2 0.03405657 10 0.03628984 38000\n[2,] 69.791919     2 0.60343428 10 0.99982180 38000\n        loglik  loglik.se\n[1,] -122.7423 0.01462075\n[2,] -104.2847 0.56960880\n```\n:::\n:::\n\n\n\n\\framebreak\n\n\n\n::: {.cell layout-align=\"center\" hash='tmp//cache/eta_profile1b_cd1e89297619c3c5743244eb3e5d98f1'}\n\n```{.r .cell-code}\nfreeze(seed=1196696958,\n  profile_design(\n    Eta=seq(0.01,0.95,length=40),\n    lower=box[1,c(\"Beta\",\"Rho\")],\n    upper=box[2,c(\"Beta\",\"Rho\")],\n    nprof=15, type=\"runif\"\n  )) -> guesses\nplot(guesses)\n```\n\n::: {.cell-output-display}\n![](tmp//figure/eta_profile1b-1.png){fig-align='center' fig-pos='h!' width=1800 height=80%}\n:::\n:::\n\n\n\n\\framebreak\n\n-   Now, we'll start one independent sequence of iterated filtering operations from each of these points.\n-   We'll be careful to keep $\\eta$ fixed.\n-   This is accomplished by not giving this parameter a random perturbation in the `mif2` call.\n\n\\framebreak\n\n\n\n::: {.cell layout-align=\"center\" hash='tmp//cache/eta_profile2_15e096bf61343a574a5e7107ffa997d1'}\n\n```{.r .cell-code}\nforeach(guess=iter(guesses,\"row\"), .combine=rbind,\n  .options.future=list(seed=830007657)\n) %dofuture% {\n  mf1 |>\n    mif2(params=c(guess,fixed_params),\n      rw.sd=rw_sd(Beta=0.02,Rho=0.02)) |>\n    mif2(Nmif=100,cooling.fraction.50=0.3) -> mf\n  replicate(\n    10,\n    mf |> pfilter(Np=5000) |> logLik()) |>\n    logmeanexp(se=TRUE) -> ll\n  mf |> coef() |> bind_rows() |>\n    bind_cols(loglik=ll[1],loglik.se=ll[2])\n} -> results\n```\n:::\n\n\n\n\n\n## Visualizing profile likelihood {.allowframebreaks}\n\nAs always, we save the results in our global database and plot the results.\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nread_csv(\"measles_params.csv\") |>\n  bind_rows(results) |>\n  filter(is.finite(loglik)) |>\n  arrange(-loglik) |>\n  write_csv(\"measles_params.csv\")\n```\n:::\n\n::: {.cell layout-align=\"center\" hash='tmp//cache/eta_profile_pairs_154ef9ddb588a5651e12539360bb99ab'}\n\n```{.r .cell-code}\nread_csv(\"measles_params.csv\") |>\n  filter(loglik>max(loglik)-10) -> all\n\npairs(~loglik+Beta+Eta+Rho,data=all,pch=16)\n```\n\n::: {.cell-output-display}\n![](tmp//figure/eta_profile_pairs-1.png){fig-align='center' fig-pos='h!' width=2400 height=80%}\n:::\n:::\n\n\n\n\\framebreak\n\nPlotting just the results of the profile calculation reveals that, while some of the IF2 runs either become \"stuck\" on local minima or run out of opportunity to reach the heights of the likelihood surface, many of the runs converge on high likelihoods.\n\n\n\n::: {.cell layout-align=\"center\" hash='tmp//cache/eta_profile_plot1_35352c72205c3d7ad4a0dc4cf8fb6479'}\n\n```{.r .cell-code}\nresults |>\n  ggplot(aes(x=Eta,y=loglik))+\n  geom_point() + xlab(expression(eta))\n```\n\n::: {.cell-output-display}\n![](tmp//figure/eta_profile_plot1-1.png){fig-align='center' fig-pos='h!' width=1800 height=80%}\n:::\n:::\n\n\n\n\\framebreak\n\nA closer look shows what at first appears to be quite a flat surface over much of the explored range of $\\eta$. Note that this appearance is due to the vertical scale, which is driven by the very low likelihoods associated with the smallest values of $\\eta$.\n\n\n\n::: {.cell layout-align=\"center\" hash='tmp//cache/eta_profile_plot2_c9833fbb6314e925d040fe588fbbd0c2'}\n\n```{.r .cell-code}\nresults |>\n  filter(is.finite(loglik)) |>\n  group_by(round(Eta,5)) |>\n  filter(rank(-loglik)<3) |>\n  ungroup() |>\n  filter(loglik>max(loglik)-20) |>\n  ggplot(aes(x=Eta,y=loglik))+\n  geom_point() + xlab(expression(eta))\n```\n\n::: {.cell-output-display}\n![](tmp//figure/eta_profile_plot2-1.png){fig-align='center' fig-pos='h!' width=1800 height=80%}\n:::\n:::\n\n\n\n\\framebreak\n\nFocusing on just the top of the surface shows that, in fact, one is able to estimate $\\eta$ using these data. In the following plot, the cutoff for the 95% confidence interval (CI) is shown.\n\n\\framebreak\n\n\n\n::: {.cell layout-align=\"center\" hash='tmp//cache/eta_profile_plot3_87366a607d8d15d70487f93d68497392'}\n\n```{.r .cell-code}\nmaxloglik <- max(results$loglik,na.rm=TRUE)\nci.cutoff <- maxloglik-0.5*qchisq(df=1,p=0.95)\n\nresults |>\n  filter(is.finite(loglik)) |>\n  group_by(round(Eta,5)) |>\n  filter(rank(-loglik)<3) |>\n  ungroup() |>\n  ggplot(aes(x=Eta,y=loglik))+\n  geom_point() + xlab(expression(eta)) +\n  geom_smooth(method=\"loess\",span=0.25)+\n  geom_hline(color=\"red\",yintercept=ci.cutoff)+\n  lims(y=maxloglik-c(5,0))\n```\n\n::: {.cell-output-display}\n![](tmp//figure/eta_profile_plot3-1.png){fig-align='center' fig-pos='h!' width=1800 height=80%}\n:::\n:::\n\n\n\n\\framebreak\n\n-   As one varies $\\eta$ across the profile, the model compensates by adjusting the other parameters.\n-   It can be very instructive to understand how the model does this.\n-   For example, how does the reporting efficiency, $\\rho$, change as $\\eta$ is varied?\n-   We can plot $\\rho$ vs $\\eta$ across the profile.\n-   This is called a *profile trace*.\n\n\\framebreak\n\n\n\n::: {.cell layout-align=\"center\" hash='tmp//cache/eta_profile_eta_by_rho_41f60b638d163248bfbdceaf6b5d49ea'}\n\n```{.r .cell-code}\nresults |>\n  filter(is.finite(loglik)) |>\n  group_by(round(Eta,5)) |>\n  filter(rank(-loglik)<3) |>\n  ungroup() |>\n  mutate(in_ci=loglik>max(loglik)-1.92) |>\n  ggplot(aes(x=Eta,y=Rho,color=in_ci))+\n  geom_point()+\n  labs(\n    color=\"inside 95% CI?\",\n    x=expression(eta),\n    y=expression(rho),\n    title=\"profile trace\"\n  )\n```\n\n::: {.cell-output-display}\n![](tmp//figure/eta_profile_eta_by_rho-1.png){fig-align='center' fig-pos='h!' width=1800 height=80%}\n:::\n:::\n\n\n\n## Profile over $\\rho$ {.allowframebreaks}\n\n\n\n\n\n\n\nWhile the above profile trace is suggestive that the 95% CI for $\\rho$ must be between roughly 4% and 20%, to confirm this, we should construct a proper profile likelihood over $\\rho$. We do so now.\n\nThis time, we will initialize the IF2 computations at points we have already established have high likelihoods.\n\n\n\n::: {.cell layout-align=\"center\" hash='tmp//cache/rho_profile1_f49fb33f79e4c47f3ea3778b8765e55c'}\n\n```{.r .cell-code}\nread_csv(\"measles_params.csv\") |>\n  group_by(cut=round(Rho,2)) |>\n  filter(rank(-loglik)<=10) |>\n  ungroup() |>\n  arrange(-loglik) |>\n  select(-cut,-loglik,-loglik.se) -> guesses\n```\n:::\n\n\n\n\\framebreak\n\n\n\n::: {.cell layout-align=\"center\" hash='tmp//cache/rho_profile2_128300bfb677d54178fa11829960fd01'}\n\n```{.r .cell-code}\nforeach(guess=iter(guesses,\"row\"), .combine=rbind,\n  .options.future=list(seed=2105684752)\n) %dofuture% {\n  mf1 |>\n    mif2(params=guess,\n      rw.sd=rw_sd(Beta=0.02,Eta=ivp(0.02))) |>\n    mif2(Nmif=100,cooling.fraction.50=0.3) |>\n    mif2() -> mf\n  replicate(\n    10,\n    mf |> pfilter(Np=5000) |> logLik()) |>\n    logmeanexp(se=TRUE) -> ll\n  mf |> coef() |> bind_rows() |>\n    bind_cols(loglik=ll[1],loglik.se=ll[2])\n} -> results\n```\n:::\n\n\n\n\n\n\n\n## Profile over $\\rho$: results {.allowframebreaks}\n\n\n\n::: {.cell layout-align=\"center\" hash='tmp//cache/prof-rho1_670a40515d967c789e942e6e7490abf6'}\n::: {.cell-output-display}\n![](tmp//figure/prof-rho1-1.png){fig-align='center' fig-pos='h!' width=2400 height=80%}\n:::\n:::\n\n\n\n\\framebreak\n\n\n\n::: {.cell layout-align=\"center\" hash='tmp//cache/unnamed-chunk-4_6aec387960446cc7a6621874020824ab'}\n::: {.cell-output-display}\n![](tmp//figure/unnamed-chunk-4-1.png){fig-align='center' fig-pos='h!' width=1800 height=80%}\n:::\n:::\n\n\n\n\\framebreak\n\n\n\n::: {.cell layout-align=\"center\" hash='tmp//cache/rho_ci_9e7e12b91ba1a07645fcc5fee0bec7d4'}\n\n```{.r .cell-code}\nresults |>\n  filter(loglik>max(loglik)-0.5*qchisq(df=1,p=0.95)) |>\n  summarize(min=min(Rho),max=max(Rho)) -> rho_ci\n```\n:::\n\n\n\nThe data appear to be consistent with reporting efficiencies in the 3.4--50% range (95% CI).\n\n## The investigation continues\\dots.\n\n# Making predictions\n\n## Parameter estimates as model predictions {.allowframebreaks}\n\n-   The estimated parameters are one kind of model prediction.\n-   When we can estimate parameters using other data, we can test these predictions.\n-   In the case of a highly contagious, immunizing childhood infection such as measles, we can obtain an estimate of the reporting efficiency, $\\rho$ by simply regressing cumulative cases on cumulative births [@Anderson1991] over many years.\n-   When we do this for Consett, we see that the reporting efficiency is roughly 60%.\n-   Since such a value makes the outbreak data quite unlikely, the prediction does not appear to be borne out.\n-   We can conclude that one or more of our model assumptions is inconsistent with the data.\n-   Let's revisit our assumption that the infectious period is known to be 0.5\\~wk.\n-   Indeed, it would not be surprising were we to find that the *effective* infectious period, at the population scale, were somewhat shorter than the *clinical* infectious period.\n-   For example, confinement of patients should reduce contact rates, and might therefore curtail the effective infectious period.\n-   To investigate this, we'll relax our assumption about the value of $\\gamma$.\n\n# Searching in another direction\n\n## Another global search {.allowframebreaks}\n\nWe will estimate the model under the assumption that $\\rho=0.6$, but without making assumptions about the duration of the infectious period. As before, we'll construct a random design of starting parameters.\n\n\n\n::: {.cell layout-align=\"center\" hash='tmp//cache/exp_global_search1_d04374594a3eb89e0245fb65c65a1380'}\n\n```{.r .cell-code}\nfreeze(seed=55266255,\n  runif_design(\n    lower=c(Beta=5,Gamma=0.2,Eta=0),\n    upper=c(Beta=80,Gamma=5,Eta=0.99),\n    nseq=1000\n  )) |>\n  mutate(\n    Rho=0.6, k=10, N=38000\n  ) -> guesses\n```\n:::\n\n\n\n\\framebreak\n\n-   For each of these starting points, we'll run a series of IF2 computations.\n-   Since we have gained some experience applying `mif2` to this model and these data, we have some expectation about how much computation is required.\n-   In the following, we'll use a lot more computational power than we have so far.\n\n\\framebreak\n\nFor each of the starting points, we'll first perform 100 IF2 iterations:\n\n\n\n::: {.cell layout-align=\"center\" hash='tmp//cache/exp_global_search2a_ecd82f779f33b71e9b33ffe285dacc23'}\n\n```{.r .cell-code}\nmeasSIR |>\n  mif2(params=guess, Np=2000, Nmif=100,\n    cooling.fraction.50=0.5,\n    partrans=parameter_trans(\n      log=c(\"Beta\",\"Gamma\"),\n      logit=\"Eta\"), paramnames=c(\"Beta\",\"Gamma\",\"Eta\"),\n    rw.sd=rw_sd(Beta=0.02,Gamma=0.02,Eta=ivp(0.02))) -> mf\n```\n:::\n\n\n\nWe use random perturbations of the same magnitude as before, taking care to transform the parameters we are estimating.\n\n\\framebreak\n\nWe adopt a *simulated tempering* approach (following a metallurgical analogy), in which we increase the size of the random perturbations some amount (i.e., \"reheat\"), and then continue cooling.\n\n\n\n::: {.cell layout-align=\"center\" hash='tmp//cache/exp_global_search2b_737daca38b4f9989ec412ff31fa29095'}\n\n```{.r .cell-code}\nmf |>\n  mif2(\n    Nmif=100,rw.sd=rw_sd(Beta=0.01,Gamma=0.01,Eta=ivp(0.01))\n  ) |>\n  mif2(\n    Nmif=100,\n    rw.sd=rw_sd(Beta=0.005,Gamma=0.005,Eta=ivp(0.005))\n  ) -> mf\n```\n:::\n\n\n\n\\framebreak\n\nWe wrap the above in a `foreach` loop as before and take care to evaluate the likelihood at each end-point using `pfilter`.\n\nSee the [`R` code for this lesson](./main.R) to see exactly how this is done.\n\n\n\n\n\n\n\n\n\nThe computations above required 55.8 minutes on 10 processors.\n\n\n\n::: {.cell layout-align=\"center\" hash='tmp//cache/another-search_096d3ae0951c55926694caa596d75b9f'}\n\n```{.r .cell-code}\nread_csv(\"measles_params.csv\") |>\n  filter(loglik>max(loglik)-20) -> all\n\npairs(~loglik+Rho+Gamma+Beta+Eta,data=all,pch=16,cex=0.3,\n  col=if_else(round(all$Rho,3)==0.6,1,4))\n```\n\n::: {.cell-output-display}\n![](tmp//figure/another-search-1.png){fig-align='center' fig-pos='h!' width=2400 height=80%}\n:::\n:::\n\n::: {.cell layout-align=\"center\" hash='tmp//cache/poorman-gamma2_36a25e68c0f4d8efcfb17211d38d90ec'}\n\n```{.r .cell-code}\nresults |>\n  filter(loglik>max(loglik)-20,loglik.se<1) |>\n  ggplot(aes(x=Gamma,y=loglik))+\n  geom_point()+\n  geom_hline(\n    color=\"red\",\n    yintercept=max(results$loglik)-0.5*qchisq(df=1,p=0.95)\n  )\n```\n\n::: {.cell-output-display}\n![](tmp//figure/poorman-gamma2-1.png){fig-align='center' fig-pos='h!' width=2400 height=80%}\n:::\n:::\n\n\n\n## Profile over infectious period {.allowframebreaks}\n\nTo make inferences about $\\gamma$, we can again compute a profile likelihood. As before, we bound the region we will search:\n\n\n\n::: {.cell layout-align=\"center\" hash='tmp//cache/gamma_profile1a_8a0a38ed449f2d6008565e1ed0cff82e'}\n\n```{.r .cell-code}\nread_csv(\"measles_params.csv\") |>\n  filter(\n    loglik>max(loglik)-20,\n    loglik.se<2,\n    abs(Rho-0.6)<0.01\n  ) |>\n  sapply(range) -> box\n```\n:::\n\n\n\n\\framebreak\n\n\n\n::: {.cell layout-align=\"center\" hash='tmp//cache/gamma_profile1b_9677ae8a933dbf9b32b683cf6701a2da'}\n\n```{.r .cell-code}\nfreeze(seed=610408798,\n  profile_design(\n    Gamma=seq(0.2,2,by=0.1),\n    lower=box[1,c(\"Beta\",\"Eta\")],\n    upper=box[2,c(\"Beta\",\"Eta\")],\n    nprof=100, type=\"runif\"\n  )) |>\n  mutate(\n    N=38000,\n    Rho=0.6,\n    k=10\n  ) -> guesses\n```\n:::\n\n\n\n\\framebreak\n\n\\AddToHookNext{env/Highlighting/begin}{\\small}\n\n\n\n::: {.cell layout-align=\"center\" hash='tmp//cache/gamma_profile2_ebc6926b38889937df1e0fd5c2b2c2f5'}\n\n```{.r .cell-code}\nforeach(guess=iter(guesses,\"row\"), .combine=rbind,\n  .options.future=list(seed=610408798)\n) %dofuture% {\n  measSIR |>\n    mif2(params=guess, Np=2000, Nmif=100,\n      partrans=parameter_trans(log=\"Beta\",logit=\"Eta\"),\n      paramnames=c(\"Beta\",\"Eta\"), cooling.fraction.50=0.5,\n      rw.sd=rw_sd(Beta=0.02,Eta=ivp(0.02))\n    ) |>\n    mif2(Nmif=100) |>\n    mif2(Nmif=100,rw.sd=rw_sd(Beta=0.01,Eta=ivp(0.01))) |>\n    mif2(Nmif=100,rw.sd=rw_sd(Beta=0.005,Eta=ivp(0.005))) -> mf\n  replicate(10,mf |> pfilter(Np=5000) |> logLik()) |>\n    logmeanexp(se=TRUE) -> ll\n  mf |> coef() |> bind_rows() |>\n    bind_cols(loglik=ll[1],loglik.se=ll[2])\n} -> results\n```\n:::\n\n\n\n\n\n\n\n## Infectious period profile {.allowframebreaks}\n\n\n\n::: {.cell layout-align=\"center\" hash='tmp//cache/gamma_profile_plot_3dbd9fdaa6e7f2ea0272aaaa46824551'}\n\n```{.r .cell-code}\nresults |>\n  group_by(round(Gamma,2)) |>\n  filter(rank(-loglik)<=1) |>\n  ungroup() |>\n  ggplot(aes(x=Gamma,y=loglik))+\n  geom_point() + xlab(expression(gamma)) +\n  geom_hline(\n    color=\"red\",\n    yintercept=max(results$loglik)-0.5*qchisq(df=1,p=0.95)\n  )\n```\n\n::: {.cell-output-display}\n![](tmp//figure/gamma_profile_plot-1.png){fig-align='center' fig-pos='h!' width=1800 height=80%}\n:::\n:::\n\n\n\n\\framebreak\n\n-   This suggests that $\\rho=0.6$ is consistent only with smaller values of $\\gamma$, and hence *longer* infectious periods than are possible if the duration of shedding is actually less than one week.\n-   Thus the model is incapable of reconciling both an infectious period of less than one week and a reporting rate of 60%.\n-   *What structural changes to the model might we make to improve its ability to explain the data?*\n\n## Visualizing predictions {.allowframebreaks}\n\nAfter all these analyses, we would like to visualize how exactly the model with the MLEs matches the data. We can do it by plotting the simulations with 95% the prediction interval.\n\n\n\n::: {.cell layout-align=\"center\" hash='tmp//cache/unnamed-chunk-7_8a88282f68ce4dba7d0c73b6d9355a35'}\n\n```{.r .cell-code}\nread_csv(\"measles_params.csv\") |>\n  filter(loglik == max(loglik)) |>\n  select(-loglik, -loglik.se) -> best.params\n\nmeasSIR |>\n  simulate(\n    params=unlist(best.params),\n    nsim=1000, format=\"data.frame\", include.data=TRUE\n  ) -> sims\n```\n:::\n\n\n\n\\framebreak\n\n\n\n::: {.cell layout-align=\"center\" hash='tmp//cache/viz-predictions_3f56498ab3b7080b0575852c191a0a38'}\n\n```{.r .cell-code}\nsims |>\n  mutate(data=.id==\"data\") |>\n  group_by(week,data) |>\n  reframe(\n    p=c(0.025,0.5,0.975),\n    value=wquant(reports,probs=p),\n    name=c(\"lo\",\"med\",\"up\")\n  ) |>\n  select(-p) |> pivot_wider() |> ungroup() |>\n  ggplot(aes(x=week,y=med,color=data,fill=data,ymin=lo,ymax=up))+\n  geom_line()+ geom_ribbon(alpha=0.2,color=NA) +\n  labs(y=\"reports\")+\n  theme_bw() + guides(color=\"none\",fill=\"none\")\n```\n\n::: {.cell-output-display}\n![](tmp//figure/viz-predictions-1.png){fig-align='center' fig-pos='h!' width=1800 height=80%}\n:::\n:::\n\n\n\n# Exercises\n\n## Fitting the SEIR model {.allowframebreaks}\n\nIn this exercise, you will estimate the parameters and likelihood of the SEIR model you implemented in the earlier lessons by following the template above. Purely for the sake of simplicity, you may assume that the values of $\\gamma$ and $k$ are known. To do this efficiently, we will make use of a system of *run-levels*. At each run-level, we will select some number of particles (`Np`), number of IF2 iterations (`Nmif`), and number of starting guesses, to achieve a particular result.\n\n1.  First, conduct a local search and compute the likelihood at the end of each `mif2` run, as shown above. Use only as many parallel `mif2` computations as you have processors on your computer (or perhaps somewhat fewer). Track the time used and compute the amount of time used per cpu per IF2 iteration per 1000 particles. (Recall that one particle filter computation is roughly equal to a IF2 iteration in computational complexity if they both use the same number of particles.)\n2.  At run-level 1, we want a quick calculation that verifies that the codes are working as expected. Using the expense estimates you generated in Step\\~(\\@ref(it:zero)), choose a number of IF2 iterations so that you can do a very crude \"global search\" that will complete in two or three minutes. Do not reduce `Np` drastically, as we don\\`t want to degrade the performance of the individual IF2 computations. Run your global search with these settings. This serves to debug your global search code.\n3.  At run-level 2, we want a computation that gives us some results we can begin to interpret, but that is still as quick as possible. Choose `Nmif` and the number of random starts so that you can obtain the beginnings of a global search of the parameter space in one hour or less. Run your global search with these settings and plot the results.\n4.  Run-level 3 is intended for final or near-final results. You may want to tune your settings (`Nmif`, `Np`, `rw.sd`, `cooling.fraction.50`) at this point, based on what you found at run-level 2. Decide how much time in the next 18 hours is available to you for a computation. Choose the number of starting guesses so that you can obtain as thorough a global search as possible within this period. Run your global search and identify a maximum likelihood estimate.\n5.  How does the SEIR model compare with the SIR model? Discuss the overall quality of the fit as well as important differences in how the two models are explaining the data.\n\n\\vspace{1em}\n\n[Worked solution to the Exercise](./Q_fit_seir.html)\n\n## Fitting all parameters of the SIR model\n\nIn all of the foregoing, we have assumed a fixed value of the dispersion parameter, $k$, of the negative binomial measurement model. We've also fixed one or the other of $\\gamma$, $\\eta$. Now attempt to estimate all the parameters simultaneously. To accomplish this, use the same system of run-levels as in the previous Exercise.\n\nHow much is the fit improved? How has the model's explanation of the data changed?\n\n\\vspace{1em}\n\n[Worked solution to the Exercise](./Q_fit_all.html)\n\n## Construct a profile likelihood\n\nHow strong is the evidence about the contact rate, $\\beta$, given this model and data? Use `mif2` to construct a profile likelihood. Due to time constraints, you may be able to compute only a preliminary version.\n\nIt is also possible to profile over the basic reproduction number, $R_0=\\beta /\\gamma$. Is this more or less well determined than $\\beta$ for this model and data?\n\n## Checking the source code {.allowframebreaks}\n\nCheck the source code for the `measSIR` pomp object, using the `spy` command.\n\n1.  Does the code implement the model described?\n\nIt can be surprisingly hard to make sure that the written equations and the code are perfectly matched. Papers should be written to be readable, and therefore people rarely choose to clutter papers with numerical details which they hope and believe are scientifically irrelevant.\n\n2.  What problems can arise due to the conflict between readability and reproducibility?\n3.  What solutions are available?\n\n\\vspace{1mm}\n\n[Worked solution to the Exercise](./Q_check_code.html)\n\n## Beware errors in `rprocess`\n\nSuppose that there is an error in the coding of `rprocess` and suppose that plug-and-play statistical methodology is used to infer parameters. As a conscientious researcher, you carry out a simulation study to check the soundness of your inference methodology on this model. To do this, you use `simulate` to generate realizations from the fitted model and you check that your parameter inference procedure recovers the known parameters, up to some statistical error.\n\n1.  Will this procedure help to identify the error in `rprocess`?\n2.  If not, how might you debug `rprocess`?\n3.  What research practices help minimize the risk of errors in simulation code?\n\n\\vspace{1mm}\n\n[Worked solution to the Exercise](./Q_error_in_rprocess.html)\n\n## Choosing the algorithmic settings for IF2\n\nHave a look at [our advice on tuning IF2](./if2_settings.html). An exercise at the end of the linked document invites you to test this advice on the Consett measles example.\n\n# References\n\n## References {.allowframebreaks}\n\n::: {#refs}\n:::\n\n## License, acknowledgments, and links\n\n-   This lesson is prepared for the [Simulation-based Inference for Epidemiological Dynamics](https://rubbislam.quarto.pub/episim/) module at the Summer Institute in Statistics and Modeling in Infectious Diseases, [SISMID](https://sph.emory.edu/SISMID/index.html).\n\n-   The materials build on [previous versions of this course and related courses](../acknowledge.html).\n\n-   Licensed under the [Creative Commons Attribution-NonCommercial license](https://creativecommons.org/licenses/by-nc/4.0/). Please share and remix non-commercially, mentioning its origin. \\includegraphics[height=12pt]{../graphics/cc-by-nc}\n\n-   Produced with R version 4.3.2 and pomp version 5.10.\n\n-   Compiled on 2024-07-24.\n\n\\vfill\n\n[Back to Lesson](index.html)\n\n[`R` code for this lesson](./main.R)\n",
    "supporting": [
      "main_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}