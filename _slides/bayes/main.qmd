---
title: "Bayesian statistics and pomp"
author:
  - Qianying (Ruby) Lin
  - Spencer J. Fox
  - Zian (Larry) Zhuang
format: 
  beamer:
    keep_tex: true
    slide_level: 2
    section-titles: false
    aspectratio: 169
    classoption: "t"
    beameroptions:
      - setbeamertemplate: "footline[frame number]"
      - setbeamertemplate: "navigation symbols{}"
  html:
    toc: false
  pdf:
    toc: false
editor_options: 
  chunk_output_type: console
bibliography: ["../sbied.bib"]  # Assuming this is the bibliography file you're using
---

```{r knitr_opts,include=FALSE,cache=FALSE,purl=FALSE}
source("../_includes/setup.R", local = knitr::knit_global())
library(pomp)
library(tidyverse)
library(foreach)
library(iterators)
library(doFuture)
plan(multisession)
set.seed(1350254336)
```

## Lecture outline

1.  Motivating Bayesian statistics
2.  Short introduction to Bayesian statistics and theory
3.  Introduction to MCMC
4.  Introduction to PMCMC
5.  Simple influenza case study

## What we have covered so far

\Huge

$$p(y|\theta)$$

\Large

-   $y$ can be thought of as your data or observations
-   $\theta$ can be thought of as the model or parameter values
-   Called the "Likelihood"

\normalsize

## Issues with maximum likelihood estimation (MLE)

-   Assumes results occur with some given "frequency" over period time or replicates/repeated experiments
    -   If we had the same outbreak hundreds of time, what proportion of them would provide confidence intervals that contain the true value for the $R_0$
-   Some difficulties in constraining parameter values based on outside data, information, or expert opinion
-   Just not really intuitive...
    -   We typically want to say something about the parameters based on the data, $p(\theta|y)$

## Bayesian statistics

-   Bayes theorem provides an intuitive framework to update parameter estimates based on both prior knowledge and experimental data
-   End result is a posterior distribution, $p(\theta|y)$, directly describing the parameter and model of interest
-   Easy to communicate results
    -   "The reproduction number is estimated to be x, with a 95% credible interval from y to z"
-   Issues
    -   Computationally expensive
    -   Without enough data, prior can bias posterior distribution, but this is what you want!

## Bayes theorem 1

\Huge

$$p(\theta|y) = \frac{p(y|\theta)p(\theta)}{p(y)}$$

\Large

-   $p(\theta|y)$ is the posterior distribution
-   $p(y|\theta)$ is the likelihood
-   $p(\theta)$ is the prior distribution
-   $p(y)$ is the marginal distribution (sometimes called a normalizing constant as it doesn't depend on the parameters)

\normalsize

## Bayes theorem 2

\Huge

$$p(\theta|y) = \frac{p(y|\theta)p(\theta)}{p(y)}$$

\Large

-   $p(y) = \int p(y|\theta) p(\theta) d\theta$
-   Probability of observing $y$ marginal over all possible values of $\theta$
-   Typically is very difficult to calculate
-   The good news is that $p(y)$ is a constant

\normalsize

## Bayes theorem 3

\Huge

$$p(\theta|y) \propto p(y|\theta)p(\theta)$$

\large

-   Since $p(y)$ is a constant, the posterior distribution is proportional to the likelihood times the prior
-   If we can solve this we can get the posterior distribution because $\int p(\theta|y)d\theta = 1$
-   Intuitively our parameter estimates are based on a combination of our observations $p(y|\theta)$ and our prior beliefs $p(\theta)$
-   Only need to sample from the likelihood and prior distribution to get the posterior

\normalsize

## How do we do so?

\Large

-   Many ways to do so (and many software packages), but we're only going to talk about one...
-   Markov chain Monte Carlo (MCMC) is a class of algorithms used to draw samples from a probability distribution
-   Will not cover the theoretical details, but will attempt to motivate

## Assume you have an unknown probability distribution (hill) to explore... How would you do so?

\Large

-   You don't know where it is in parameter space
-   You don't know it's shape

## MCMC Robot Rules

```{=tex}
\begin{center}
  \includegraphics[height=9cm]{../graphics/mcmc-robot.png}
\end{center}
```

## MCMC Robot Rules (actual)  {.allowframebreaks}

```{=tex}
\begin{center}
  \includegraphics[height=7cm]{../graphics/mcmc-robot2.png}
\end{center}
```

\framebreak

```{=tex}
\begin{center}
  \includegraphics[height=7cm]{../graphics/mcmc-robot3.png}
\end{center}
```


\framebreak

```{=tex}
\begin{center}
  \includegraphics[height=7cm]{../graphics/mcmc-robot4.png}
\end{center}
```

## MCMC Demonstration (https://plewis.github.io/applets/mcmc-robot/) {.allowframebreaks}

```{=tex}
\begin{center}
  \includegraphics[height=7cm]{../graphics/mcmc-demo1.png}
\end{center}
```

\framebreak

```{=tex}
\begin{center}
  \includegraphics[height=7cm]{../graphics/mcmc-demo2.png}
\end{center}
```

\framebreak

```{=tex}
\begin{center}
  \includegraphics[height=7cm]{../graphics/mcmc-demo3.png}
\end{center}
```

\framebreak

```{=tex}
\begin{center}
  \includegraphics[height=7cm]{../graphics/mcmc-demo4.png}
\end{center}
```

\framebreak

```{=tex}
\begin{center}
  \includegraphics[height=7cm]{../graphics/mcmc-demo5.png}
\end{center}
```

## Steps for Metropolis-Hastings MCMC

\large

1.  Choose a reasonable starting place for parameters ($\theta$)
2.  Propose new parameter values ($\theta'$)
3.  Calculate the proposal probability $\alpha$
4.  Accept (change parameters to $\theta'$) or reject (keep parameters at $\theta$) with probability $\alpha$
5.  Repeat steps 2-5 for as many MCMC iterations as desired

## Steps for Metropolis-Hastings MCMC

\large

1.  Choose a reasonable starting place for parameters ($\theta$)
2.  Propose new parameter values ($\theta'$)
3.  Calculate the proposal probability $\alpha$
4.  Accept (change parameters to $\theta'$) or reject (keep parameters at $\theta$)
5.  Repeat steps 2-5 for as many MCMC iterations as desired

Where $$\alpha = \text{min}(1, \rho)$$ and $$\rho = \frac{p(\theta'|y)}{p(\theta|y)} \cdot \frac{G(\theta|\theta')}{G(\theta'|\theta)}$$

## $\rho$ equals the ratio of posterior distributions multiplied by the proposal probability ratio

\Huge

$$\rho = \frac{p(\theta'|y)}{p(\theta|y)} \cdot \frac{G(\theta|\theta')}{G(\theta'|\theta)}$$ \large

-   For symmetric proposal distributions (random walks) $\frac{G(\theta|\theta')}{G(\theta'|\theta)} = 1$
-   Notice that the posterior denominators from before ($p(y)$) cancel each other out in this ratio

## We can simplify the acceptance probability significantly

\Huge

$$\rho = \frac{p(y|\theta')p(\theta')}{p(y|\theta)p(\theta)}$$ \Large

-   Only depends on the likelihood and prior probabilities

## Prior parameter distributions

\Large

-   Prior distributions assign probabilities to specific parameter values and are created separately for every parameter
-   Other than strict constraints, we expect the data and likelihood to drive the posterior distribution, but always good idea to check the impact the prior distributions have in a sensitivity analysis

## Three main types of prior distributions

\Large

1.  Informative priors craft a distribution based on previous scientific studies
    -   Typically only used if a specific quantity is well known from outside data (e.g. the infection fatality rate or infectious period)
2.  Weakly informative priors craft a distribution with reasonable constraints
    -   Used for regularization (e.g. to "suggest" that a parameter is most likely to be positive)
3.  Uninformative (flat priors) assign equal probabilities across a range of plausible values
    -   Can constrain parameters to be strictly positive or in biologically/epidemiologically relevant range
    -   Not always "uninformative"

## Questions about MCMC?

## PMCMC

\Huge

$$p(y|\theta)p(\theta)$$

\large

-   Standard MCMC algorithm assumes a deterministic likelihood
-   When process stochasticisty is included we use the particle filter to obtain likelihood
-   Particle filter (Sequential Monte Carlo procedure) approximates $p(y|\theta)$, but with variability
    - Can complicate things a bit, but usually not an issue

## Further PMCMC resources

\large

-   For an awesome field-specific explanation
    -   [Introduction to particle Markov-chain Monte Carlo for disease dynamics modellers](https://doi.org/10.1016/j.epidem.2019.100363)
-   For some considerations with modeling and inference
    -   [Choices and trade-offs in inference with infectious disease models](https://www.sciencedirect.com/science/article/pii/S1755436519300441)
-   For an example of how it has been used in recent epidemiological literature
    -   [Estimating SARS-CoV-2 transmission parameters between coinciding outbreaks in a university population and the surrounding community](https://www.medrxiv.org/content/10.1101/2024.01.10.24301116v1)
-   For working with pmcmc in the `pomp` package
    -   [Getting started with pomp](https://kingaa.github.io/pomp/vignettes/getting_started.html#Sampling_the_posterior_using_particle_Markov_chain_Monte_Carlo)

## Considerations for moving from `mif2` to `pmcmc` in `pomp`

\Large

1.  Specify the prior distributions for all parameters
2.  Use the `pmcmc()` function
3.  Use MCMC diagnostics to check convergence, etc.

## Influenza boarding school example

\large

-   1978 influenza epidemic in boarding school that infected much of the school
-   Data are actually kids in beds, but we are assuming it's new infections

```{r boardingplot, echo=F, fig.dim = c(6, 3), out.height="60%"}
bsflu |> 
  ggplot(aes(day, B)) +
  geom_line()
```

## Modeling as an SIR model with reporting of new infections

```{=tex}
\begin{center}
  \includegraphics[height=6cm]{../graphics/boarding-school-model.png}
\end{center}
```
## Modeling as an SIR model with reporting of new infections

```{r, eval = F}
rproc <- Csnippet("
  double N = 2000;
  double t1 = rbinom(S,1-exp(-Beta*I/N*dt));
  double t2 = rbinom(I,1-exp(-mu_I*dt));
  S  -= t1;
  I  += t1 - t2;
  NI += t1;
  R += t2;
")

rmeas <- Csnippet("
  B = rpois(rho*NI+1e-6);
")
```

## Specifying the prior distribution

```{r prior-ex}
priorDens <- Csnippet("
  lik = dunif(Beta, 1, 4, 1) +
        dunif(mu_I, 0.5, 3, 1) +
        dunif(rho, 0.5, 1, 1);
  if (!give_log) lik = exp(lik);
")
```

\large

-   Add the densities (probabilities) for each parameter value independently
-   Include the same log functionality used before
-   We are specifying a plausible range of values for each parameter here

## Running an MCMC chain

```{r, echo = FALSE}
source('bayes-example/boarding-school-model-incidence.R')

# Setting up model -----------------------------------------
## Let's build the model with the data
## We use euler process and simulate 12 steps per day
bsflu %>%
  select(day,B) %>%
  pomp(
    times="day", 
    t0=0,
    rmeasure=rmeas,
    dmeasure=dmeas,
    rprocess=euler(rproc, delta.t=1/12),
    rinit=rinit,
    partrans=parameter_trans(fromEst=fromEst,toEst=toEst),
    accumvars = 'NI',
    statenames=statenames,
    paramnames=paramnames
  ) -> flu

sim_params <- c(Beta = 2, ## Makes an R0 of 2 (beta/mu_I)
                mu_I = 1, ## average infectious period of 1 day (1/1)
                rho = 0.9, ## 90% of infections reported
                mu_R1 = 1/3)
```

```{r pmcmc-fxn, eval=FALSE}
flu |> ## Standard pomp object that has already been created 
  pomp(dprior = priorDens, ## Prior specified from previous slide
       params = sim_params, ## Parameter starting point
       paramnames=c("Beta","mu_I","rho")) |> ## Parameter names
  pmcmc(Nmcmc = 10000, ## Number of MCMC iterations
        Np = 200, ## Number of particles to use
        proposal = mvn_diag_rw(rw.sd = c(Beta=0.3, mu_I=0.3, rho=0.1))
  ) -> test_mcmc
```

## Proposal distributions in `pomp`

\Large

-   `mvn_diag_rw(rw.sd)` - you provide the standard deviations for the proposals for each parameter
-   `mvn_rw(rw.var)` - you provide the variance/covariance matrix for proposals
-   `mvn_rw_adaptive()` - you provide either of the above and it attempts to automatically "tune" parameters to achieve good MCMC mixing

## Typically you need to test different values initially

```{r pmcmc-fxn2, eval=FALSE}
flu |> ## Standard pomp object that has already been created 
  pomp(dprior = priorDens, ## Prior specified from previous slide
       params = sim_params, ## Parameter starting point
       paramnames=c("Beta","mu_I","rho")) |> ## Parameter names
  pmcmc(Nmcmc = 10000, ## Number of MCMC iterations
        Np = 200, ## Number of particles to use
        proposal = mvn_diag_rw(rw.sd = c(Beta=0.3, mu_I=0.3, rho=0.1))
  ) -> test_mcmc
```

```{r pmcmc-fxn2-1, echo=FALSE}
bake(file="pmcmc1.rds",{
  <<pmcmc-fxn2>>
  attr(test_mcmc,"ncpu") <- nbrOfWorkers()
  test_mcmc
}) -> test_mcmc
t_loc <- attr(test_mcmc,"system.time")
ncpu_loc <- attr(test_mcmc,"ncpu")
```

## Diagnosing chain mixing

```{=tex}
\begin{center}
  \includegraphics[height=8cm]{../graphics/test-mcmc-trace.png}
\end{center}
```
## Diagnosing chain mixing - autocorrelation

```{r autocorr}
library(coda)

test_mcmc |> 
  traces() |>  
  autocorr.diag(lags=c(10, 50, 100))
```

## Second step often uses the empirical covariance matrix for proposals to improve mixing

```{r pmcmc-fxn3, eval=FALSE}
flu |> 
  pomp(dprior = priorDens,
       params = sim_params, ##Using the sim params as a starting spot
       paramnames=c("Beta","mu_I","rho")) |> 
  pmcmc(Nmcmc = 10000, 
        Np = 200,
        proposal = mvn_rw(covmat(test_mcmc, thin = 50))
  ) -> test_mcmc2
```

```{r pmcmc-fxn3-1, echo=FALSE}
bake(file="pmcmc2.rds",{
  <<pmcmc-fxn3>>
  attr(test_mcmc2,"ncpu") <- nbrOfWorkers()
  test_mcmc2
}) -> test_mcmc2
t_loc <- attr(test_mcmc2,"system.time")
ncpu_loc <- attr(test_mcmc2,"ncpu")
```

## Improved trace plots!

```{=tex}
\begin{center}
  \includegraphics[height=8cm]{../graphics/test-mcmc-trace2.png}
\end{center}
```
## Improved autocorrelation!

```{r autocorr2}
test_mcmc2 |> 
  traces() |>  
  autocorr.diag(lags=c(10, 50, 100))
```

## Estimating posterior distributions

\large

1.  Once you are happy with the mixing of your `pmcmc` you are ready to do a run for estimating posteriors
2.  First randomly choose 3-5 starting parameter values (example uses Latin-hypercube sampling)
3.  Run (in parallel or not depending on computational time) a chain intialized with each
4.  Diagnose chain mixing with trace plots and Gelman-Rubin convergence diagnostic
5.  Remove the burn-in period and thin based on diagnostics
6.  Summarize parameter posterior distributions and credible intervals

## Summary process for PMCMC in `pomp`

\large

1.  Create `pomp` object exactly as normal
2.  Create the prior distributions for parameters
3.  Run an initial `pmcmc` with `mvn_diag_rw` proposal to make sure it's working and diagnose
4.  Randomly sample 3-5 parameter starting conditions
5.  Run a PMCMC chain with `mvn_rw()` proposal and the variance/covariance matrix from the first run for each of the initial parameter combinations
6.  Diagnose mixing and convergence (alter components as needed)
7.  Summarize parameters

## Activity: how do stochastic and deterministic models differ?

\Large

1.  Go to https://plewis.github.io/applets/mcmc-robot/, and play with different MCMC parameters and variations
2.  Download the exercise code for the influenza boarding school example and test the impact of the following on mixing and traceplots:
    - Different parameters for the proposal distribution 
    - Different starting parameter values
    - Different number of particles
    
## License, acknowledgments, and links

-   This lesson is prepared for the [Simulation-based Inference for Epidemiological Dynamics](https://rubbislam.quarto.pub/episim/) module at the Summer Institute in Statistics and Modeling in Infectious Diseases, [SISMID](https://sph.emory.edu/SISMID/index.html).

-   The materials build on [previous versions of this course and related courses](../acknowledge.html).

-   Licensed under the [Creative Commons Attribution-NonCommercial license](https://creativecommons.org/licenses/by-nc/4.0/). Please share and remix non-commercially, mentioning its origin. \includegraphics[height=12pt]{../graphics/cc-by-nc}

-   Produced with R version `r getRversion()` and pomp version `r packageVersion("pomp")`.

-   Compiled on 2024-07-24.

\vfill

[Back to Lesson](index.html)

[`R` code for this lesson](./main.R)