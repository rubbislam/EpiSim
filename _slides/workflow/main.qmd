---
title: "Suggested workflow for epidemiological modeling and inference"
author:
  - Qianying (Ruby) Lin
  - Spencer J. Fox
  - Zian (Larry) Zhuang
format: 
  beamer:
    classoption: "t"
    # fontsize: "11pt"
    link-citations: true
    keep_tex: true
    slide_level: 2
    section-titles: false
    aspectratio: 169
    include-in-header: "../_includes/header.tex"
    # beameroptions:
    #   - setbeamertemplate: "footline[frame number]"
    #   - setbeamertemplate: "navigation symbols{}"
    header-includes: |
       \setbeamertemplate{navigation symbols}{}
       \setbeamertemplate{footline}[page number]
    hyperrefoptions:
      - linktoc=all
  html:
    toc: true
  pdf:
    toc: true
editor_options: 
  chunk_output_type: console
bibliography: ["../sbied.bib"]
---

```{r knitr_opts,include=FALSE,cache=FALSE,purl=FALSE}
source("../_includes/setup.R", local = knitr::knit_global())
```

```{r prelims,echo=F,cache=F}
library(foreach)
library(iterators)
library(doFuture)
library(tidyverse)
library(microbenchmark)
library(pomp)
plan(multisession)
```

# Summary

## Summary {.allowframebreaks}

1. **Data Preparation and Model Specification**
    - **Data Initialization**: Visualize the initial data to get a sense of the disease outbreak dynamics. (*Simulation P22*)
    - **Model Definition**: Implement the defined SIR model in `pomp`
      - Set up a stochastic SIR (Susceptible, Infected, Recovered) model using custom C snippets for state transitions and measurements. (*Simulation P36*)
      - Specify the measurement, process, and initialization functions along with model parameters. (*Simulation P38-40*)

2. **Preliminary Analysis**
    - **Simulation and Visualization**: Simulate the SIR model to visualize potential outcomes and compare to data. (*Simulation P44*)
    - **Likelihood Estimation**: Evaluate the likelihood and Effective Sample Size of the initial parameter estimates using particle filtering. (*likelihood P28-29*)
    
\framebreak

3. **Local Optimization**
    - **Iterated Filtering for Local Maximum**: Apply iterated filtering to find a local maximum near the initial guesses. (*Iterated filtering P27*)
    - **Diagnostics and Visualization**: Diagnose the iterated filtering process and visualize parameter traces to assess convergence. (*Iterated filtering P29,33,34*)

4. **Global Optimization**
    - **Set Up for Global Search**: Define a parameter space for global optimization. (*Iterated filtering P38*)
    - **Global Search Execution**: Conduct global optimization using multiple starting points to ensure robustness of the findings. (*Iterated filtering P39*)
    - **Result Compilation**: Compile and compare results from global searches, focusing on the best parameter estimates. (*Iterated filtering P42,45*)

\framebreak

5. **Profile Likelihood Estimation**
    - **Parameter Profiling**: For each parameter, perform profiling to map the likelihood landscape and establish confidence intervals. (*Iterated filtering P48,49,52*)
    - **Visualization of Profile Likelihoods**: Visualize the profile likelihoods to understand parameter sensitivity and uncertainty. (*Iterated filtering P53, 57, 60*)

6. **Final Model Evaluation**
    - **Model Predictions**: Use the best-fitting parameters to simulate the model outcomes and compare these against observed data. (*Iterated filtering P87*)
    - **Visualization of Predictions**: Create visualizations to compare the model predictions with actual data, highlighting the prediction intervals. (*Iterated filtering P88*)
    
7. **Documentation and Storage**
    - **Record-Keeping**: All intermediate and final results should be stored in CSV files for further analysis or reproducibility of the study.

# Example

## Measle example: Data initialization {.allowframebreaks}

```{r meas-data1}
library(tidyverse)
read_csv("Measles_Consett_1948.csv") |>
  select(week,reports=cases) |>
  filter(week<=42) -> meas
meas |> as.data.frame() |> head(n=3)
```

\framebreak

```{r meas-data2, fig.width=5, fig.height=1.5, dpi=300}
meas |>
  ggplot(aes(x=week,y=reports)) +
  geom_line() + geom_point()
```

## Measle example: stochastic SIR model in `pomp` {.allowframebreaks}

```{r sir_pomp1}
sir_stoch <- Csnippet("
  double dN_SI = rbinom(S,1-exp(-Beta*I/N*dt));
  double dN_IR = rbinom(I,1-exp(-Gamma*dt));
  S -= dN_SI; I += dN_SI - dN_IR;
  R += dN_IR; H += dN_IR;"
)

sir_init <- Csnippet("
  S = nearbyint(Eta*N);
  I = 1;
  R = nearbyint((1-Eta)*N);
  H = 0;"
)
```

\framebreak

```{r sir_pomp2}
dmeas <- Csnippet("lik = dnbinom_mu(reports,k,Rho*H,give_log);")
rmeas <- Csnippet("reports = rnbinom_mu(k,Rho*H);")

meas |>
  pomp(
    times="week",t0=0,
    rprocess=euler(sir_stoch,delta.t=1/7),
    rinit=sir_init, rmeasure=rmeas,
    dmeasure=dmeas, accumvars="H",
    statenames=c("S","I","R","H"),
    paramnames=c("Beta","Gamma","Eta","Rho","k","N"),
    params=c(Beta=15,Gamma=2,Rho=0.5,k=10,Eta=0.06,N=38000)
  ) -> measSIR
```

# Preliminary Analysis

## Preliminary analysis: simulation and exploration

```{r, fig.width=8, fig.height=2}
measSIR |>
  simulate(nsim=20,format="data.frame",include.data=TRUE) |>
  ggplot(aes(x=week,y=reports,group=.id,color=.id=="data")) +
  geom_line() + guides(color="none")
```


## Preliminary analysis: likelihood and ess {.allowframebreaks}

Evaluate the likelihood and Effective Sample Size (ESS) of the initial parameter estimates using particle filtering

```{r, fig.width=5, fig.height=2}
pf <- measSIR |> pfilter(Np=5000)
min(pf@eff.sample.size)
```

ESS refers to the number of particles that effectively contribute to the approximation of the posterior distribution.

```{r, fig.width=7, fig.height=4}
#| out-height: 80%
plot(pf)
```


\framebreak

Use repeated particle filtering to refine estimates
 
```{r pf2}
foreach(i=1:10,.combine=c,
  .options.future=list(seed=TRUE)) %dofuture% {
  measSIR |> pfilter(Np=5000)
} -> pf
pf |> logLik() |> logmeanexp(se=TRUE) -> L_pf
L_pf
``` 

Then we get likelihood at a single point. Store this point, together with the estimated likelihood and SE:

```{r init_csv,cache=FALSE}
pf[[1]] |> coef() |> bind_rows() |>
  bind_cols(loglik=L_pf[1],loglik.se=L_pf[2]) |>
  write_csv("measles_params.csv")
```

# Local optimization

## Local optimization {.allowframebreaks} 

- Apply iterated filtering to find a local maximum near the initial guesses

```{r local_search,eval=FALSE,purl=FALSE}
foreach(i=1:20,.combine=c,
  .options.future=list(seed=482947940)) %dofuture% {
  measSIR |>
    mif2(
      Np=2000, Nmif=50, cooling.fraction.50=0.5,
      rw.sd=rw_sd(Beta=0.02, Rho=0.02, Eta=ivp(0.02)),
      partrans=parameter_trans(
        log="Beta",logit=c("Rho","Eta")
      ),
      paramnames=c("Beta","Rho","Eta")
    )
} -> mifs_local
``` 

```{r local_search_eval,echo=FALSE}
bake(file="local_search.rds",{
  <<local_search>>
  attr(mifs_local,"ncpu") <- nbrOfWorkers()
  mifs_local
}) -> mifs_local
t_loc <- attr(mifs_local,"system.time")
ncpu_loc <- attr(mifs_local,"ncpu")
```

\framebreak

- Diagnostics and Visualization:

```{r local_search_plot,fig.width=6, fig.height=4, dpi=300}
#| out-height: 80%
mifs_local |>
  traces() |>
  melt() |>
  ggplot(aes(x=iteration,y=value,group=.L1,color=factor(.L1)))+
  geom_line()+
  guides(color="none")+
  facet_wrap(~name,scales="free_y")
```

- evaluate the likelihood, together with a standard error

```{r lik_local,eval=FALSE,purl=FALSE}
foreach(mf=mifs_local,.combine=rbind,
  .options.future=list(seed=900242057)
) %dofuture% {
  evals <- replicate(10, logLik(pfilter(mf,Np=5000)))
  ll <- logmeanexp(evals,se=TRUE)
  mf |> coef() |> bind_rows() |>
    bind_cols(loglik=ll[1],loglik.se=ll[2])
} -> results
```

```{r lik_local_eval,include=FALSE}
bake(file="lik_local.rds",{
  <<lik_local>>
  attr(results,"ncpu") <- nbrOfWorkers()
  results
}) -> results
t_local <- attr(results,"system.time")
ncpu_local <- attr(results,"ncpu")
```

```{r mle_local}
results |> filter(loglik==max(loglik))
```

- Generate likelihood surface

```{r pairs_local,fig.width=8, fig.height=6, dpi=300}
#| out-height: 80%
pairs(~loglik+Beta+Eta+Rho,data=results,pch=16)
```

# Global Optimization 

## Global optimization: setup {.allowframebreaks} 

1. Likelihood maximization from diverse starting points. 
2. A large box in parameter space. 
3. Expect to see stable conclusions with starting values drawn randomly from this box

```{r cluster_setup,include=FALSE,purl=TRUE}
if (file.exists("CLUSTER.R")) {
  source("CLUSTER.R")
}
```

```{r global_search1}
set.seed(2062379496) 
runif_design(
  lower=c(Beta=5,Rho=0.2,Eta=0),
  upper=c(Beta=80,Rho=0.9,Eta=1),
  nseq=400
) -> guesses
fixed_params <- c(N=38000, Gamma=2, k=10)
mf1 <- mifs_local[[1]]
```

## Global optimization: global Search {.allowframebreaks} 

```{r global_search2,eval=FALSE,purl=FALSE}
foreach(guess=iter(guesses,"row"), .combine=rbind,
  .options.future=list(seed=1270401374)
) %dofuture% {
  mf1 |>
    mif2(params=c(guess,fixed_params)) |>
    mif2(Nmif=100) -> mf
  replicate(
    10,
    mf |> pfilter(Np=5000) |> logLik()
  ) |>
    logmeanexp(se=TRUE) -> ll
  mf |> coef() |> bind_rows() |>
    bind_cols(loglik=ll[1],loglik.se=ll[2])
} -> results
``` 

```{r global_search_eval,include=FALSE}
bake(file="global_search.rds",
  dependson=guesses,{
    <<global_search2>>
    attr(results,"ncpu") <- nbrOfWorkers()
    results
  }) |>
  filter(is.finite(loglik)) -> results
t_global <- attr(results,"system.time")
ncpu_global <- attr(results,"ncpu")
``` 

\framebreak

```{r cache=FALSE,include=FALSE}
read_csv("measles_params.csv") |>
  bind_rows(results) |>
  filter(is.finite(loglik)) |>
  arrange(-loglik) |>
  write_csv("measles_params.csv")
```

Then get the best result

```{r mle_global}
results |> filter(loglik==max(loglik))
```

## Global optimization: diagnostics and visualization {.allowframebreaks} 

Again, we attempt to visualize the global geometry of the likelihood surface using a scatterplot matrix. Then plot contains both the starting values (grey) and the IF2 estimates (red).

```{r pairs_global1,fig.width=8, fig.height=6, dpi=300}
#| out-height: 80%
read_csv("measles_params.csv") |>
  filter(loglik>max(loglik)-50) |>
  bind_rows(guesses) |>
  mutate(type=if_else(is.na(loglik),"guess","result")) |>
  arrange(type) -> all
```

```{r pairs_global11,fig.width=8, fig.height=6, dpi=300}
#| out-height: 80%
pairs(~loglik+Beta+Eta+Rho, data=all, pch=16, cex=0.3,
  col=ifelse(all$type=="guess",grey(0.5),"red"))
```

\framebreak

The projections of the estimates give us ``poor man`s profiles'':

```{r pairs_global2, fig.width=8, fig.height=6, dpi=300}
#| out-height: 80%
all |>
  filter(type=="result") |>
  filter(loglik>max(loglik)-10) |>
  ggplot(aes(x=Eta,y=loglik))+
  geom_point()+
  labs(
    x=expression(Eta),
    title="poor man's profile likelihood"
  )
```


# Profile Likelihood Estimation 

## Profile: for the first parameter {.allowframebreaks} 

We first bound the uncertainty by putting a box around the highest-likelihood.

```{r eta_profile1a}
read_csv("measles_params.csv") |>
  filter(loglik>max(loglik)-20,loglik.se<2) |>
  sapply(range) -> box
box
```

\framebreak

Within this box, we’ll choose some random starting points

```{r eta_profile1b, fig.width=4, fig.height=2, dpi=300}
freeze(seed=1196696958,
  profile_design(
    Eta=seq(0.01,0.95,length=40),
    lower=box[1,c("Beta","Rho")],
    upper=box[2,c("Beta","Rho")],
    nprof=15, type="runif"
  )) -> guesses
```

\framebreak

```{r, fig.width=6, fig.height=4, dpi=300}
#| out-height: 60%
plot(guesses)
```

\framebreak

Now, we’ll start one independent sequence of iterated filtering operations from each of these points.

```{r eta_profile2,eval=FALSE,purl=FALSE}
foreach(guess=iter(guesses,"row"), .combine=rbind,
  .options.future=list(seed=830007657)
) %dofuture% {
  mf1 |>
    mif2(params=c(guess,fixed_params),
      rw.sd=rw_sd(Beta=0.02,Rho=0.02)) |>
    mif2(Nmif=100,cooling.fraction.50=0.3) -> mf
  replicate(10, mf |> pfilter(Np=5000) |> logLik()) |>
    logmeanexp(se=TRUE) -> ll
  mf |> coef() |> bind_rows() |>
    bind_cols(loglik=ll[1],loglik.se=ll[2])
} -> results
```

```{r eta_profile2_eval,include=FALSE}
mf1 <- mifs_local[[1]]
bake(file="eta_profile.rds",
  dependson=guesses,{
    <<eta_profile2>>
    attr(results,"ncpu") <- nbrOfWorkers()
    results
  }) -> results
t_eta <- attr(results,"system.time")
ncpu_eta <- attr(results,"ncpu")
```

## Profile visualization: for the first parameter {.allowframebreaks}

```{r eta_profile_database, echo=FALSE, cache=FALSE}
read_csv("measles_params.csv") |>
  bind_rows(results) |>
  filter(is.finite(loglik)) |>
  arrange(-loglik) |>
  write_csv("measles_params.csv")
```

```{r eta_profile_pairs, fig.width=8, fig.height=6, dpi=300}
#| out-height: 70%
read_csv("measles_params.csv") |> 
  filter(loglik>max(loglik)-10) -> all

pairs(~loglik+Beta+Eta+Rho,data=all,pch=16)
```

\framebreak

Focusing on the top of the surface

```{r eta_profile_plot2, fig.width=6, fig.height=5, dpi=300}
#| out-height: 80%
results |>
  filter(is.finite(loglik)) |>
  group_by(round(Eta,5)) |>
  filter(rank(-loglik)<3) |>
  ungroup() |>
  filter(loglik>max(loglik)-20) |>
  ggplot(aes(x=Eta,y=loglik))+
  geom_point() + xlab(expression(eta))
``` 

\framebreak

Estimate $\eta$ with 95% CI using these data

```{r eta_profile_plot3, fig.width=6, fig.height=4, dpi=300}
#| out-height: 80%
maxloglik <- max(results$loglik,na.rm=TRUE)
ci.cutoff <- maxloglik-0.5*qchisq(df=1,p=0.95)

results |>
  filter(is.finite(loglik)) |>
  group_by(round(Eta,5)) |>
  filter(rank(-loglik)<3) |> ungroup() |>
  ggplot(aes(x=Eta,y=loglik))+
  geom_point() + xlab(expression(eta)) +
  geom_smooth(method="loess",span=0.25)+
  geom_hline(color="red",yintercept=ci.cutoff)+
  lims(y=maxloglik-c(5,0))
```

\framebreak

```{r eta_ci}
results |>
  filter(is.finite(loglik)) |>
  filter(loglik>max(loglik)-0.5*qchisq(df=1,p=0.95)) |> 
  summarize(min=min(Eta),max=max(Eta)) -> Eta_ci
```

Then we know $\eta$ is in the `r signif(Eta_ci$min,2)`--`r signif(Eta_ci$max,2)`% range (95% CI).

## Profile: for the later parameters {.allowframebreaks}

For the next parameter, we can initialize the IF2 computations at points we have already established have high likelihoods.

```{r rho_profile1}
read_csv("measles_params.csv") |>
  group_by(cut=round(Rho,2)) |>
  filter(rank(-loglik)<=10) |>
  ungroup() |>
  arrange(-loglik) |>
  select(-cut,-loglik,-loglik.se) -> guesses
```

\framebreak

Then again, generate the profile likelihood

```{r rho_profile2,eval=FALSE,purl=FALSE}
foreach(guess=iter(guesses,"row"), .combine=rbind,
  .options.future=list(seed=2105684752)
) %dofuture% {
  mf1 |>
    mif2(params=guess,
      rw.sd=rw_sd(Beta=0.02,Eta=ivp(0.02))) |>
    mif2(Nmif=100,cooling.fraction.50=0.3) |>
    mif2() -> mf
  replicate(
    10,
    mf |> pfilter(Np=5000) |> logLik()) |>
    logmeanexp(se=TRUE) -> ll
  mf |> coef() |> bind_rows() |>
    bind_cols(loglik=ll[1],loglik.se=ll[2])
} -> results
```

```{r rho_profile_eval,include=FALSE}
mf1 <- mifs_local[[1]]
bake(file="rho_profile.rds",
  dependson=guesses,{
    <<rho_profile2>>
    attr(results,"ncpu") <- nbrOfWorkers()
    results
  }) -> results
t_rho <- attr(results,"system.time")
ncpu_rho <- attr(results,"ncpu")
```

```{r include=FALSE,cache=FALSE}
read_csv("measles_params.csv") |>
  bind_rows(results) |>
  filter(is.finite(loglik)) |>
  arrange(-loglik) |>
  write_csv("measles_params.csv")
``` 

## Profile visualization: for the later parameters {.allowframebreaks}

```{r prof-rho1, echo=FALSE, fig.width=8, fig.height=6, dpi=300}
#| out-height: 80%
results |> filter(is.finite(loglik)) -> results

pairs(~loglik+Beta+Eta+Rho,data=results,pch=16)
```

\framebreak

```{r echo=FALSE, fig.width=6, fig.height=4, dpi=300}
#| out-height: 80%
results |>
  filter(loglik>max(loglik)-10,loglik.se<1) |>
  group_by(round(Rho,2)) |>
  filter(rank(-loglik)<3) |>
  ungroup() |>
  ggplot(aes(x=Rho,y=loglik))+
  geom_point() + xlab(expression(rho)) +
  geom_hline(
    color="red",
    yintercept=max(results$loglik)-0.5*qchisq(df=1,p=0.95)
  )
```

\framebreak 

```{r rho_ci}
results |>
  filter(loglik>max(loglik)-0.5*qchisq(df=1,p=0.95)) |>
  summarize(min=min(Rho),max=max(Rho)) -> rho_ci
```

Then we know reporting efficiencies $\rho$ is in the `r signif(100*rho_ci$min,2)`--`r signif(100*rho_ci$max,2)`% range (95% CI).


\framebreak

# Check predictions
## Prediction  {.allowframebreaks}

After all these analyses, we would like to visualize how exactly the model 
with the MLEs matches the data.
We can do it by plotting the simulations with 95\% the prediction interval.

```{r}
read_csv("measles_params.csv") |>
  filter(loglik == max(loglik)) |>
  select(-loglik, -loglik.se) -> best.params

measSIR |>
  simulate(
    params=unlist(best.params),
    nsim=1000, format="data.frame", include.data=TRUE
  ) -> sims
```

 

```{r viz-predictions, fig.height=4, fig.width=6, dpi=300}
#| out-height: 80%
sims |>
  mutate(data=.id=="data") |>
  group_by(week,data) |>
  reframe(
    p=c(0.025,0.5,0.975),
    value=wquant(reports,probs=p),
    name=c("lo","med","up")
  ) |>
  select(-p) |> pivot_wider() |> ungroup() |>
  ggplot(aes(x=week,y=med,color=data,fill=data,ymin=lo,ymax=up))+
  geom_line()+ geom_ribbon(alpha=0.2,color=NA) +
  labs(y="reports")+
  theme_bw() + guides(color="none",fill="none")
```

# References

## License, acknowledgments, and links

-   This lesson is prepared for the [Simulation-based Inference for Epidemiological Dynamics](https://rubbislam.quarto.pub/episim/) module at the Summer Institute in Statistics and Modeling in Infectious Diseases, [SISMID](https://sph.emory.edu/SISMID/index.html).

-   The materials build on [previous versions of this course and related courses](../acknowledge.html).

-   Licensed under the [Creative Commons Attribution-NonCommercial license](https://creativecommons.org/licenses/by-nc/4.0/). Please share and remix non-commercially, mentioning its origin. \includegraphics[height=12pt]{../graphics/cc-by-nc}

-   Produced with R version `r getRversion()` and pomp version `r packageVersion("pomp")`.

-   Compiled on 2024-07-24.

\vfill

[Back to Lesson](index.html)

[`R` code for this lesson](./workflow.R)




