---
title: "Lesson 4: Iterated filtering: principles and practice"
author:
  - Spencer J. Fox
  - Qianying (Ruby) Lin
  - Jesse Wheeler
format: 
  beamer:
    classoption: "t"
    # fontsize: "11pt"
    link-citations: true
    keep_tex: true
    slide_level: 2
    section-titles: false
    aspectratio: 169
    include-in-header: "../_includes/header.tex"
    # beameroptions:
    #   - setbeamertemplate: "footline[frame number]"
    #   - setbeamertemplate: "navigation symbols{}"
    header-includes: |
       \setbeamertemplate{navigation symbols}{}
       \setbeamertemplate{footline}[page number]
    hyperrefoptions:
      - linktoc=all
  html:
    toc: true
  pdf:
    toc: true
editor_options: 
  chunk_output_type: console
bibliography: ["../sbied.bib"]
---

```{r knitr_opts,include=FALSE,cache=FALSE,purl=FALSE}
source("../_includes/setup.R", local = knitr::knit_global())
```

```{r prelims,echo=F,cache=F}
library(foreach)
library(iterators)
library(doFuture)
library(tidyverse)
library(pomp)
set.seed(1350254336)
```

# Introduction

## Introduction

-   This tutorial covers likelihood estimation via the method of iterated filtering.
-   It presupposes familiarity with building partially observed Markov process (POMP) objects in the `R` package pomp [@King2016].
-   This tutorial follows on from the [topic of particle filtering](../pfilter/main.pdf) (also known as sequential Monte Carlo) via `pfilter` in pomp.

## Objectives

1.  To review the available options for inference on POMP models, to put iterated filtering in context.
2.  To understand how iterated filtering algorithms carry out repeated particle filtering operations, with randomly perturbed parameter values, in order to maximize the likelihood.
3.  To gain experience carrying out statistical investigations using iterated filtering in a relatively simple situation: fitting an SIR model to data from a measles outbreak.

# Classification of statistical methods for POMP models

## Classification of statistical methods for POMP models {.allowframebreaks}

-   Many, many statistical methods have been proposed for inference on POMP models [@He2010,@King2016].

-   The volume of research indicates both the importance and the difficulty of the problem.

-   Let's start by considering three criteria to categorize inference methods:

    -   the plug-and-play property
    -   full-information or feature-based
    -   frequentist or Bayesian

# The plug-and-play property

## Plug-and-play (also called simulation-based) methods {.allowframebreaks}

-   Inference methodology that calls `rprocess` but not `dprocess` is said to be *plug-and-play*. All popular modern Monte Carlo methods for POMP models are in this category.
-   "Simulation-based" is equivalent to "plug-and-play".
-   Historically, simulation-based meant simulating forward from initial conditions to the end of the time series.
-   However, particle filtering methods instead consider each observation interval sequentially. They carry out multiple, carefully selected, simulations over each interval.

\framebreak

-   Plug-and-play methods can call `dmeasure`. A method that uses only `rprocess` and `rmeasure` is called "doubly plug-and-play".
-   Two *non-plug-and-play* methods---expectation-maximization (EM) and Markov chain Monte Carlo (MCMC)---have theoretical convergence problems for nonlinear POMP models. The failures of these two workhorses of statistical computation have prompted development of alternative methodologies.

# Full information vs.\~feature-based methods

## Full-information and feature-based methods {.allowframebreaks}

-   *Full-information* methods are defined to be those based on the likelihood function for the full data (i.e., likelihood-based frequentist inference and Bayesian inference).
-   *Feature-based* methods either consider a summary statistic (a function of the data) or work with an an alternative to the likelihood.
-   Asymptotically, full-information methods are statistically efficient and feature-based methods are not.
-   In some cases, loss of statistical efficiency might be an acceptable tradeoff for advantages in computational efficiency.

\framebreak

-   However:

    -   Good low-dimensional summary statistics can be hard to find.
    -   When using statistically inefficient methods, it can be hard to know how much information you are losing.
    -   Intuition and scientific reasoning can be inadequate tools to derive informative low-dimensional summary statistics [@Shrestha2011,Ionides2011a].

# Bayesian vs.\~frequentist approaches

## Bayesian and frequentist methods {.allowframebreaks}

-   Recently, plug-and-play Bayesian methods have been discovered:

    -   particle Markov chain Monte Carlo (PMCMC) [@Andrieu2010].
    -   approximate Bayesian computation (ABC) [@Toni2009].

-   Prior belief specification is both the strength and weakness of Bayesian methodology:

-   The likelihood surface for nonlinear POMP models often contains nonlinear ridges and variations in curvature.

\framebreak

-   These situations bring into question the appropriateness of independent priors derived from expert opinion on marginal distributions of parameters.
-   They also are problematic for specification of "flat" or "uninformative" prior beliefs.
-   Expert opinion can be treated as data for non-Bayesian analysis. However, our primary task is to identify the information in the data under investigation, so it can be helpful to use methods that do not force us to make our conclusions dependent on quantification of prior beliefs.

## Summary

```{=tex}
\begin{center}
  \includegraphics[height=6cm]{../graphics/lec4 table.png}
\end{center}
```
# Iterated filtering in theory

## Full-information, plug-and-play, frequentist methods

-   Iterated filtering methods [@Ionides2015] are the only currently available, full-information, plug-and-play, frequentist methods for POMP models.
-   Iterated filtering methods have been shown to solve likelihood-based inference problems for epidemiological situations which are computationally intractable for available Bayesian methodology [@Ionides2015].

## An iterated filtering algorithm (IF2) {.allowframebreaks}

**Input:**

-   simulators for $f_{X_0}(x_0;\theta)$ and $f_{X_n|X_{n-1}}(x_n| x_{n-1}; \theta)$
-   evaluator for $f_{Y_n|X_n}(y_n| x_n;\theta)$
-   data, $y^*_{1:N}$

\textbf{Algorithmic parameters and corresponding \texttt{mif2} arguments:}

-   number of iterations, \texttt{Nmif} = $M$
-   number of particles, \texttt{Np} = $J$
-   initial parameter swarm
<!-- , \texttt{params} = $\{\Theta^{j,0}, j=1,\dots,J\}$ -->
-   perturbation, random walk standard deviation for each parameter, \texttt{rw.sd}, squared to construct a diagonal variance matrix, $V_n$
-   decay of perturbation, cooling fraction in 50 iterations, \texttt{cooling.fraction.50} = $a$

**Output:**

-   final parameter swarm
<!-- , $\{\Theta^{j,M}, j=1,\dots,J\}$ -->

\framebreak

\begin{center}
  \includegraphics[height=7cm]{../graphics/mif_graph.pdf}
\end{center}

\framebreak

<!-- **Procedure:** -->

<!-- 1.  For $m$ in $1{:}M$ -->
<!-- 2.  $\qquad$ $\Theta^{F,m}_{0,j}\sim \normal\big(\Theta^{m-1}_{j},V_0 \, a^{2m/50}\big)$ for $j$ in $1{:} J$ -->
<!-- 3.  $\qquad$ $X_{0,j}^{F,m}\sim f_{X_0}(x_0 ; \Theta^{F,m}_{0,j})$ for $j$ in $1{:} J$ -->
<!-- 4.  $\qquad$ For $n$ in $1{:} N$ -->
<!-- 5.  $\qquad\qquad$ $\Theta^{P,m}_{n,j}\sim \normal\big(\Theta^{F,m}_{n-1,j},V_n \, a^{2m/50}\big)$ for $j$ in $1{:} J$ -->
<!-- 6.  $\qquad\qquad$ $X_{n,j}^{P,m}\sim f_{X_n|X_{n-1}}(x_n | X^{F,m}_{n-1,j}; \Theta^{P,m}_{n,j})$ for $j$ in $1{:} J$ -->
<!-- 7.  $\qquad\qquad$ $w_{n,j}^m = f_{Y_n|X_n}(y^*_n| X_{n,j}^{P,m} ; \Theta^{P,m}_{n,j})$ for $j$ in $1{:} J$ -->
<!-- 8.  $\qquad\qquad$ Draw $k_{1{:}J}$ with $P[k_j=i]= w_{n,i}^m\Big/\sum_{u=1}^J w_{n,u}^m$ -->
<!-- 9.  $\qquad\qquad$ $\Theta^{F,m}_{n,j}=\Theta^{P,m}_{n,k_j}$ and $X^{F,m}_{n,j}=X^{P,m}_{n,k_j}$ for $j$ in $1{:} J$ -->
<!-- 10. $\qquad$ End For -->
<!-- 11. $\qquad$ Set $\Theta^{m}_{j}=\Theta^{F,m}_{N,j}$ for $j$ in $1{:} J$ -->
<!-- 12. End For -->

<!-- \framebreak -->

<!-- **Remarks:** -->

<!-- -   The $N$ loop (lines 4 through 10) is a basic particle filter applied to a model with stochastic perturbations to the parameters. -->
<!-- -   The $M$ loop repeats this particle filter with decreasing perturbations. -->
<!-- -   The superscript $F$ in $\Theta^{F,m}_{n,j}$ and $X^{F,m}_{n,j}$ denote solutions to the *filtering problem*, with the particles $j=1,\dots,J$ providing a Monte Carlo representation of the conditional distribution at time $n$ given data $y^*_{1:n}$ for filtering iteration $m$. -->
<!-- -   The superscript $P$ in $\Theta^{P,m}_{n,j}$ and $X^{P,m}_{n,j}$ denote solutions to the *prediction problem*, with the particles $j=1,\dots,J$ providing a Monte Carlo representation of the conditional distribution at time $n$ given data $y^*_{1:n-1}$ for filtering iteration $m$. -->
<!-- -   The *weight* $w^m_{n,j}$ gives the likelihood of the data at time $n$ for particle $j$ in filtering iteration $m$. -->



## The IF2 diagram in \texttt{pomp}

<!-- We focus on the IF2 algorithm of @Ionides2015. In this algorithm: -->

<!-- -   Each iteration consists of a particle filter, carried out with the parameter vector, for each particle, doing a random walk. -->
<!-- -   At the end of the time series, the collection of parameter vectors is recycled as starting parameters for the next iteration. -->
<!-- -   The random-walk variance decreases at each iteration. -->

<!-- In theory, this procedure converges toward the region of parameter space maximizing the maximum likelihood.\ -->
<!-- In practice, we can test this claim on examples. -->

```{=tex}
\begin{center}
  \includegraphics[height=8cm]{../graphics/lec4 pf.png}
\end{center}
```

<!-- ## Analogy with evolution by natural selection {.allowframebreaks} -->

<!-- -   The parameters characterize the *genotype*. -->
<!-- -   The swarm of particles is a *population*. -->
<!-- -   The likelihood, a measure of the compatibility between the parameters and the data, is the analogue of *fitness*. -->
<!-- -   Each successive observation is a new *generation*. -->
<!-- -   Since particles reproduce in each generation in proportion to their likelihood, the particle filter acts like *natural selection*. -->
<!-- -   The artificial perturbations augment the "genetic" variance and therefore correspond to *mutation*. -->
<!-- -   IF2 increases the *fitness* of the population of particles. -->
<!-- -   However, because our scientific interest focuses on the model without the artificial perturbations, we decrease the intensity of the latter with successive iterations. -->

## Exercise 1: IF2 in \texttt{pomp}, `mif2` basics  {.allowframebreaks}
The following codes (also see `/scripts/exercise_mif.R`) run ONE iterated filtering with 50 iterations and show how parameters change over iteration.
Change some numbers to see how the results are impacted by those changes.

```{r mif2-basics1, eval=FALSE}
measSIR |>
  mif2(
    Np=2000, Nmif=50, cooling.fraction.50=0.5,
    rw.sd=rw_sd(Beta=0.02, Rho=0.02, Eta=ivp(0.02)),
    partrans=parameter_trans(log="Beta",logit=c("Rho","Eta")),
    paramnames=c("Beta","Rho","Eta")
  ) -> mf

plot(mf)
```

\framebreak

We can also recycle the results in the last iteration within `mf` and run another 50 iterations with different setting.
Update other arguments to see how it change the results.

```{r mif2-basics2, eval=FALSE}
mf |>
  mif2(
    cooling.fraction.50 = 0.3
  ) -> mf

plot(mf)
```

## Exercise 2: `mif2` in parallel  {.allowframebreaks}

The following codes run `mif2` for 20 replicates within a parallel computing scheme.

```{r mif2-parallel1, eval=FALSE}
library(foreach)
library(doFuture)
plan(multisession, workers=8)
foreach(
  i=1:20,.combine=c, .options.future=list(seed=482947940)
) %dofuture% {
  measSIR |>
    mif2(Np=2000, Nmif=50, cooling.fraction.50=0.5,
      rw.sd=rw_sd(Beta=0.02, Rho=0.02, Eta=ivp(0.02)),
      partrans=parameter_trans(log="Beta",logit=c("Rho","Eta")),
      paramnames=c("Beta","Rho","Eta"))
} -> mifs_local
```

\framebreak

We can track the how parameters change over iteration of each replicate in one plot.
Discuss how those 20 replicates change and what the takeaway is.

```{r mif2-parallel2, eval=FALSE}
mifs_local |>
  traces() |>
  melt() |>
  ggplot(aes(x=iteration,y=value,group=.L1,color=factor(.L1))) +
    geom_line()+
    guides(color="none")+
    facet_wrap(~name,scales="free_y")
```

## Exercise 3: likelihood estimation for `mif2` objects

Why we need to estimate the likelihood in a different manner?

```{r mif2-likelihood1, eval=FALSE}
foreach(
  mf=mifs_local,.combine=rbind,
  .options.future=list(seed=900242057)
) %dofuture% {
  evals <- replicate(10, logLik(pfilter(mf,Np=5000)))
  ll <- logmeanexp(evals,se=TRUE)
  mf |> coef() |> bind_rows() |>
  bind_cols(loglik=ll[1],loglik.se=ll[2])
} -> results

results |> filter(loglik==max(loglik))
```

# References

## References {.allowframebreaks}

::: {#refs}
:::

## License, acknowledgments, and links

-   This lesson is prepared for the [Simulation-based Inference for Epidemiological Dynamics](https://rubbislam.quarto.pub/episim/) module at the Summer Institute in Statistics and Modeling in Infectious Diseases, [SISMID](https://sph.emory.edu/SISMID/index.html).

-   The materials build on [previous versions of this course and related courses](../acknowledge.html).

-   Licensed under the [Creative Commons Attribution-NonCommercial license](https://creativecommons.org/licenses/by-nc/4.0/). Please share and remix non-commercially, mentioning its origin. \includegraphics[height=12pt]{../graphics/cc-by-nc}

-   Produced with R version `r getRversion()` and pomp version `r packageVersion("pomp")`.

-   Compiled on 2024-07-24.

\vfill

[Back to Lesson](index.html)

[`R` code for this lesson](./main.R)
