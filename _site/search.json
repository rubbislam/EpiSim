[
  {
    "objectID": "likelihood.html",
    "href": "likelihood.html",
    "title": "Simulation and Likelihood",
    "section": "",
    "text": "Model implementation in pomp\nCreating a pomp object with different components\nComputing likelihood using pfilter\n\n\noptions(warning = FALSE)\nlibrary(dplyr)\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\nlibrary(tidyr)\nlibrary(readr)\nlibrary(iterators)\nlibrary(pomp)\nlibrary(ggplot2)\nlibrary(foreach)\nlibrary(doParallel)\n\nLoading required package: parallel"
  },
  {
    "objectID": "likelihood.html#summary",
    "href": "likelihood.html#summary",
    "title": "Simulation and Likelihood",
    "section": "",
    "text": "Model implementation in pomp\nCreating a pomp object with different components\nComputing likelihood using pfilter\n\n\noptions(warning = FALSE)\nlibrary(dplyr)\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\nlibrary(tidyr)\nlibrary(readr)\nlibrary(iterators)\nlibrary(pomp)\nlibrary(ggplot2)\nlibrary(foreach)\nlibrary(doParallel)\n\nLoading required package: parallel"
  },
  {
    "objectID": "likelihood.html#recap-a-stochastic-sir-model-in-the-pomp-framework",
    "href": "likelihood.html#recap-a-stochastic-sir-model-in-the-pomp-framework",
    "title": "Simulation and Likelihood",
    "section": "Recap: A stochastic SIR model in the POMP framework",
    "text": "Recap: A stochastic SIR model in the POMP framework\nWe define the state process as a stochastic SIR model, \\(X_t=(S_t, I_t, R_t, D_t)\\) where \\(D_t\\) is the weekly cumulative diagnoses, getting reset to 0 every 7 days. The number of infections \\(\\Delta N_{SI}\\) and recoveries \\(\\Delta N_{IR}\\) are random variables, following Binomial distributions:\n\\[\\begin{aligned}\n  \\Delta N_{SI} &\\sim \\mathrm{Binomial}\\left(S, 1-e^{-\\beta\\frac{I}{N}\\Delta t}\\right) \\\\\n  \\Delta N_{IR} &\\sim \\mathrm{Binomial}\\left(I, 1-e^{-\\gamma \\Delta t}\\right)\n\\end{aligned}\\]\nwhere \\(\\beta\\) is the transmission rate and \\(\\gamma\\) is the recovery rate.\nWe define a weekly measurement model which follows a Negative Binomial distribution with mean \\(\\rho D_t\\) and the dispersion parameter \\(k\\):\n\\[\n\\textrm{reports}_t \\sim \\textrm{NegBin}(\\rho D_t, k),\n\\]\nwhere \\(\\rho\\) is the reporting ratio and \\(0 \\leq \\rho \\leq 1\\) ."
  },
  {
    "objectID": "likelihood.html#implementing-the-sir-model-in-pomp-package",
    "href": "likelihood.html#implementing-the-sir-model-in-pomp-package",
    "title": "Simulation and Likelihood",
    "section": "Implementing the SIR model in pomp package",
    "text": "Implementing the SIR model in pomp package\n\nExample: the Consett measles outbreak\nWe first download the data and take a look at them:\n\nread_csv(paste0(\"https://kingaa.github.io/sbied/stochsim/\", \n  \"Measles_Consett_1948.csv\")) |&gt;\n  select(week,reports=cases) -&gt; dat_meas\n\nRows: 53 Columns: 2\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\ndbl (2): week, cases\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\ndat_meas |&gt; head()\n\n# A tibble: 6 × 2\n   week reports\n  &lt;dbl&gt;   &lt;dbl&gt;\n1     1       0\n2     2       0\n3     3       2\n4     4       0\n5     5       3\n6     6       0\n\ndat_meas |&gt; summary()\n\n      week       reports     \n Min.   : 1   Min.   : 0.00  \n 1st Qu.:14   1st Qu.: 0.00  \n Median :27   Median : 1.00  \n Mean   :27   Mean   : 9.83  \n 3rd Qu.:40   3rd Qu.: 7.00  \n Max.   :53   Max.   :75.00  \n\n\nand visualize:\n\ndat_meas |&gt;\n  ggplot(aes(x=week, y=reports)) +\n  geom_line() +\n  geom_point() +\n  theme_bw()\n\n\n\n\n\n\n\n\n\n\nA stochastic SIR model for measles in pomp\nTo accelerate the computation, we would like to code the model in C/C++ using C snippets in pomp. Now we can have our **one-step transition** function:\n\nsird_step &lt;- Csnippet(\"\n  double N = S + I + R;\n  double dN_SI = rbinom(S, 1-exp(-Beta*I/N*dt));\n  double dN_IR = rbinom(I, 1-exp(-gamma*dt));\n  S -= dN_SI;\n  I += dN_SI - dN_IR;\n  R += dN_IR;\n  D += dN_IR;\n\")\n\nand the initialization and measurement model:\n\nsird_rinit &lt;- Csnippet(\"\n  S = S0;\n  I = I0;\n  R = R0;\n  D = 0;\n\")\n\nsird_dmeas &lt;- Csnippet(\"\n  lik = dnbinom_mu(reports, k, rho*D, give_log);\n\")\n\nsird_rmeas &lt;- Csnippet(\"\n  reports = rnbinom_mu(k, rho*D);\n\")\n\nNow we can put the model together in pomp and simulate the model dynamics:\n\ndat_meas |&gt;\n  pomp(\n    times=\"week\",\n    t0=0,\n    rprocess = euler(sird_step, delta.t = 1/7),\n    rinit = sird_rinit,\n    rmeasure = sird_rmeas,\n    dmeasure = sird_dmeas,\n    accumvars = \"D\",\n    statenames = c(\"S\",\"I\",\"R\",\"D\"),\n    paramnames = c(\"Beta\",\"gamma\",\"rho\",\"k\",\"S0\",\"I0\",\"R0\")\n  ) -&gt; sird_measle\n\nAfter getting the components in place in pomp, we can simulate the dynamics with some parameters from literature or anything you want:\n\nsird_measle |&gt;\n  simulate(\n    params = c(\n      Beta = 7.5, gamma = .5, rho = .5, k = 10,\n      S0 = 1140, I0 = 1, R0 = 36860\n    ),\n    nsim = 100,\n    format = \"data.frame\",\n    include.data = TRUE\n  ) -&gt; sims_measle\n\nsims_measle |&gt; head()\n\n  week  .id reports  S  I  R  D\n1    1 data       0 NA NA NA NA\n2    2 data       0 NA NA NA NA\n3    3 data       2 NA NA NA NA\n4    4 data       0 NA NA NA NA\n5    5 data       3 NA NA NA NA\n6    6 data       0 NA NA NA NA\n\nsummary(sims_measle)\n\n      week         .id          reports              S              I          \n Min.   : 1   data   :  53   Min.   : 0.0000   Min.   :1132   Min.   :0.00000  \n 1st Qu.:14   1      :  53   1st Qu.: 0.0000   1st Qu.:1139   1st Qu.:0.00000  \n Median :27   2      :  53   Median : 0.0000   Median :1140   Median :0.00000  \n Mean   :27   3      :  53   Mean   : 0.1151   Mean   :1139   Mean   :0.06415  \n 3rd Qu.:40   4      :  53   3rd Qu.: 0.0000   3rd Qu.:1140   3rd Qu.:0.00000  \n Max.   :53   5      :  53   Max.   :75.0000   Max.   :1140   Max.   :5.00000  \n              (Other):5035                     NA's   :53     NA's   :53       \n       R               D          \n Min.   :36860   Min.   :0.00000  \n 1st Qu.:36861   1st Qu.:0.00000  \n Median :36861   Median :0.00000  \n Mean   :36862   Mean   :0.03396  \n 3rd Qu.:36862   3rd Qu.:0.00000  \n Max.   :36869   Max.   :3.00000  \n NA's   :53      NA's   :53       \n\nsims_measle |&gt;\n  ggplot(aes(x=week, y=reports, group=.id,color=(.id==\"data\"))) +\n  geom_line() +\n  guides(color=\"none\") +\n  theme_bw()\n\n\n\n\n\n\n\n\nObviously, the proposed set of parameters doesn’t seem to be right. We can play around with the parameters to see whether we can obtain better simulations.\n\n\nA SEIR model?"
  },
  {
    "objectID": "likelihood.html#likelihood-based-inference-for-pomp-models",
    "href": "likelihood.html#likelihood-based-inference-for-pomp-models",
    "title": "Simulation and Likelihood",
    "section": "Likelihood-based inference for POMP models",
    "text": "Likelihood-based inference for POMP models\n\nGoal: estimating the parameters\nWe are interested in estimating the parameters \\(\\theta = (\\beta, \\gamma, \\rho, k, S_0, I_0, R_0)\\) of the SIR model, given the weekly reported cases. The likelihood-based inference is to find the set of parameters that maximize the likelihood of the data, i.e., the probability of observing the data given the parameters. Now we break down this problem into two steps:\n\ncompute the likelihood of the data given the parameters;\nfind the set of parameters that maximize the likelihood.\n\nThis session will focus on the first step and we will use the pfilter function in pomp to compute the likelihood of the data given the model and parameters.\n\n\nRecap: the POMP structure\nThe POMP model consist of two components: the state process \\(X_t\\) and the observations \\(Y_t\\). The state process is Markovian, and transitions between states are driven by the process model, where we code it as sird_step; the observations are derived from the states, governed by a measurement model which we code in sird_rmeas for simulation and sird_dmeas for density computation.\n\n\n\nPOMP schematic. In previous example, we define the states as \\(X_t = (S_t, I_t, R_t, D_t)\\) and the process model as the SIR dynamics; the observations are the weekly reported cases and the measurement model is defined as a negative binomial distribution.\n\n\n\n\nLikelihood of the stochastic SIR model for measles\nIt will be straight-forward to compute the log-likelihood of \\(\\rho\\) and \\(k\\) of the data (i.e., weekly reports), given the underlying weekly cumulative diagnoses:\n\\[\n\\ell(\\rho, k) = \\sum_{w=1}^W\\, \\log p(report_w \\vert D_w, \\rho, k),\n\\]\nwhere \\(W\\) is the total number of reporting weeks and \\(p\\) is the probability mass function of Negative Binomial distribution. Unfortunately, we don’t have any knowledge of \\(D_t\\) because it is part of the unobserved underlying stochastic SIR dynamics, which we can simulate.\nNow, let’s keep in mind the observations \\(Y_w\\) is the weekly reported cases \\(\\mathrm{report}_w\\), and the unobserved states \\(X_t = (S_t, I_t, R_t, D_t)\\). We denote the density of the process model as \\(f_{X_w\\vert X_{w-1}}\\), the density of the measurement model as \\(f_{Y_w\\vert X_w}\\), the initialization density \\(f_{X_0}\\), and the set of parameters \\(\\theta = (\\beta, \\gamma, \\rho, k, S_0, I_0, R_0)\\). The likelihood for a POMP model takes the form of an integral:\n\\[\n\\mathcal{L}(\\theta) = \\int f_{X_0}(x_0;\\theta)\\, \\prod_{w=1}^W f_{Y_w\\vert X_w} (y_w^\\ast\\vert x_w;\\theta)\\,f_{X_w\\vert X_{w-1}}(x_w \\vert x_w;\\theta) d x_{0:W},\n\\]\nwhere \\(y_w^\\ast\\) is the data (reported cases) in \\(w\\)-th week.\n\n\nMonte Carlo likelihood by direct simulation\nThe intuitive idea is to simulate the sequence of states at each week, denoted as \\(X_{0:W}\\), and then the likelihood is given by\n\\[\\begin{aligned}\n  \\mathcal{L}(\\theta) &= \\int \\left\\{\\prod_{w=1}^W\\,f_{Y_w\\vert X_w}(y_w^\\ast\\vert x_w;\\theta)\\right\\} f_{X_{0:W}}(x_{0:W};\\theta) dx_{0:W} \\\\\n    &= \\mathbb{E}\\left[\\prod_{w=1}^W\\,f_{Y_w\\vert X_w}(y_w^\\ast\\vert x_w;\\theta)\\right],\n\\end{aligned}\\]\nwhere the expectation is taken with \\(X_{0:W}\\sim f_{X_{0:W}}(x_{0:W};\\theta)\\).\nUsing a law of large numbers, we can simulate a large set (\\(J\\)) of sequences \\(\\{X_{0:W}^j, j=1,\\dots,J\\}\\) and take the average of this Monte Carlo sample, which converges to the expectation: \\[\n\\mathcal{L}(\\theta) \\approx \\frac{1}{J}\\sum_{j=1}^J\\,\\prod_{w=1}^W\\,f_{Y_w\\vert X_w}(y_w^\\ast\\vert x_w;\\theta).\n\\]\nHowever, things are not that straight-forward and intuitive because this naive approach scales poorly with dimension, in other words, it requires a Monte Carlo effort (\\(J\\)) that scales exponentially with the length of the time series, and so is unfeasible on anything but a short data set. Another aspect to consider is that, the underlying process is stochastic, the simulations therefore diverge mostly, many of which don’t align with the observed data thus are useless for parameter estimation.\n\n\nSequential Monte Carlo: the particle filter\nThe idea of Sequential Monte Carlo (SMC) is that, instead of computing and evaluating the whole sequence of states at once, we evaluate the state at every time step. At current state, we evaluate whether it aligns with the data: if yes, then we keep it and continue to simulate the next state; if no, we just drop it. This procedure is called “importance sampling”.\nWe therefore can factorize the likelihood:\n\\[\\begin{aligned}\n  \\mathcal{L}(\\theta) &= f_{Y_{1:W}}(y_{1:W}^\\ast;\\theta) = \\prod_{w=1}^W\\,f_{Y_w\\vert Y_{1:w-1}}(y_w^\\ast\\vert y^\\ast_{1:w-1};\\theta) \\\\\n  &= \\prod_{w=1}^W\\,\\int f_{Y_w\\vert X_w}(y_w^\\ast \\vert x_w;\\theta) f_{X_w\\vert Y_{1:w-1}}(x_w\\vert y_{1:w-1}^\\ast) d x_w,\n\\end{aligned}\\]\nwhere obviously \\(f_{X_1\\vert Y_{1:0}} = f_{X_1}\\). With the Markov property and the Baye’s theorem, we can break the factorization into two steps, predition and filtering:\n\nthe prediction formula, gives the prediction at time \\(t_w\\) conditioned on the filtering distribution at time \\(t_{w-1}\\):\n\n\\[\\begin{aligned}\n  & f_{X_w\\vert Y_{1:w-1}}(x_w\\vert y_{1:w-1}^\\ast;\\theta) \\\\\n  & \\qquad = \\int f_{X_w\\vert X_{w-1}}(x_w\\vert x_{w-1};\\theta) f_{X_{w-1}\\vert Y_{1:w-1}} (x_{w-1}\\vert y_{1:w-1}^\\ast) d x_{w-1};\n\\end{aligned}\\]\n\nthe filtering formula, gives the filtering distribution at time \\(t_w\\) using the prediction distribution at time \\(t_w\\):\n\n\\[\\begin{aligned}\n  & f_{X_w\\vert Y_{1:w}} (x_w\\vert y_{1:w}^\\ast;\\theta) \\\\\n  & \\qquad = f_{X_w\\vert Y_w, Y_{1:w-1}} (x_w \\vert y_w^\\ast, y_{1:w-1}^\\ast;\\theta) \\\\\n  & \\qquad = \\dfrac{f_{Y_w\\vert X_w}(y_w^\\ast\\vert x_w;\\theta)\\,f_{X_w\\vert Y_{1:w-1}}(x_w\\vert y_{1:w-1}^\\ast;\\theta)}{\\int f_{Y_w\\vert X_w} (y_{w}^\\ast\\vert u_w;\\theta)\\,f_{X_w\\vert Y_{1:w-1}}(u_w\\vert y_{1:w-1}^\\ast;\\theta)} d u_w.\n\\end{aligned}\\]\nWe now can have a one-step particle filtering as follows:\n\nSuppose we have a set of \\(J\\) samples for the state \\(X_{w-1}^F, j=1,\\dots,J\\) drawn from the filtering distribution at time \\(t_{w-1}\\);\nWe then can obtain a set of samples \\(X_{w, j}^P\\) drawn from the prediction distribution at time \\(t_w\\) by simulating the process model: \\[\nX_{w,j}^P \\sim \\mathrm{process}(X_{w-1}^F,\\theta), j=1,\\dots,J;\n\\]\nGiven the set of samples for the state at time \\(t_w\\), we can obtain the filtering distribution by resampling from this set \\(\\{X_{w,j}^P, j=1,\\dots,J\\}\\) with weights (the density of the measurement model): \\[\n\\nu_{w,j} = f_{Y_w\\vert X_w}(y_w^\\ast\\vert X_{w,j}^P;\\theta);\n\\]\nThe Monte Carlo principle (law of large numbers) gives the approximated likelihood: \\[\n\\hat{\\mathcal{L}}_w(\\theta) \\approx \\frac{1}{J} \\sum_j f_{Y_w\\vert X_w} (y_{w}^\\ast \\vert X_{w,j}^P;\\theta)\n\\] since \\(X_{w,j}^P\\) is approximately a draw from \\(f_{X_w\\vert Y_{1:w-1}} (x_w \\vert y_{1:w-1}^\\ast;\\theta)\\);\n\nThen we can iterate through the end of the time and the approximated full log-likelihood is given by: \\[\n\\ell(\\theta) = \\log \\mathcal{L}(\\theta) = \\sum_w \\log \\mathcal{L}_{w}(\\theta) \\approx \\sum_w \\hat{\\mathcal{L}}_{w}(\\theta).\n\\]"
  },
  {
    "objectID": "likelihood.html#paticle-filtering-in-pomp",
    "href": "likelihood.html#paticle-filtering-in-pomp",
    "title": "Simulation and Likelihood",
    "section": "Paticle filtering in pomp",
    "text": "Paticle filtering in pomp\nWe can now try to compute the log-likelihood in pomp using the function pfilter with \\(J=5000\\):\n\nsird_measle |&gt;\n  pfilter(\n    params = c(\n      Beta = 7.5, gamma = .5, rho = .5, k = 10,\n      S0 = 1140, I0 = 1, R0 = 36860\n    ),\n    Np=5000\n  ) -&gt; pf\nlogLik(pf)\n\n[1] -Inf\n\n\nWe can try another set of parameters:\n\nsird_measle |&gt;\n  pfilter(\n    params = c(\n      Beta = 15, gamma = .5, rho = .5, k = 10,\n      S0 = 2280, I0 = 1, R0 = 35720\n    ),\n    Np=5000\n  ) |&gt;\n  logLik()\n\n[1] -136.1367\n\n\nUsing parallel computation, we can see the average log-likelihood and the variation in log-likelihoods:\n\nregisterDoParallel(cores=detectCores()-1)\nforeach (\n  i=1:10, .combine=c, .options.future=list(seed=652643293)\n  ) %dopar% {\n  sird_measle |&gt;\n  pfilter(\n    params = c(\n      Beta = 15, gamma = .5, rho = .5, k = 10,\n      S0 = 2280, I0 = 1, R0 = 35720\n    ),\n    Np=5000\n  )\n} -&gt; pf\nlogLik(pf) -&gt; ll\nlogmeanexp(ll,se=TRUE)\n\n       est         se \n-130.43782    2.57487"
  },
  {
    "objectID": "likelihood.html#slice-likelihood-changing-one-specific-parameter",
    "href": "likelihood.html#slice-likelihood-changing-one-specific-parameter",
    "title": "Simulation and Likelihood",
    "section": "Slice likelihood: changing one specific parameter",
    "text": "Slice likelihood: changing one specific parameter\n\nsird_measle |&gt;\n  pomp(\n    params =  c(\n      Beta = 15, gamma = .5, rho = .5, k = 10,\n      S0 = 2280, I0 = 1, R0 = 35720\n    )\n  ) -&gt; sird_measle\n\nslice_design(\n  center=coef(sird_measle),\n  Beta = rep(seq(from=5,to=30,length=40),each=3),\n  gamma = rep(seq(from=0.2,to=2,length=40),each=3)\n) -&gt; params_slice\n\nhead(params_slice)\n\n      Beta gamma rho  k   S0 I0    R0 slice\n1 5.000000   0.5 0.5 10 2280  1 35720  Beta\n2 5.000000   0.5 0.5 10 2280  1 35720  Beta\n3 5.000000   0.5 0.5 10 2280  1 35720  Beta\n4 5.641026   0.5 0.5 10 2280  1 35720  Beta\n5 5.641026   0.5 0.5 10 2280  1 35720  Beta\n6 5.641026   0.5 0.5 10 2280  1 35720  Beta\n\nsummary(params_slice)\n\n      Beta           gamma            rho            k            S0      \n Min.   : 5.00   Min.   :0.200   Min.   :0.5   Min.   :10   Min.   :2280  \n 1st Qu.:15.00   1st Qu.:0.500   1st Qu.:0.5   1st Qu.:10   1st Qu.:2280  \n Median :15.00   Median :0.500   Median :0.5   Median :10   Median :2280  \n Mean   :16.25   Mean   :0.800   Mean   :0.5   Mean   :10   Mean   :2280  \n 3rd Qu.:17.34   3rd Qu.:1.088   3rd Qu.:0.5   3rd Qu.:10   3rd Qu.:2280  \n Max.   :30.00   Max.   :2.000   Max.   :0.5   Max.   :10   Max.   :2280  \n       I0          R0          slice    \n Min.   :1   Min.   :35720   Beta :120  \n 1st Qu.:1   1st Qu.:35720   gamma:120  \n Median :1   Median :35720              \n Mean   :1   Mean   :35720              \n 3rd Qu.:1   3rd Qu.:35720              \n Max.   :1   Max.   :35720              \n\n\nNow we can run the computations of likelihood at different combinations of parameters and visualize:\n\nforeach (\n  theta=iter(params_slice,\"row\"), .combine=rbind, .options.future=list(seed=108028909)\n  ) %dopar% {\n    sird_measle |&gt; pfilter(params=theta,Np=5000) -&gt; pf\n    theta$loglik &lt;- logLik(pf)\n    theta\n} -&gt; llks_slice\n\nsummary(llks_slice)\n\n      Beta           gamma            rho            k            S0      \n Min.   : 5.00   Min.   :0.200   Min.   :0.5   Min.   :10   Min.   :2280  \n 1st Qu.:15.00   1st Qu.:0.500   1st Qu.:0.5   1st Qu.:10   1st Qu.:2280  \n Median :15.00   Median :0.500   Median :0.5   Median :10   Median :2280  \n Mean   :16.25   Mean   :0.800   Mean   :0.5   Mean   :10   Mean   :2280  \n 3rd Qu.:17.34   3rd Qu.:1.088   3rd Qu.:0.5   3rd Qu.:10   3rd Qu.:2280  \n Max.   :30.00   Max.   :2.000   Max.   :0.5   Max.   :10   Max.   :2280  \n       I0          R0          slice         loglik      \n Min.   :1   Min.   :35720   Beta :120   Min.   :-462.5  \n 1st Qu.:1   1st Qu.:35720   gamma:120   1st Qu.:-176.8  \n Median :1   Median :35720               Median :-136.4  \n Mean   :1   Mean   :35720               Mean   :-164.0  \n 3rd Qu.:1   3rd Qu.:35720               3rd Qu.:-129.7  \n Max.   :1   Max.   :35720               Max.   :-111.8  \n\nllks_slice |&gt;\n  pivot_longer(c(Beta,gamma)) |&gt;\n  filter(name==slice) |&gt;\n  ggplot(aes(x=value,y=loglik,color=name))+\n  geom_point()+\n  facet_grid(~name,scales=\"free_x\")+\n  guides(color=\"none\")+\n  labs(x=\"parameter value\",color=\"\")"
  },
  {
    "objectID": "simulation.html",
    "href": "simulation.html",
    "title": "Simulation",
    "section": "",
    "text": "Compartment model introduction\n\nSIR model, deterministic\nSIR model, stochastic\n\nModel with observations\n\nSIR model with a diagnosis compartment\n\nPartially Observed Markov Process (POMP)\n\nstate process and measurement model\nSIR model in a POMP model"
  },
  {
    "objectID": "simulation.html#summary",
    "href": "simulation.html#summary",
    "title": "Simulation",
    "section": "",
    "text": "Compartment model introduction\n\nSIR model, deterministic\nSIR model, stochastic\n\nModel with observations\n\nSIR model with a diagnosis compartment\n\nPartially Observed Markov Process (POMP)\n\nstate process and measurement model\nSIR model in a POMP model"
  },
  {
    "objectID": "simulation.html#installation-of-related-packages",
    "href": "simulation.html#installation-of-related-packages",
    "title": "Simulation",
    "section": "Installation of related packages",
    "text": "Installation of related packages\nWe will be using pomp, tidyverse, and ggplot2 across this series of sessions. For Windows users, please go to https://cran.r-project.org/bin/windows/Rtools/, download and install RTools 4.4 first. For all platform users, please install pomp by running command\ninstall.packages(\"pomp\")\nwithin RStudio Console."
  },
  {
    "objectID": "simulation.html#introduction-to-epidemiological-compartmental-models",
    "href": "simulation.html#introduction-to-epidemiological-compartmental-models",
    "title": "Simulation",
    "section": "Introduction to Epidemiological Compartmental Models",
    "text": "Introduction to Epidemiological Compartmental Models\nCompartment models, such as Susceptible-Infected-Recovered (SIR) model, divide the total population into different groups (i.e., compartments) and model the dynamics for each group by incorporating transitions between them. We can use diagrams to illustrate the model structures and the events (transitions) between compartments and formulate the dynamics using ODEs.\n\nExample: SIR model\n\n\n\nSIR model diagram. Three compartments are included in this presented SIR model, with two events: transmission (i.e. S to I) with per-capita rate \\(\\beta\\,I\\) and recovery (i.e. I to R) with per-capita rate \\(\\gamma\\).\n\n\n\\[\\begin{aligned}\n  \\frac{d S}{d t} &= -\\beta S \\frac{I}{N} \\\\\n  \\frac{d I}{d t} &= \\beta S \\frac{I}{N} - \\gamma I \\\\\n  \\frac{d R}{d t} &= \\gamma I \\\\\n  N &= S + I + R\n\\end{aligned}\\]"
  },
  {
    "objectID": "simulation.html#simulate-the-sir-model",
    "href": "simulation.html#simulate-the-sir-model",
    "title": "Simulation",
    "section": "Simulate the SIR model",
    "text": "Simulate the SIR model\n\nA deterministic model\nBased on the ODEs, we can write a function to simulate one-step-transition of the deterministic SIR model:\n\nsir_determ_step &lt;- function (S, I, R, N, Beta, gamma, delta.t, ...) {\n  dN_SI &lt;- Beta * I * S / N * delta.t\n  dN_IR &lt;- gamma * I * delta.t\n  S &lt;- S - dN_SI\n  I &lt;- I + dN_SI - dN_IR\n  R &lt;- R + dN_IR\n  return (c(S = S, I = I, R = R))\n}\n\nTo simulate the full dynamics of the proposed SIR model, we also need the initial values for \\(S\\), \\(I\\), and \\(R\\), which we denote as \\(S_0\\), \\(I_0\\), and \\(R_0\\).\n\nsir_init &lt;- function (S0, I0, R0) {\n  return(c(S = S0, I = I0, R = R0))\n}\n\nNow we can combine the initialization function and the one-step-transition function to simulate the deterministic SIR dynamics:\n\n# initialize the system\ninits &lt;- sir_init(S0=9999, I0=1, R0=0)\n# restore the results\nresults &lt;- data.frame(t = 0, S = inits[1], I = inits[2], R = inits[3])\n# the end time and time step\nt &lt;- 0\ntend &lt;- 200\ndelta.t &lt;- 1\n# parameter setting\nparams &lt;- c(Beta = 0.1, gamma = 0.01)\nwhile (t &lt; tend) {\n  t &lt;- t + delta.t\n  prev_states &lt;- results[nrow(results),]\n  current_states &lt;- sir_determ_step(\n    S = prev_states$S,\n    I = prev_states$I,\n    R = prev_states$R,\n    N = prev_states$S + prev_states$I + prev_states$R,\n    Beta = params[\"Beta\"],\n    gamma = params[\"gamma\"],\n    delta.t = delta.t\n  )\n  results &lt;- rbind(\n    results,\n    c(t = t, S = current_states[1], I = current_states[2], R = current_states[3])\n  )\n}\n\nlibrary(dplyr)\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\nlibrary(ggplot2)\n\nggplot(data = results, aes(x = t)) +\n  geom_line(aes(y = S), color=\"darkgreen\") + \n  geom_line(aes(y = I), color=\"red\")+ \n  geom_line(aes(y = R), color=\"blue\") +\n  labs(x = \"time\", y = \"population\") +\n  theme_minimal() -&gt; p_determin\n\np_determin\n\n\n\n\n\n\n\n# zoom-in\np_determin +\n  coord_cartesian(xlim=c(120,130), ylim=c(6e3, 7e3))\n\n\n\n\n\n\n\n\n\n\nA stochastic model\nTo make the model more realistic, we incorporate some level of stochasticity in each transitions assuming the number of individuals in each transitions follows a binomial distribution:\n\nsir_stoch_step &lt;- function (S, I, R, N, Beta, gamma, delta.t, ...) {\n  dN_SI &lt;- rbinom(n=1, size=S, prob=1-exp(-Beta * I / N * delta.t))\n  dN_IR &lt;- rbinom(n=1, size=I, prob=1-exp(-gamma * delta.t))\n  S &lt;- S - dN_SI\n  I &lt;- I + dN_SI - dN_IR\n  R &lt;- R + dN_IR\n  return (c(S = S, I = I, R = R))\n}\n\nAgain we can simulate this stochastic SIR model:\n\n# initialize the system\ninits &lt;- sir_init(S0=9999, I0=1, R0=0)\n# restore the results\nresults &lt;- data.frame(t = 0, S = inits[1], I = inits[2], R = inits[3])\n# the end time and time step\nt &lt;- 0\ntend &lt;- 200\ndelta.t &lt;- 1\n# parameter setting\nparams &lt;- c(Beta = 0.1, gamma = 0.01)\nwhile (t &lt; tend) {\n  t &lt;- t + delta.t\n  prev_states &lt;- results[nrow(results),]\n  current_states &lt;- sir_stoch_step(\n    S = prev_states$S,\n    I = prev_states$I,\n    R = prev_states$R,\n    N = prev_states$S + prev_states$I + prev_states$R,\n    Beta = params[\"Beta\"],\n    gamma = params[\"gamma\"],\n    delta.t = delta.t\n  )\n  results &lt;- rbind(\n    results,\n    c(t = t, S = current_states[1], I = current_states[2], R = current_states[3])\n  )\n}\n\nlibrary(dplyr)\nlibrary(ggplot2)\n\nggplot(data = results, aes(x = t)) +\n  geom_line(aes(y = S), color=\"darkgreen\") + \n  geom_line(aes(y = I), color=\"red\") + \n  geom_line(aes(y = R), color=\"blue\") +\n  labs(x = \"time\", y = \"population\") +\n  theme_minimal() -&gt; p_stoch\n\np_stoch\n\n\n\n\n\n\n\np_stoch +\n  coord_cartesian(xlim=c(120,130), ylim=c(6e3, 7e3))\n\n\n\n\n\n\n\n\n\n\nTry some other models?"
  },
  {
    "objectID": "simulation.html#sir-model-observations",
    "href": "simulation.html#sir-model-observations",
    "title": "Simulation",
    "section": "SIR model + observations",
    "text": "SIR model + observations\nUnfortunately, we don’t observed all states (\\(S\\), \\(I\\), and \\(R\\)).\n\nSIR model with a diagnosis compartment\nWe assume that the infections (i.e. patients) go to the hospitial and get diagnosed and denote this new compartment as \\(D\\):\n\n\n\nSIR + Diagnosis model diagram.\n\n\n\\[\\begin{aligned}\n  \\frac{d S}{d t} &= -\\beta S \\frac{I}{N} \\\\\n  \\frac{d I}{d t} &= \\beta S \\frac{I}{N} - \\gamma I \\\\\n  \\frac{d R}{d t} &= \\gamma I \\\\\n  \\frac{d D}{d t} &= \\gamma I \\\\\n  N &= S + I + R\n\\end{aligned}\\]\nNow we define a new one-step-transition function:\n\nsird_stoch_step &lt;- function (S, I, R, N, D, Beta, gamma, delta.t, ...) {\n  dN_SI &lt;- rbinom(n=1, size=S, prob=1-exp(-Beta * I / N * delta.t))\n  dN_IR &lt;- rbinom(n=1, size=I, prob=1-exp(-gamma * delta.t))\n  S &lt;- S - dN_SI\n  I &lt;- I + dN_SI - dN_IR\n  R &lt;- R + dN_IR\n  D &lt;- D + dN_IR\n  return (c(S = S, I = I, R = R, D = D))\n}\n\nwith the new initialization function:\n\nsird_init &lt;- function (S0, I0, R0, D0=0) {\n  return(c(S = S0, I = I0, R = R0, D = D0))\n}"
  },
  {
    "objectID": "simulation.html#partially-observed-markov-process-pomp",
    "href": "simulation.html#partially-observed-markov-process-pomp",
    "title": "Simulation",
    "section": "Partially Observed Markov Process (POMP)",
    "text": "Partially Observed Markov Process (POMP)\nWhat’s worse, we cannot even observe the true values of any state. We only have partial observations within a period of time, let’s say, 7 days.\n\nState process and measurement model\nIn the POMP model, we have two main components: the underlying state process and measurement model to the observations.\n\n\n\nPOMP structure.\n\n\nLet’s break it into details, defining the underlying state process as \\(X_t\\) and the observed measurements as \\(Y_t\\), both of which are markovian.\n\n\n\nPOMP schematic.\n\n\n\n\nSIR model in a POMP model\nNow we can define the state process as a stochastic SIR model, \\(X_t = (S_t, I_t, R_t, D_t)\\), where we call \\(D_t\\) as the weekly cumulative diagnoses, which resets to 0 every 7 days.\nWe further can define a weekly measurement model: \\[\n\\textrm{reports}_t \\sim \\textrm{NegBin}(\\rho D_t, k),\n\\] where \\(\\rho\\) is the reporting ratio and \\(k\\) is the dispersion parameter in the Negative Binomial distribution. Of course, we can also define the measurement model as a Binomial distribution or Poisson distribution.\nUsing this measurement model, we can have the probability a case report given the true diagnoses, as well as simulate the reported cases based on the SIR with diagnosis model:\n\n## density/probability\nsird_dmeas &lt;- function (reports, D, rho, k, log, ...) {\n  dnbinom(x=reports, size=k, mu=rho*D, log=log)\n}\n\n## simulation\nsird_rmeas &lt;- function (D, rho, k, ...) {\n  return (c(reports=rnbinom(n=1, size=k, mu=rho*D)))\n}\n\nWe can update the one-step transition function:\n\nsird_stoch_pomp_step &lt;- function (S, I, R, N, D, Beta, gamma, t, delta.t, cum_step, ...) {\n  dN_SI &lt;- rbinom(n=1, size=S, prob=1-exp(-Beta * I / N * delta.t))\n  dN_IR &lt;- rbinom(n=1, size=I, prob=1-exp(-gamma * delta.t))\n  S &lt;- S - dN_SI\n  I &lt;- I + dN_SI - dN_IR\n  R &lt;- R + dN_IR\n  if (t %% cum_step == 1)  D &lt;- 0\n  D &lt;- D + dN_IR\n  return (c(S = S, I = I, R = R, D = D))\n}\n\nNow we can simulate the underlying SIRD models and the reported cases:\n\n# initialize the system\ninits &lt;- sird_init(S0=9999, I0=1, R0=0, D0=0)\n# restore the results\nresults &lt;- data.frame(t = 0, S = inits[1], I = inits[2], R = inits[3], D = inits[4], reports = 0)\n# the end time and time steps\nt &lt;- 0\ntend &lt;- 200\ndelta.t &lt;- 1\ncum_step &lt;- 7\n# parameter setting\nparams &lt;- c(Beta = 0.1, gamma = 0.01, rho=0.2, k=3)\nwhile (t &lt; tend) {\n  t &lt;- t + delta.t\n  prev_states &lt;- results[nrow(results),]\n  current_states &lt;- sird_stoch_pomp_step(\n    S = prev_states$S,\n    I = prev_states$I,\n    R = prev_states$R,\n    N = prev_states$S + prev_states$I + prev_states$R,\n    D = prev_states$D, \n    Beta = params[\"Beta\"],\n    gamma = params[\"gamma\"],\n    t = t,\n    delta.t = delta.t,\n    cum_step = cum_step\n  )\n  reports &lt;- 0\n  if (t %% cum_step == 0)\n    reports &lt;- sird_rmeas(D = current_states[4], rho = params[\"rho\"], k = params[\"k\"])\n  results &lt;- rbind(\n    results,\n    c(t = t, S = current_states[1], I = current_states[2], R = current_states[3], D = current_states[4], reports = reports[1])\n  )\n}\n\nlibrary(dplyr)\nlibrary(ggplot2)\n\nggplot(data = results, aes(x = t)) +\n  geom_line(aes(y = S), color=\"darkgreen\") + \n  geom_line(aes(y = I), color=\"red\") + \n  geom_line(aes(y = R), color=\"blue\") +\n  geom_line(aes(y = D), color=\"grey\") +\n  geom_step(aes(y = reports), color=\"black\") +\n  labs(x = \"time\", y = \"population\") +\n  scale_y_log10() +\n  theme_minimal() -&gt; p_stoch_pomp\n\np_stoch_pomp\n\nWarning in scale_y_log10(): log-10 transformation introduced infinite values.\nlog-10 transformation introduced infinite values.\nlog-10 transformation introduced infinite values."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Simulation-based Inference for Epidemiological Dynamics",
    "section": "",
    "text": "This module introduces statistical inference techniques and computational methods for dynamic models of epidemiological systems. The course will explore deterministic and stochastic formulations of epidemiological dynamics and develop inference methods appropriate for a range of models. Special emphasis will be on exact and approximate likelihood as the key elements in parameter estimation, hypothesis testing, and model selection. Specifically, the course will cover sequential Monte Carlo and synthetic likelihood techniques. Students will learn to implement these in R to carry out maximum likelihood and Bayesian inference. Knowledge of the material in Module 1 is assumed. Students new to R should complete a tutorial before the module."
  },
  {
    "objectID": "index.html#module-description",
    "href": "index.html#module-description",
    "title": "Simulation-based Inference for Epidemiological Dynamics",
    "section": "",
    "text": "This module introduces statistical inference techniques and computational methods for dynamic models of epidemiological systems. The course will explore deterministic and stochastic formulations of epidemiological dynamics and develop inference methods appropriate for a range of models. Special emphasis will be on exact and approximate likelihood as the key elements in parameter estimation, hypothesis testing, and model selection. Specifically, the course will cover sequential Monte Carlo and synthetic likelihood techniques. Students will learn to implement these in R to carry out maximum likelihood and Bayesian inference. Knowledge of the material in Module 1 is assumed. Students new to R should complete a tutorial before the module."
  },
  {
    "objectID": "index.html#course-objectives",
    "href": "index.html#course-objectives",
    "title": "Simulation-based Inference for Epidemiological Dynamics",
    "section": "Course objectives",
    "text": "Course objectives\n\nTo introduce partially observed Markov process (POMP) models as tools for scientific investigation and public health policy.\nTo give students the ability to formulate POMP models of their own.\nTo teach efficient approaches for performing scientific inference using POMP models.\nTo familiarize students with the pomp package.\nTo give students opportunities to work with such inference methods.\nTo provide documented examples for student re-use."
  },
  {
    "objectID": "index.html#lessons",
    "href": "index.html#lessons",
    "title": "Simulation-based Inference for Epidemiological Dynamics",
    "section": "Lessons",
    "text": "Lessons\n\nInstructions for preparing your laptop for the course exercises.\nIntroduction: What is “Simulation-based Inference for Epidemiological Dynamics”? POMPs and pomp.\nSimulation of stochastic dynamic models.\nLikelihood for POMPs: theory and practice.\nIterated filtering: theory and practice.\nCase study: measles. Recurrent epidemics, long time series, covariates, extra-demographic stochasticity, interpretation of parameter estimates.\nCase study: polio. Workflow for a real research problem.\nCase study: Ebola. Model diagnostics and forecasting.\nCase study: HIV and fluctuating sexual contact rates. Panel data."
  },
  {
    "objectID": "index.html#credits",
    "href": "index.html#credits",
    "title": "Simulation-based Inference for Epidemiological Dynamics",
    "section": "Credits",
    "text": "Credits\nThis website and all the materials are adapted from https://kingaa.github.io/sbied/. We would like send our sincere gratitude to Professors Aaron A. King and Edward L. Ionides for creating this wonderful course and for helping us developing our own version."
  },
  {
    "objectID": "iterated.html",
    "href": "iterated.html",
    "title": "Maximum likelihood estimation and iterated filtering",
    "section": "",
    "text": "Model implementation in pomp\nCreating a pomp object with different componenets\nComputing likelihood using pfilter"
  },
  {
    "objectID": "iterated.html#summary",
    "href": "iterated.html#summary",
    "title": "Maximum likelihood estimation and iterated filtering",
    "section": "",
    "text": "Model implementation in pomp\nCreating a pomp object with different componenets\nComputing likelihood using pfilter"
  },
  {
    "objectID": "iterated.html#goal-estimating-the-parameters",
    "href": "iterated.html#goal-estimating-the-parameters",
    "title": "Maximum likelihood estimation and iterated filtering",
    "section": "Goal: estimating the parameters",
    "text": "Goal: estimating the parameters\nWe are interested in estimating the parameters \\(\\theta = (\\beta, \\gamma, \\rho, k, S_0, I_0, R_0)\\) of the SIR model, given the weekly reported cases. The likelihood-based inference is to find the set of parameters that maximize the likelihood of the data, i.e., the probability of observing the data given the parameters. Now we break down this problem into two steps:\n\ncompute the likelihood of the data given the parameters;\nfind the set of parameters that maximize the likelihood.\n\nThis session will focus on the second step and we will use the mif2 function in pomp to compute the maximum likelihood estimate (MLE) of the parameters."
  },
  {
    "objectID": "iterated.html#an-iterated-filtering-algorithm-if2",
    "href": "iterated.html#an-iterated-filtering-algorithm-if2",
    "title": "Maximum likelihood estimation and iterated filtering",
    "section": "An iterated filtering algorithm (IF2)",
    "text": "An iterated filtering algorithm (IF2)\nSince we already learn how to compute the likelihood of the data given the parameters using Sequential Monte Carlo (SMC) method, implemented in pfilter function in pomp, now we would like to explore the space of parameters to find the set of parameters that maximize the likelihood.\nThe iterated filtering algorithm (IF2) consists of multiple rounds of iterations, call it \\(M\\). In each round of iteration, we have \\(J\\) particles, carrying out with the parameter vector. Assume that we have data \\(y^\\ast_{1:W}\\).\nWe can consider this procedure as a two-layer process:\n\nOuter loop: run through \\(M\\) rounds of iterations:\n\n\nat the beginning of each round \\(m\\), the particles resulted in the last time step from the previous round \\(m-1\\) are recycled, with a random walk perturbation;\nthe particles run through the inner loop to update the weights and the parameters are updated based on the likelihood of the data given the parameters.\n\n\nInner loop: run through \\(W\\) time steps:\n\n\nat the beginning of each time step \\(w\\), the particles are sampled from those in the last time step \\(w-1\\), with a random walk perturbation;\nthe current state at time \\(w\\) are updated based on the stochastic compartmental model (e.g., SIR model);\nthe weights are updated based on the likelihood of the data given the parameters;\nthe particles are resampled based on the weights.\n\n\n\n\nIterated filtering diagram.\n\n\n\nAnalogy with evolutiion by natural selection\n\nThe parameters characterize the genotype.\nThe swarm of particles is a population.\nThe likelihood, a measure of the compatibility between the parameters and the data, is the analogue of fitness.\nEach successive observation is a new generation.\nSince particles reproduce in each generation in proportion to their likelihood, the particle filter acts like natural selection.\nThe artificial perturbations augment the “genetic” variance and therefore correspond to mutation.\nIF2 increases the fitness of the population of particles.\nHowever, because our scientific interest focuses on the model without the artificial perturbations, we decrease the intensity of the latter with successive iterations.\n\n\n\nIF2 pseudocode\nInput:\n\nsimulators for \\(f_{X_0}(x_0;\\theta)\\) and \\(f_{X_w\\vert X_{w-1}}(x_w\\vert x_{w-1};\\theta)\\);\nevaluator for \\(f_{Y_w\\vert X_w}(y_w\\vert x_w;\\theta)\\);\ndata, \\(y^\\ast_{1:W}\\);\n\nAlgorithm parameters and mif2 arguments\n\nnumber of iterations, Nmif=\\(M\\)\nnumber of particles, Np=\\(J\\)\ninitial parameter vector swarm, params=\\(\\left\\{\\Theta_j^0, j=1,\\dots,J\\right\\}\\)\nrandom walk standard deviation for each parameter within the vector, rw.sd, such that a diagonal variance matrix \\(V_w\\) can be constructed\ncooling fraction every 50 iterations, cooling.fraction.50 = \\(a\\)\n\nOutput\n\nfinal parameter vector swarm after \\(M\\) iterations, \\(\\left\\{\\Theta_j^M, j=1,\\dots,J\\right\\}\\)\n\nProcedure\n\n\n\\begin{algorithm} \\caption{Iterated filtering algorithm (IF2)} \\begin{algorithmic} \\For{$m=1$ to $M$} \\State $\\Theta_{0,j}^{F,m} \\sim \\mathrm{Normal}(\\Theta_j^{m-1}, V_0\\,a^{2m/50})$ for $j=1$ to $J$ \\State $X_{0,j}^{F,m} \\sim f_{X_0}(x_0;\\Theta_{0,j}^{F,m})$ for $j=1$ to $J$ \\For{$w=1$ to $W$} \\State $\\Theta_{w,j}^{P,m} \\sim \\mathrm{Normal}(\\Theta_{w-1,j}^{F,m}, V_n\\, a^{2m/50})$ for $j=1$ to $J$ \\State $X_{n,j}^{P,m} \\sim f_{X_w\\vert X_{w-1}}(x_w\\vert X_{w-1}^{F,m};\\Theta_{w,j}^{P,m})$ for $j=1$ to $J$ \\State $\\omega_{n,j}^{P,m} = f_{Y_w\\vert X_w}(y_w^\\ast\\vert X_{w,j}^{P,m};\\Theta_{w,j}^{P,m})$ for $j=1$ to $J$ \\State Draw $k_{1:J}$ with $P[k_j=i]=\\omega_{w,j}^m/\\sum_{u=1}^J\\,\\omega_{w,u}$ \\State $\\Theta_{w,j}^{F,m} = \\Theta_{w,k_j}^{P,m}$ and $X_{w,j}^{F,m}=X_{w,k_j}^{P,m}$ for $j=1$ to $J$ \\EndFor \\State Set $\\Theta_j^m = \\Theta_{W,j}^{F,m}$ for $j=1$ to $J$ \\EndFor \\end{algorithmic} \\end{algorithm}\n\n\n\n\nParticle filter in pomp for the Consett measles outbreak\n\nRecap one wave of the Consett measles outbreak:\n\n\nsource(\"model.R\")\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\n\nRows: 53 Columns: 2\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\ndbl (2): week, cases\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\ndat_meas |&gt;\n  ggplot(aes(x=week, y=reports)) +\n  geom_line() +\n  geom_point() +\n  theme_bw()\n\n\n\n\n\n\n\n\n\nTest the particle filter with pre-specified parameters:\n\n\nsird_measle |&gt;\n  pfilter(\n    params =  c(\n      Beta = 15, gamma = .5, rho = .5, k = 10,\n      S0 = 2280, I0 = 1, R0 = 35720\n    ),\n    Np=1000\n  ) -&gt; pf\n\nplot(pf)\n\n\n\n\n\n\n\n\nThe above plot shows the data (reports), along with the effective sample size (ESS) of the particle filter (ess) and the log-likelihood of each observation conditional on the preceding ones (cond.logLik). The ESS is the equivalent number of independent particles. In this case, the ESS appears to be everywhere adequate.\n\nsetting up the estimation problem\n\nAssume the initial population sizes are known, that is \\(S_0=2280, I_0 = 1, R_0 = 35720\\). We also assume that the infectious period is 2 weeks, i.e., \\(\\gamma = 0.5\\). We would also like to fix the over-dispersion parameter \\(k=10\\). We can relax these assumptions later. By fixing these parameters, we can constrain the parameter space and try our best to avoid un-identifiability. We can then proceed to estimate the transmission rate \\(\\beta\\) and the reporting ratio \\(\\rho\\), with initial guesses \\(\\beta=15\\) and \\(\\rho=0.5\\).\n\nfixed_params &lt;- c(\n  S0 = 2280, I0 = 1, R0 = 35720,\n  gamma = .5, k = 10\n)\n\ncoef(sird_measle, names(fixed_params)) &lt;- fixed_params\n\nWarning: in 'coef&lt;-': names of 'value' are being discarded.\n\nlibrary(foreach)\nlibrary(doParallel)\n\nLoading required package: iterators\n\n\nLoading required package: parallel\n\nregisterDoParallel(cores=detectCores()-1)\nforeach(i=1:10,.combine=c, .options.future=list(seed=TRUE)) %dopar% {\n  sird_measle |&gt; pfilter(Np=5000)\n} -&gt; pf\npf |&gt; logLik() |&gt; logmeanexp(se=TRUE) -&gt; L_pf\nL_pf\n\n        est          se \n-131.880413    1.645576 \n\n\n\npf[[1]] |&gt; coef() |&gt; bind_rows() |&gt;\n  bind_cols(loglik=L_pf[1],loglik.se=L_pf[2]) |&gt;\n  write_csv(\"measles_params.csv\")"
  },
  {
    "objectID": "iterated.html#global-search-and-mle",
    "href": "iterated.html#global-search-and-mle",
    "title": "Maximum likelihood estimation and iterated filtering",
    "section": "Global Search and MLE",
    "text": "Global Search and MLE\nTo avoid confusion, we have the following terminology:\n\ninitial guess: the initial parameter vector \\(\\Theta^0_j\\)\ninitial state: the initial state of population model \\(X_{0,j}\\) at time \\(t_0\\)\n\nFor our measles model, a box containing reasonable parameter values might be \\(\\beta\\in(5,80)\\) and \\(\\rho\\in(0.2,0.9)\\). Now we can build a group of 400 initial guesses to cover this box:\n\nset.seed(2062379496)\nrunif_design(\n  lower=c(Beta=5,rho=0.2),\n  upper=c(Beta=80,rho=0.9),\n  nseq=400\n) -&gt; guesses\n\nmf1 &lt;- mifs_local[[1]]  # retrieve the local search as the model template\n\nWith the group of 400 initial guesses, we can now apply the iterated filtering algorithm to find the global MLE:\n\nthe codes run ONE local search for each of 400 initial guesses\neach local search consists of two runs: the first run with 50 iterations, followed by the second with 100 iterations\na general pomp behavior:\n\nre-running a command on an object (i.e., mif2 on mf1) created by the same command perserves the algorithmic arguments\nrunning mif2 on the results of a mif2 computation re-runs IF2 from the endpoint of the first run\nthe second/subsequent run will preserve all the algorithmic parameters from the previous run by default; here we overrode the default choice of Nmif from 50 to 100\n\nfollowing the local search, the particle filter is used to evaluate the likelihood as the previous session\n\n\nbake(\"results_global.rds\", {\n  foreach(guess=iter(guesses,\"row\"), .combine=rbind) %dopar% {\n    # run the iterated filtering algorithm\n    mf1 |&gt;\n      mif2(params=c(guess,fixed_params)) |&gt;\n      mif2(Nmif=100) -&gt; mf\n    # compute the likelihood and se from 10 replications\n    replicate(\n      10,\n      mf |&gt; pfilter(Np=5000) |&gt; logLik()\n    ) |&gt;\n    logmeanexp(se=TRUE) -&gt; ll\n    # store the results\n    mf |&gt; coef() |&gt; bind_rows() |&gt;\n      bind_cols(loglik=ll[1],loglik.se=ll[2])\n  } -&gt; results_global\n}) -&gt; results_global\n\nresults_global |&gt; filter(loglik == max(loglik))\n\n# A tibble: 1 × 9\n   Beta   rho    S0    I0    R0 gamma     k loglik loglik.se\n  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt;     &lt;dbl&gt;\n1  17.6 0.265  2280     1 35720   0.5    10  -106.    0.0272\n\n\nThe best result of this search had a likelihood of -105.704 with a standard error of ~0.027. Now we can update the database:\n\nread_csv(\"measles_params.csv\") |&gt;\n  bind_rows(results_global) |&gt;\n  arrange(-loglik) |&gt;\n  write_csv(\"measles_params.csv\")\n\nRows: 421 Columns: 9\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\ndbl (9): Beta, gamma, rho, k, S0, I0, R0, loglik, loglik.se\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\nWe attempt to visualize the global geometry of the likelihood surface using a scatterplot matrix. In particular, here we plot both the initial guesses (grey) and the IF2 estimates (red).\n\nread_csv(\"measles_params.csv\") |&gt;\n  filter(loglik&gt;max(loglik)-50) |&gt;\n  bind_rows(guesses) |&gt;\n  mutate(type=if_else(is.na(loglik),\"guess\",\"result\")) |&gt;\n  arrange(type) -&gt; all\n\nRows: 821 Columns: 9\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\ndbl (9): Beta, gamma, rho, k, S0, I0, R0, loglik, loglik.se\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\npairs(~loglik+Beta+rho, data=all, pch=16, cex=0.3, col=ifelse(all$type==\"guess\",grey(.5),\"red\"))\n\n\n\n\n\n\n\n\n\na poo rman’s profile\n\nall |&gt;\n  filter(type==\"result\") |&gt;\n  filter(loglik&gt;max(loglik)-10) |&gt;\n  ggplot(aes(x=Beta,y=loglik))+\n  geom_point()+\n  labs(\n    x=expression(Beta),\n    title=\"poor man’s profile likelihood\"\n  )"
  },
  {
    "objectID": "iterated.html#local-search-of-the-likelihood-surface",
    "href": "iterated.html#local-search-of-the-likelihood-surface",
    "title": "Maximum likelihood estimation and iterated filtering",
    "section": "Local search of the likelihood surface",
    "text": "Local search of the likelihood surface\nA “local search” means that we are looking for the maximum likelihood estimate (MLE) starting from ONE initial guess. Afterwards, by applying “local search” to a number of different initial guesses covering the reasonable parameter space, we can explore the likelihood surface and find the global MLE.\nA few parameters for iterated filtering need to be specified:\n\nthe random walk standard deviation, rw.sd for each parameter to be estimated\nthe cooling fraction every 50 iterations, cooling.fraction.50\nthe transmission rate \\(\\beta\\) will estimated in log scale (since \\(\\beta &gt; 0\\)) and the reporting ratio \\(\\rho\\) in logistic scale (since \\(0 &lt; \\rho &lt; 1\\))\nthe perturbation for \\(\\beta\\) and \\(\\rho\\) are both \\(0.02\\) (a very common and efficient choice from experience)\nfix cooling.fraction.50=0.5, such that after 50 mif2 iterations, the perturbation are reduced to half their original magnitudes\n\nNow we can repeat the mif2 algorithm for 20 times with the same initial guesses:\n\nforeach(i=1:20,.combine=c) %dopar% {\n  sird_measle |&gt;\n    mif2(\n      Np=2000, Nmif=50,\n      cooling.fraction.50=0.5,\n      rw.sd=rw_sd(Beta=0.02, rho=0.02),\n      partrans=parameter_trans(log=\"Beta\",logit=\"rho\"),\n      paramnames=c(\"Beta\",\"rho\")\n    )\n} -&gt; mifs_local\n\n\nIterated filtering diagnostics\n\nmifs_local |&gt;\n  traces() |&gt;\n  melt() |&gt;\n  ggplot(aes(x=iteration,y=value,group=.L1,color=factor(.L1)))+\n    geom_line() +\n    guides(color=\"none\") +\n    facet_wrap(~name,scales=\"free_y\")\n\n\n\n\n\n\n\n\nFrom the traces of the parameters, we can see that the iterated filtering algorithm has converged to a stable region after about 10 iterations. We can conclude that:\n\nour initial guesses are reasonable;\nthis Monte Carlo algorithm is stochastic and the results may vary from run to run.\n\n\n\nEstimating the likelihood\nThe diagnostic plot shows that the 20 replications of the iterated filtering algorithm have converged to a stable region after 50 iterations, and now we can compute the likelihood of the data given these 20 converged parameter vectors.\n\nbake(\"results_local.rds\", {\n  foreach(mf=mifs_local,.combine=rbind) %dopar% {\n    evals &lt;- replicate(10, logLik(pfilter(mf,Np=5000)))\n    ll &lt;- logmeanexp(evals,se=TRUE)\n    mf |&gt; coef() |&gt; bind_rows() |&gt;\n    bind_cols(loglik=ll[1],loglik.se=ll[2])\n  } -&gt; results_local\n}) -&gt; results_local\n\npairs(~loglik+Beta+rho,data=results_local,pch=16)\n\n\n\n\n\n\n\n\nThe scatter plot shows a hint of a “ridge” in the likelihood surface. However, the sampling is as yet too sparse to give a clear picture. There are things that we can try next:\n\nIncrease the number of iterations, Nmif, to confirm the convergence\nIncrease the number of particles, Np, to improve the accuracy of the estimates\nTry different initial guesses to explore the likelihood surface\n\n\nread_csv(\"measles_params.csv\") |&gt;\n  bind_rows(results_local) |&gt;\n  arrange(-loglik) |&gt;\n  write_csv(\"measles_params.csv\")\n\nRows: 1 Columns: 9\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\ndbl (9): Beta, gamma, rho, k, S0, I0, R0, loglik, loglik.se\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message."
  }
]