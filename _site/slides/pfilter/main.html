<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.5.45">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="Qianying (Ruby) Lin">
<meta name="author" content="Spencer J. Fox">
<meta name="author" content="Zian (Larry) Zhuang">

<title>Lesson 3: Likelihood-based inference for POMP models – EpiSim</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { display: inline-block; text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
/* CSS for citations */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
  margin-bottom: 0em;
}
.hanging-indent div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}</style>


<script src="../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../../">
<script src="../../site_libs/quarto-html/quarto.js"></script>
<script src="../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>
<script>
MathJax = {
  loader: {
    load: ['[tex]/boldsymbol']
  },
  tex: {
    tags: "all",
    inlineMath: [['$','$'], ['\\(','\\)']],
    displayMath: [['$$','$$'], ['\\[','\\]']],
    processEscapes: true,
    processEnvironments: true,
    packages: {
      '[+]': ['boldsymbol']
    }
  }
};
</script>
<script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

  <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

<link rel="stylesheet" href="../../styles.css">
</head>

<body class="nav-fixed">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand-lg " data-bs-theme="dark">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container mx-auto">
    <a class="navbar-brand" href="../../index.html">
    <span class="navbar-title">EpiSim</span>
    </a>
  </div>
            <div id="quarto-search" class="" title="Search"></div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" role="menu" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll me-auto">
  <li class="nav-item">
    <a class="nav-link" href="../../index.html"> 
<span class="menu-text">Home</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../instruction.html"> 
<span class="menu-text">Instruction</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../introduction.html"> 
<span class="menu-text">Introduction</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../simulation.html"> 
<span class="menu-text">Simulation</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../likelihood.html"> 
<span class="menu-text">Likelihood</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../iterated.html"> 
<span class="menu-text">Iterated filtering</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../Bayesian.html"> 
<span class="menu-text">Bayesian</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../case study.html"> 
<span class="menu-text">Case study</span></a>
  </li>  
</ul>
          </div> <!-- /navcollapse -->
            <div class="quarto-navbar-tools">
</div>
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
    <div class="quarto-alternate-formats"><h2>Other Formats</h2><ul><li><a href="main.pdf"><i class="bi bi-file-pdf"></i>Beamer</a></li><li><a href="main.pdf"><i class="bi bi-file-pdf"></i>PDF</a></li></ul></div></div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title">Lesson 3: Likelihood-based inference for POMP models</h1>
</div>



<div class="quarto-title-meta">

    <div>
    <div class="quarto-title-meta-heading">Authors</div>
    <div class="quarto-title-meta-contents">
             <p>Qianying (Ruby) Lin </p>
             <p>Spencer J. Fox </p>
             <p>Zian (Larry) Zhuang </p>
          </div>
  </div>
    
  
    
  </div>
  


</header>


<section id="introduction" class="level1">
<h1>Introduction</h1>
<section id="objectives" class="level2">
<h2 class="anchored" data-anchor-id="objectives">Objectives</h2>
<p>Students completing this lesson will:</p>
<ol type="1">
<li>Gain an understanding of the nature of the problem of likelihood computation for POMP models.</li>
<li>Be able to explain the simplest particle filter algorithm.</li>
<li>Gain experience in the visualization and exploration of likelihood surfaces.</li>
<li>Be able to explain the tools of likelihood-based statistical inference that become available given numerical accessibility of the likelihood function.</li>
</ol>
</section>
<section id="overview" class="level2 allowframebreaks">
<h2 class="allowframebreaks anchored" data-anchor-id="overview">Overview</h2>
<p>A general framework of epidemiological inference includes three layers:</p>
<div class="columns">
<div class="column" style="width:50%;">
<ul>
<li>The input: a model of interest and the given data</li>
<li>A method for inference</li>
<li>Inferences include estimation, uncertainty, prediction and forecast, and model selection.</li>
</ul>
</div><div class="column" style="width:50%;">

</div>
</div>
<p>Methods for inference can be categorized into three groups:</p>
<ul>
<li>Optimization-based: minimize a cost function (e.g., SSE, MSE, MAE) that measures the difference between observed data and model predictions</li>
<li>Likelihood-based: maximize a likelihood function, which represents the probability of observing the given data given the parameters</li>
<li>Summary Statistics-based: use a set of features of the data instead of the full set of data</li>
</ul>
<p>In this lesson, we focus on the likelihood-based method because</p>
<ul>
<li>it fits for stochastic models and</li>
<li>it incorporates all data (i.e., full-information).</li>
</ul>
<!-- ## Overview {.allowframebreaks} -->
<!--   The following schematic diagram represents conceptual links between different components of the methodological approach we're developing for statistical inference on epidemiological dynamics. -->
<!-- \begin{center} -->
<!--     \includegraphics[height=4cm]{../graphics/lec3 overview} -->
<!--   \end{center} -->
<!--   -  In this lesson, we're going to discuss the orange compartments. -->
<!--   -  The Monte Carlo technique called the "particle filter" is central for connecting the higher-level ideas of POMP models and likelihood-based inference to the lower-level tasks involved in carrying out data analysis. -->
<!--   -  We employ a standard toolkit for likelihood based inference: -->
<!--     Maximum likelihood estimation, profile likelihood confidence intervals, likelihood ratio tests for model selection, and other likelihood-based model comparison tools such as AIC. -->
<!--   -  We seek to better understand these tools, and to figure out how to implement and interpret them in the specific context of POMP models. -->
</section>
</section>
<section id="the-likelihood-function" class="level1">
<h1>The likelihood function</h1>
</section>
<section id="general-considerations" class="level1">
<h1>General considerations</h1>
<section id="the-likelihood" class="level2 allowframebreaks">
<h2 class="allowframebreaks anchored" data-anchor-id="the-likelihood">The likelihood</h2>
<ul>
<li>The basis for modern frequentist, Bayesian, and information-theoretic inference.</li>
<li>Method of maximum likelihood introduced by <span class="citation" data-cites="Fisher1922">Fisher (<a href="#ref-Fisher1922" role="doc-biblioref">1922</a>)</span>.</li>
<li>The likelihood function itself is a representation of the what the data have to say about the parameters.</li>
<li>A good general reference on likelihood is by <span class="citation" data-cites="Pawitan2001">Pawitan (<a href="#ref-Pawitan2001" role="doc-biblioref">2001</a>)</span>.</li>
</ul>
<div class="columns">
<div class="column" style="width:50%;">
<ul>
<li>Goal: fit the model to the data and conduct statistical inferences, such as parameter estimation.</li>
<li>The likelihood, thus, can be considered as a metric to assess the <em>goodness</em> of the proposed parameters.</li>
<li>By exploring the space of parameters, we can eventually obtain the maximum likelihood estimator (MLE).</li>
</ul>
</div><div class="column" style="width:50%;">

</div>
</div>
<p>Thus, the objective of this lesson is to discuss how we compute the likelihood given a model of interest with a proposed set of parameters in both theory and in <code>pomp</code>.</p>
</section>
<section id="definition-of-the-likelihood-function" class="level2">
<h2 class="anchored" data-anchor-id="definition-of-the-likelihood-function">Definition of the likelihood function</h2>
<ul>
<li>Notations:
<ul>
<li><span class="math inline">\(y_{1:N}^*\)</span>: the data, a sequence of <span class="math inline">\(N\)</span> observations</li>
<li><span class="math inline">\(f_{Y_{1:N}}(y_{1:N};\theta)\)</span>: the statistical model, a probability distribution for each value of a parameter vector <span class="math inline">\(\theta\)</span></li>
<li><span class="math inline">\(Y_{1:N} \sim f_{Y_{1:N}}(y_{1:N};\theta)\)</span>: a random variable drawn from distribution <span class="math inline">\(f_{Y_{1:N}}(y_{1:N};\theta)\)</span></li>
</ul></li>
<li>The likelihood function is <span class="math display">\[\lik(\theta) = f_{Y_{1:N}}(y^*_{1:N};\theta),\]</span> the density function evaluated at the data.</li>
<li>It is often convenient to work with the log-likelihood function, <span class="math display">\[\loglik(\theta)= \log \lik(\theta) = \log f_{Y_{1:N}}(y^*_{1:N};\theta).\]</span></li>
</ul>
<!-- ## Modeling using discrete and continuous distributions -->
<!-- -   Recall that the probability distribution -->
<!--     $f_{Y_{1:N}}(y_{1:N};\theta)$ defines a random variable $Y_{1:N}$ -->
<!--     for which probabilities can be computed as integrals of -->
<!--     $f_{Y_{1:N}}(y_{1:N};\theta)$. -->
<!-- -   Specifically, for any event $E$ describing a set of possible -->
<!--     outcomes of $Y_{1:N}$, -->
<!--     $$\prob{Y_{1:N} \in E} = \int_E f_{Y_{1:N}}(y_{1:N};\theta)\, dy_{1:N}.$$ -->
<!-- -   If the model corresponds to a discrete distribution, then the -->
<!--     integral is replaced by a sum and the probability density function -->
<!--     is called a *probability mass function*. -->
<!-- -   The definition of the likelihood function remains unchanged. We will -->
<!--     use the notation of continuous random variables, but all the methods -->
<!--     apply also to discrete models. -->
</section>
<section id="a-simulator-is-implicitly-a-statistical-model" class="level2">
<h2 class="anchored" data-anchor-id="a-simulator-is-implicitly-a-statistical-model">A simulator is implicitly a statistical model</h2>
<ul>
<li><span class="math inline">\(f_{Y_{1:N}}(y_{1:N};\theta)\)</span> is simple and with an explicit expression:
<ul>
<li>the simulation of <span class="math inline">\(Y_{1:N}\)</span> is direct, e.g., <span class="math inline">\(Y_k \sim N(0,1)\)</span> for <span class="math inline">\(k=1,\dots,N\)</span></li>
<li>the likelihood function is explicit</li>
</ul></li>
<li><span class="math inline">\(f_{Y_{1:N}}(y_{1:N};\theta)\)</span> is complex or even without an explicit expression:
<ul>
<li>the simulation of <span class="math inline">\(Y_{1:N}\)</span>, given the underlying dynamical model, is a bit more complex but convenient</li>
<li>the likelihood function exists with a complicated expression or even without an explicit expression</li>
</ul></li>
</ul>
<p>Thus, we can develop numerical methods to compute the complex or implicit likelihood functions!</p>
<!-- -   For simple statistical models, we may describe the model by -->
<!--     explicitly writing the density function -->
<!--     $f_{Y_{1:N}}(y_{1:N};\theta)$. One may then ask how to simulate a -->
<!--     random variable $Y_{1:N}\sim f_{Y_{1:N}}(y_{1:N};\theta)$. -->
<!-- -   For many dynamic models it is much more convenient to define the -->
<!--     model via a procedure to simulate the random variable $Y_{1:N}$. -->
<!--     This *implicitly* defines the corresponding density -->
<!--     $f_{Y_{1:N}}(y_{1:N};\theta)$. -->
<!-- -   For a complicated simulation procedure, it may be difficult or -->
<!--     impossible to write down or even compute -->
<!--     $f_{Y_{1:N}}(y_{1:N};\theta)$ exactly. -->
<!-- -   It is important to bear in mind that the likelihood function exists -->
<!--     even when we don't know what it is! We can still talk about the -->
<!--     likelihood function, and develop numerical methods that take -->
<!--     advantage of its statistical properties. -->
</section>
</section>
<section id="likelihood-of-a-pomp-model" class="level1">
<h1>Likelihood of a POMP model</h1>
<section id="the-likelihood-for-a-pomp-model" class="level2 allowframebreaks">
<h2 class="allowframebreaks anchored" data-anchor-id="the-likelihood-for-a-pomp-model">The likelihood for a POMP model</h2>
<p>Recall the following schematic diagram, showing dependence among variables in a POMP model.</p>
<p>Recall the following definitions and properties:</p>
<ul>
<li><p><strong>Measurements</strong>: <span class="math inline">\(Y_n\)</span>, at time <span class="math inline">\(t_n\)</span> depend on the latent process, <span class="math inline">\(X_n\)</span>, at that time.</p></li>
<li><p><strong>The Markov property</strong>: latent process variables depend on their value at the previous timestep.</p>
<ol type="1">
<li>The distribution of the state <span class="math inline">\(X_{n+1}\)</span>, conditional on <span class="math inline">\(X_{n}\)</span>, is independent of the values of <span class="math inline">\(X_{k}\)</span>, <span class="math inline">\(k&lt;n\)</span> and <span class="math inline">\(Y_{k}\)</span>, <span class="math inline">\(k\le n\)</span>.</li>
<li>The distribution of the measurement <span class="math inline">\(Y_{n}\)</span>, conditional on <span class="math inline">\(X_{n}\)</span>, is independent of all other variables.</li>
</ol></li>
<li><p><strong>The latent process</strong>: <span class="math inline">\(X(t)\)</span>, may be defined at all times, but we are particularly interested in its value at observation times. Therefore, we write <span class="math display">\[X_n=X(t_n).\]</span></p>
<ul>
<li>We write collections of random variables using the notation <span class="math inline">\(X_{0:N}=(X_0,\dots,X_N)\)</span>.</li>
</ul></li>
<li><p><strong>The one-step transition density</strong>: <span class="math inline">\(f_{X_n|X_{n-1}}(x_n|x_{n-1};\theta)\)</span>, together with the measurement density, <span class="math inline">\(f_{Y_n|X_n}(y_n|x_n;\theta)\)</span> and the initial density, <span class="math inline">\(f_{X_0}(x_0;\theta)\)</span>, specify the entire joint density via</p>
<p><span class="math display">\[
  \begin{split}
          &amp;f_{X_{0:N},Y_{1:N}}(x_{0:N},y_{1:N};\theta)\\
          &amp; \qquad = f_{X_0}(x_0;\theta)\,\prod_{n=1}^N\!f_{X_n | X_{n-1}}(x_n|x_{n-1};\theta)\,f_{Y_n|X_n}(y_n|x_n;\theta).
  \end{split}
\]</span></p></li>
<li><p><strong>The marginal density for sequence of measurements</strong>: <span class="math inline">\(Y_{1:N}\)</span>, evaluated at the data, <span class="math inline">\(y_{1:N}^*\)</span>, is <span class="math display">\[
  \lik(\theta) = f_{Y_{1:N}}(y^*_{1:N};\theta)=\int\!f_{X_{0:N},Y_{1:N}}(x_{0:N},y^*_{1:N};\theta)\, dx_{0:N}.
\]</span></p></li>
</ul>
</section>
<section id="special-case-deterministic-latent-process" class="level2">
<h2 class="anchored" data-anchor-id="special-case-deterministic-latent-process">Special case: deterministic latent process</h2>
<ul>
<li>When the latent process is non-random, the log-likelihood for a POMP model closely resembles a nonlinear regression model.</li>
<li>In this case, we can write <span class="math inline">\(X_{n}=x_n(\theta)\)</span>, and the log-likelihood is <span class="math display">\[\loglik(\theta) = \sum_{n=1}^N \log f_{Y_n|X_n}\big(y_n^*| x_n(\theta); \theta\big).\]</span></li>
<li>If we have a Gaussian measurement model, where <span class="math inline">\(Y_n\)</span> given <span class="math inline">\(X_n=x_n(\theta)\)</span> is conditionally normal with mean <span class="math inline">\(\hat{y}_n\big(x_n(\theta)\big)\)</span> and constant variance <span class="math inline">\(\sigma^2\)</span>, then the log-likelihood contains a sum of squares which is exactly the criterion that nonlinear least squares regression seeks to minimize.</li>
<li>More details on deterministic latent process models are given as a <a href="deterministic.html">supplement</a>.</li>
</ul>
</section>
<section id="general-case-stochastic-unobserved-state-process" class="level2">
<h2 class="anchored" data-anchor-id="general-case-stochastic-unobserved-state-process">General case: stochastic unobserved state process</h2>
<ul>
<li>For a POMP model, the likelihood takes the form of an integral:</li>
</ul>
<p><span id="eq-L1"><span class="math display">\[
\begin{aligned}
\lik(\theta) &amp;= f_{Y_{1:N}}({y^*_{1:N}};\theta)\\
        = &amp;\int f_{X_0}(x_0;\theta)\prod_{n=1}^{N}\!f_{Y_n|X_n}({y^*_n}| x_n; \theta)\, f_{X_n|X_{n-1}}(x_n|x_{n-1};\theta)\, dx_{0:N}.
\end{aligned}
\tag{1}\]</span></span></p>
<ul>
<li>This integral is high dimensional and, except for the simplest cases, can not be reduced analytically.</li>
</ul>
</section>
</section>
<section id="computing-the-likelihood" class="level1">
<h1>Computing the likelihood</h1>
</section>
<section id="monte-carlo-algorithms" class="level1">
<h1>Monte Carlo algorithms</h1>
<section id="monte-carlo-likelihood-direct-simulation" class="level2 allowframebreaks">
<h2 class="allowframebreaks anchored" data-anchor-id="monte-carlo-likelihood-direct-simulation">Monte Carlo likelihood: direct simulation</h2>
<!-- We work toward introducing the particle filter by first proposing a -->
<!--     simpler method that usually doesn't work on anything but very short -->
<!--     time series. -->
<p><strong>Spoiler Alert</strong>: This section serves to introduce the concept of the <strong>particle filter</strong> and the approach of <a href="monteCarlo.pdf">Monte Carlo integration</a> by first proposing an intuitive and a simpler method. This simple method usually <strong>does NOT work</strong> on anything but <strong>very short</strong> time series.</p>
<ol type="1">
<li>Let’s rewrite the likelihood integral using an equivalent factorization. As an exercise, you could check how the equivalence of <a href="#eq-L1" class="quarto-xref">Equation&nbsp;1</a> and <a href="#eq-L2" class="quarto-xref">Equation&nbsp;2</a> follows algebraically from the Markov property and the definition of conditional density.</li>
</ol>
<p><span id="eq-L2"><span class="math display">\[
    \begin{aligned}
      \lik(\theta) &amp;= f_{Y_{1:N}}({y^*_{1:N}};\theta)\\
      &amp;= \int\!\left\{\prod_{n=1}^{N}\!f_{Y_n|X_n}({y^*_n}| x_n; \theta)\right\}\,f_{X_{0:N}}(x_{0:N};\theta)\, dx_{0:N}.
    \end{aligned}
\tag{2}\]</span></span></p>
<ol start="2" type="1">
<li>Notice, using the representation in <a href="#eq-L2" class="quarto-xref">Equation&nbsp;2</a>, that the likelihood can be written as an expectation, <span class="math display">\[\begin{equation*}
  \lik(\theta) = \E \left[ \prod_{n=1}^{N}\!f_{Y_n|X_n}({y^*_n}| X_n; \theta) \right],
     \end{equation*}\]</span> where the expectation is taken with <span class="math inline">\(X_{0:N}\sim f_{X_{0:N}}(x_{0:N};\theta)\)</span>.</li>
<li>Now, using a <a href="https://en.wikipedia.org/wiki/Law_of_large_numbers">law of large numbers</a>, we can approximate an expectation by the average of a Monte Carlo sample. Thus, <span class="math display">\[
  \lik(\theta) \approx \frac{1}{J} \sum_{j=1}^{J}\prod_{n=1}^{N}\!f_{Y_n|X_n}({y^*_n}| X^j_n; \theta),
\]</span> where <span class="math inline">\(\{X^j_{0:N}, j=1,\dots,J\}\)</span> is a Monte Carlo sample of size <span class="math inline">\(J\)</span> drawn from <span class="math inline">\(f_{X_{0:N}}(x_{0:N};\theta)\)</span>.</li>
</ol>
<p>In conclusion, we can generate trajectories by simulation and all we need to do to get a Monte Carlo estimate of the likelihood is to evaluate the measurement density of the data at each trajectory and average. In the context of the <strong>plug-and-play</strong> framework, our algorithm depends on <code>rprocess</code> for simulation but does not require <code>dprocess</code> for evaluation. However, this naive approach scales poorly with dimension:</p>
<ul>
<li><p>it requires a Monte Carlo effort that scales exponentially with the length of the time series, and so is infeasible on anything but a short data set;</p></li>
<li><p>due to stochasticity, once a simulated trajectory diverges from the data, it will seldom come back;</p></li>
<li><p>simulations that lose track and deviate from the data are harmful for likelihood estimation;</p></li>
<li><p>when simulating a long time series, almost all the simulated trajectories will eventually lose track of the data.</p></li>
<li><p>measles outbreak example: <a href="directSimulation.html">supplementary material</a>.</p></li>
</ul>
</section>
</section>
<section id="sequential-monte-carlo" class="level1">
<h1>Sequential Monte Carlo</h1>
<section id="sequential-monte-carlo-the-particle-filter" class="level2 allowframebreaks">
<h2 class="allowframebreaks anchored" data-anchor-id="sequential-monte-carlo-the-particle-filter">Sequential Monte Carlo: The particle filter</h2>
<p>Fortunately, we can compute the likelihood for a POMP model by a much more efficient algorithm than direct Monte Carlo integration:</p>
<ol type="1">
<li>We proceed by factorizing the likelihood in a different way:</li>
</ol>
<p><span class="math display">\[
  \begin{aligned}
    \lik(\theta)&amp;=f_{Y_{1:N}}(y^*_{1:N}; \theta) =\prod_{n=1}^N\,f_{Y_n|Y_{1:n-1}}(y^*_n|y^*_{1:n-1};\theta)\\
    &amp;=\prod_{n=1}^N\,\int f_{Y_n|X_n}(y^*_n|x_n;\theta)\,f_{X_n|Y_{1:n-1}}(x_n|y^*_{1:n-1};\theta)\, dx_{n},
  \end{aligned}
\]</span> with the understanding that <span class="math inline">\(f_{X_1|Y_{1:0}}=f_{X_1}\)</span>.</p>
<ol start="2" type="1">
<li><p>The Markov property leads to the <strong>prediction formula:</strong> <span class="math display">\[
  \begin{aligned}
   &amp;f_{X_n|Y_{1:n-1}}(x_n|y^*_{1:n-1}; \theta) \\
   &amp;\quad = \int \! f_{X_n|X_{n-1}}(x_n|x_{n-1};\theta)\, f_{X_{n-1}|Y_{1:n-1}}(x_{n-1}| y^*_{1:n-1}; \theta) \, dx_{n-1}.
  \end{aligned}
\]</span></p></li>
<li><p>Bayes’ theorem gives the <strong>filtering formula:</strong> <span class="math display">\[
\begin{aligned}
     &amp;f_{X_n|Y_{1:n}}(x_n|y^*_{1:n}; \theta)\\
     &amp;\quad = f_{X_n|Y_n,Y_{1:n-1}}(x_n|y^*_n,y^*_{1:n-1}; \theta) \\
     &amp;\quad =\frac{f_{Y_n|X_n}(y^*_{n}|x_{n};\theta)\,f_{X_n|Y_{1:n-1}}(x_{n}|y^*_{1:n-1};\theta)}{\int f_{Y_n|X_n}(y^*_{n}|u_{n};\theta)\,f_{X_n|Y_{1:n-1}}(u_{n}|y^*_{1:n-1};\theta)\, du_n}.
\end{aligned}
\]</span></p></li>
</ol>
<ul>
<li><p>This suggests that we keep track of two key distributions at each time <span class="math inline">\(t_n\)</span>,</p>
<ul>
<li>The <strong>prediction distribution</strong> is <span class="math inline">\(f_{X_n | Y_{1:n-1}}(x_n| y^*_{1:n-1})\)</span>.</li>
<li>The <strong>filtering distribution</strong> is <span class="math inline">\(f_{X_{n} | Y_{1:n}}(x_n| y^*_{1:n})\)</span>.</li>
</ul></li>
<li><p>The prediction and filtering formulas give us a two-step recursion:</p>
<ul>
<li>The prediction formula gives the prediction distribution at time <span class="math inline">\(t_n\)</span> using the filtering distribution at time <span class="math inline">\(t_{n-1}\)</span>.</li>
<li>The filtering formula gives the filtering distribution at time <span class="math inline">\(t_n\)</span> using the prediction distribution at time <span class="math inline">\(t_n\)</span>.</li>
</ul></li>
<li><p>The <strong>particle filter</strong> use Monte Carlo techniques to sequentially estimate the integrals in the prediction and filtering recursions. Hence, the alternative name of <strong>sequential Monte Carlo (SMC)</strong>.</p></li>
</ul>
<p>A basic particle filter is described as follows:</p>
<ol type="1">
<li>Suppose <span class="math inline">\(X_{n-1,j}^{F}\)</span>, <span class="math inline">\(j=1,\dots,J\)</span> is a set of <span class="math inline">\(J\)</span> points drawn from the filtering distribution at time <span class="math inline">\(t_{n-1}\)</span>.</li>
<li>We obtain a sample <span class="math inline">\(X_{n,j}^{P}\)</span> of points drawn from the prediction distribution at time <span class="math inline">\(t_n\)</span> by simply simulating the process model: <span class="math display">\[
X_{n,j}^{P} \sim \mathrm{process}(X_{n-1,j}^{F},\theta), \qquad j=1,\dots,J.
\]</span></li>
<li>Having obtained <span class="math inline">\(x_{n,j}^{P}\)</span>, we obtain a sample of points from the filtering distribution at time <span class="math inline">\(t_n\)</span> by <em>resampling</em> from <span class="math inline">\(\big\{X_{n,j}^{P},j\in 1:J\big\}\)</span> with weights <span class="math display">\[
w_{n,j}=f_{Y_n|X_n}(y^*_{n}|X^P_{n,j};\theta).
\]</span></li>
<li>The Monte Carlo principle tells us that the conditional likelihood <span class="math display">\[
\begin{aligned}
  \lik_n(\theta) &amp;= f_{Y_n|Y_{1:n-1}}(y^*_n|y^*_{1:n-1};\theta)\\
  &amp;= \int f_{Y_n|X_n}(y^*_{n}|x_{n};\theta)\,f_{X_n|Y_{1:n-1}}(x_{n}|y^*_{1:n-1};\theta)\, dx_n
\end{aligned}
\]</span> is approximated by <span class="math display">\[
  \hat{\lik}_n(\theta)\approx\frac{1}{J}\,\sum_j\,f_{Y_n|X_n}(y^*_{n}|X_{n,j}^{P};\theta)
\]</span> since <span class="math inline">\(X_{n,j}^{P}\)</span> is approximately a draw from <span class="math inline">\(f_{X_n|Y_{1:n-1}}(x_{n}|y^*_{1:n-1};\theta)\)</span>.</li>
<li>We can iterate this procedure through the data, one step at a time, alternately simulating and resampling, until we reach <span class="math inline">\(n=N\)</span>.</li>
<li>The full log-likelihood then has approximation <span class="math display">\[
\loglik(\theta) = \log{{\lik}(\theta)} = \sum_n \log{{\lik}_n(\theta)} \approx \sum_n\log\hat{\lik}_n(\theta).
\]</span></li>
</ol>
<ul>
<li>References on the particle filter include <span class="citation" data-cites="Kitagawa1987">Kitagawa (<a href="#ref-Kitagawa1987" role="doc-biblioref">1987</a>)</span>, <span class="citation" data-cites="Arulampalam2002">Arulampalam et al. (<a href="#ref-Arulampalam2002" role="doc-biblioref">2002</a>)</span>, <span class="citation" data-cites="Doucet2001">Doucet, Freitas, and Gordon (<a href="#ref-Doucet2001" role="doc-biblioref">2001</a>)</span>, <span class="citation" data-cites="King2016">King, Nguyen, and Ionides (<a href="#ref-King2016" role="doc-biblioref">2016</a>)</span>.</li>
<li>It can be shown that the particle filter provides an unbiased estimate of the likelihood. This implies a consistent but biased estimate of the log-likelihood.</li>
</ul>
</section>
<section id="a-block-diagram-representation-of-a-particle-filter" class="level2">
<h2 class="anchored" data-anchor-id="a-block-diagram-representation-of-a-particle-filter">A block diagram representation of a particle filter</h2>

</section>
<section id="parallel-computing" class="level2">
<h2 class="anchored" data-anchor-id="parallel-computing">Parallel computing</h2>
<p>It will be helpful to parallelize most of the computations. Most machines nowadays have multiple cores and using this computational capacity is as simple as:</p>
<ol type="1">
<li>letting <code>R</code> know you plan to use multiple processors;</li>
<li>using the parallel for loop provided by the <code>foreach</code> package; and</li>
<li>paying proper attention to the use of parallel random number generators (RNG).</li>
</ol>
<p>For example:</p>
<div class="cell" data-layout-align="center">
<div class="sourceCode cell-code" id="cb1"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(foreach)            <span class="co"># load foreach</span></span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(doFuture)           <span class="co"># load doFuture (and future)</span></span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a><span class="fu">plan</span>(multisession)          <span class="co"># using multiple R sessions</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>The second line tells <code>foreach</code> that we will use the <code>doFuture</code> backend. By default, R will attempt to determine how many cores are available and will run an appropriate number of concurrent R processes.</p>
</section>
<section id="particle-filtering-in-pomp" class="level2 allowframebreaks">
<h2 class="allowframebreaks anchored" data-anchor-id="particle-filtering-in-pomp">Particle filtering in pomp</h2>
<p>Recall the measles-outbreak example and the stochastic SIR model that we construct in the previous lesson, we can using the <code>pfilter</code> function to compute the likelihood using particle filtering method, given the parameters chosen by looking at simulations. <code>R</code> code to build the model is available <a href="model_measSIR.R">here</a>. We can execute this code by sourcing the file and check the parameters:</p>
<div class="cell" data-layout-align="center">
<div class="sourceCode cell-code" id="cb2"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a><span class="fu">source</span>(<span class="st">"model_measSIR.R"</span>)</span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true" tabindex="-1"></a>measSIR<span class="sc">@</span>params</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>   Beta   Gamma     Rho       k     Eta       N 
1.5e+01 5.0e-01 5.0e-01 1.0e+01 6.0e-02 3.8e+04 </code></pre>
</div>
</div>
<p>In pomp, we can compute the likelihood using the particle filtering method, implemented by function <code>pfilter</code>. The argument <code>Np</code> assigns the number of particles used:</p>
<div class="cell" data-layout-align="center">
<div class="sourceCode cell-code" id="cb4"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(pomp)</span>
<span id="cb4-2"><a href="#cb4-2" aria-hidden="true" tabindex="-1"></a>pf <span class="ot">&lt;-</span> measSIR <span class="sc">|&gt;</span> <span class="fu">pfilter</span>(<span class="at">Np=</span><span class="dv">5000</span>)</span>
<span id="cb4-3"><a href="#cb4-3" aria-hidden="true" tabindex="-1"></a><span class="fu">logLik</span>(pf)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[1] -131.0779</code></pre>
</div>
</div>
<p>The particle filtering method relies heavily on the state process and the measurement model. Therefore, it is necessary to make sure that the basic particle filter is working.</p>
<ol type="1">
<li>Check the <code>rprocess</code> and the <code>rmeasure</code> by simulation, as shown in Lesson 2:</li>
</ol>
<div class="cell" data-layout-align="center">
<div class="sourceCode cell-code" id="cb6"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb6-1"><a href="#cb6-1" aria-hidden="true" tabindex="-1"></a>measSIR <span class="sc">|&gt;</span></span>
<span id="cb6-2"><a href="#cb6-2" aria-hidden="true" tabindex="-1"></a>  <span class="fu">simulate</span>(<span class="at">nsim=</span><span class="dv">20</span>,<span class="at">format=</span><span class="st">"data.frame"</span>,<span class="at">include.data=</span><span class="cn">TRUE</span>) <span class="sc">|&gt;</span></span>
<span id="cb6-3"><a href="#cb6-3" aria-hidden="true" tabindex="-1"></a>  <span class="fu">ggplot</span>(<span class="fu">aes</span>(<span class="at">x=</span>week,<span class="at">y=</span>reports,<span class="at">group=</span>.id,<span class="at">color=</span>.id<span class="sc">==</span><span class="st">"data"</span>)) <span class="sc">+</span></span>
<span id="cb6-4"><a href="#cb6-4" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_line</span>() <span class="sc">+</span> <span class="fu">guides</span>(<span class="at">color=</span><span class="st">"none"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="tmp//figure/pf-diagnostic-1-1.png" class="img-fluid quarto-figure quarto-figure-center figure-img" data-fig-pos="h!" width="2400"></p>
</figure>
</div>
</div>
</div>
<ol start="2" type="1">
<li>A diagnostic plot to check the <code>rprocess</code> and the <code>dmeasure</code>:</li>
</ol>
<div class="columns">
<div class="column" style="width:40%;">
<div class="cell" data-layout-align="center">
<div class="sourceCode cell-code" id="cb7"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb7-1"><a href="#cb7-1" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(pf)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<ul>
<li><p>The data, <code>reports</code>;</p></li>
<li><p>The <strong>effective sample size</strong> of the particle filter, <code>ess</code>;</p></li>
<li><p>The log-likelihood of each observation conditioned on the preceding ones, <code>cond.logLik</code>.</p></li>
</ul>
</div><div class="column" style="width:60%;">
<div class="cell" data-layout-align="center">
<div class="cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="tmp//figure/pf-diagnostic-2-2-1.png" class="img-fluid quarto-figure quarto-figure-center figure-img" data-fig-pos="h!" width="1200"></p>
</figure>
</div>
</div>
</div>
</div>
</div>
<ol start="3" type="1">
<li>The Monte Carlo variability of the likelihood:</li>
</ol>
<div class="cell" data-layout-align="center">
<div class="sourceCode cell-code" id="cb8"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb8-1"><a href="#cb8-1" aria-hidden="true" tabindex="-1"></a><span class="fu">plan</span>(multisession)</span>
<span id="cb8-2"><a href="#cb8-2" aria-hidden="true" tabindex="-1"></a><span class="fu">foreach</span> (</span>
<span id="cb8-3"><a href="#cb8-3" aria-hidden="true" tabindex="-1"></a>  <span class="at">i=</span><span class="dv">1</span><span class="sc">:</span><span class="dv">10</span>, <span class="at">.combine=</span>c, <span class="at">.options.future=</span><span class="fu">list</span>(<span class="at">seed=</span><span class="dv">652643293</span>)</span>
<span id="cb8-4"><a href="#cb8-4" aria-hidden="true" tabindex="-1"></a>) <span class="sc">%dofuture%</span> {</span>
<span id="cb8-5"><a href="#cb8-5" aria-hidden="true" tabindex="-1"></a>    measSIR <span class="sc">|&gt;</span> <span class="fu">pfilter</span>(<span class="at">Np=</span><span class="dv">5000</span>)</span>
<span id="cb8-6"><a href="#cb8-6" aria-hidden="true" tabindex="-1"></a>} <span class="ot">-&gt;</span> pf</span>
<span id="cb8-7"><a href="#cb8-7" aria-hidden="true" tabindex="-1"></a><span class="fu">logLik</span>(pf) <span class="ot">-&gt;</span> ll</span>
<span id="cb8-8"><a href="#cb8-8" aria-hidden="true" tabindex="-1"></a><span class="fu">logmeanexp</span>(ll,<span class="at">se=</span><span class="cn">TRUE</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>        est          se 
-130.037782    1.253115 </code></pre>
</div>
</div>
<p>Note that we set the parallel RNG seed in the <code>foreach</code> call.</p>
<!-- Here, we'll get some practical experience with the particle filter, and -->
<!-- the likelihood function, in the context of our measles-outbreak case -->
<!-- study. Here, we repeat the construction of the SIR model [we looked at -->
<!-- earlier](https://kingaa.github.io/sbied/stochsim/), using the parameters -->
<!-- chosen by looking at simulations. `R` code to build the model is -->
<!-- available [for download](./model.R). We can execute this code by -->
<!-- sourcing the relevant file: -->
<!-- ```{r model-construct,purl=TRUE} -->
<!--   source("https://kingaa.github.io/sbied/pfilter/model.R") -->
<!-- ``` -->
<!-- In pomp, the basic particle filter is implemented in the command -->
<!-- `pfilter`. We must choose the number of particles to use by setting the -->
<!-- `Np` argument. -->
<!-- ```{r pfilter-1,cache=TRUE} -->
<!--   library(pomp) -->
<!--   measSIR |> -->
<!--     pfilter(Np=5000) -> pf -->
<!--   logLik(pf) -->
<!-- ``` -->
<!-- We can run a few particle filters to get an estimate of the Monte Carlo -->
<!-- variability: -->
<!-- ```{r pfilter-3,cache=TRUE} -->
<!--   plan(multisession) -->
<!--   foreach ( -->
<!--     i=1:10, -->
<!--     .combine=c, -->
<!--     .options.future=list(seed=652643293) -->
<!--   ) %dofuture% { -->
<!--     measSIR |> pfilter(Np=5000) -->
<!--   } -> pf -->
<!--   logLik(pf) -> ll -->
<!--   logmeanexp(ll,se=TRUE) -->
<!-- ``` -->
<!-- Note that we set the parallel RNG seed in the `foreach` call. -->
</section>
</section>
<section id="likelihood-based-inference" class="level1">
<h1>Likelihood-based inference</h1>
</section>
<section id="parameter-estimates-and-uncertainty-quantification" class="level1">
<h1>Parameter estimates and uncertainty quantification</h1>
<section id="review-of-likelihood-based-inference" class="level2">
<h2 class="anchored" data-anchor-id="review-of-likelihood-based-inference">Review of likelihood-based inference</h2>
<p>For now, let us suppose that software exists to evaluate and maximize the likelihood function, up to a tolerable numerical error, for the dynamic models of interest. Our immediate task is to think about how to use that capability.</p>
<ul>
<li>Likelihood-based inference (meaning statistical tools based on the likelihood function) provides tools for parameter estimation, standard errors, hypothesis tests and diagnosing model misspecification.</li>
<li>Likelihood-based inference often (but not always) has favorable theoretical properties. Here, we are not especially concerned with the underlying theory of likelihood-based inference. On any practical problem, we can check the properties of a statistical procedure by simulation experiments.</li>
</ul>
</section>
<section id="the-maximum-likelihood-estimate-mle" class="level2">
<h2 class="anchored" data-anchor-id="the-maximum-likelihood-estimate-mle">The maximum likelihood estimate (MLE)</h2>
<ul>
<li>A maximum likelihood estimate (MLE) is <span class="math display">\[\begin{equation*}
\hat\theta = \argmax_{\theta} \loglik(\theta),
    \end{equation*}\]</span> where <span class="math inline">\(\argmax_{\theta} g(\theta)\)</span> means a value of argument <span class="math inline">\(\theta\)</span> at which the maximum of the function <span class="math inline">\(g\)</span> is attained, so <span class="math inline">\(g\left(\argmax_{\theta} g(\theta)\right) = \max_\theta g(\theta)\)</span>.</li>
<li>If there are many values of <span class="math inline">\(\theta\)</span> giving the same maximum value of the likelihood, then an MLE still exists but is not unique.</li>
<li>Note that <span class="math inline">\(\argmax_{\theta} \lik(\theta)\)</span> and <span class="math inline">\(\argmax_{\theta} \loglik(\theta)\)</span> are the same. Why?</li>
</ul>
</section>
<section id="standard-errors-for-the-mle" class="level2">
<h2 class="anchored" data-anchor-id="standard-errors-for-the-mle">Standard errors for the MLE</h2>
<ul>
<li>Parameter estimates are not very useful without some measure of their uncertainty.</li>
<li>Usually, this means obtaining a confidence interval, or in practice an interval close to a true confidence interval which should formally be called an approximate confidence interval. In practice, the word “approximate” is often dropped!</li>
</ul>
<p>There are three main approaches to estimating the statistical uncertainty in an MLE.</p>
<ol type="1">
<li><p>The Fisher information.</p></li>
<li><p>Profile likelihood estimation.</p></li>
<li><p>A simulation study, also known as a bootstrap.</p></li>
</ol>
</section>
<section id="fisher-information" class="level2">
<h2 class="anchored" data-anchor-id="fisher-information">Fisher information</h2>
<ul>
<li>A computationally quick approach when one has access to satisfactory numerical second derivatives of the log-likelihood.</li>
<li>The approximation is satisfactory only when <span class="math inline">\(\hat\theta\)</span> is well approximated by a normal distribution.</li>
<li>Neither of the two requirements above are typically met for POMP models.</li>
<li>A review of standard errors via Fisher information is provided as a <a href="fisherSE.html">supplement</a>.</li>
</ul>
</section>
<section id="profile-likelihood-estimation" class="level2">
<h2 class="anchored" data-anchor-id="profile-likelihood-estimation">Profile likelihood estimation</h2>
<p>This approach is generally preferable to the Fisher information for POMP models.</p>
<p>We will explain this method below and put it into practice in the <a href="https://kingaa.github.io/sbied/mif/">next lesson</a>.</p>
</section>
<section id="the-bootstrap" class="level2">
<h2 class="anchored" data-anchor-id="the-bootstrap">The bootstrap</h2>
<ul>
<li>If done carefully and well, this can be the best approach.</li>
<li>A confidence interval is a claim about reproducibility. You claim, so far as your model is correct, that on 95% of realizations from the model, a 95% confidence interval you have constructed will cover the true value of the parameter.</li>
<li>A simulation study can check this claim fairly directly, but requires the most effort.</li>
<li>The simulation study takes time for you to develop and debug, time for you to explain, and time for the reader to understand and check what you have done. We usually carry out simulation studies to check our main conclusions only.</li>
<li>Further discussion of bootstrap methods for POMP models is provided as a <a href="bootstrap.html">supplement</a>.</li>
</ul>
</section>
<section id="confidence-intervals-via-the-profile-likelihood" class="level2 allowframebreaks">
<h2 class="allowframebreaks anchored" data-anchor-id="confidence-intervals-via-the-profile-likelihood">Confidence intervals via the profile likelihood</h2>
<ul>
<li>Let’s consider the problem of obtaining a confidence interval for the first component of <span class="math inline">\(\theta\)</span>. We’ll write <span class="math display">\[\theta=(\phi,\psi).\]</span></li>
<li>The <strong>profile log-likelihood function</strong> of <span class="math inline">\(\phi\)</span> is defined to be <span class="math display">\[\begin{equation*}
  \profileloglik{{}}(\phi) = \max_{\psi}\loglik(\phi,\psi).
\end{equation*}\]</span> In general, the profile likelihood of one parameter is constructed by maximizing the likelihood function over all other parameters.</li>
<li>Note that, <span class="math inline">\(\max_{\phi}\profileloglik{{}}(\phi) = \max_{\theta}\loglik(\theta)\)</span> and that maximizing the profile likelihood <span class="math inline">\(\profileloglik{{}}(\phi)\)</span> gives the MLE, <span class="math inline">\(\hat{\theta}\)</span>. Why?</li>
<li>An approximate 95% confidence interval for <span class="math inline">\(\phi\)</span> is given by <span class="math display">\[\begin{equation*}
  \big\{\phi : \loglik(\hat\theta) - \profileloglik{{}}(\phi) &lt; 1.92\big\}.
\end{equation*}\]</span></li>
<li>This is known as a profile likelihood confidence interval. The cutoff <span class="math inline">\(1.92\)</span> is derived using <a href="https://en.wikipedia.org/wiki/Likelihood-ratio_test#Distribution:_Wilks.27s_theorem">Wilks’ theorem</a>, which we will discuss in more detail when we develop likelihood ratio tests.</li>
<li>Although the asymptotic justification of Wilks’ theorem is the same limit that justifies the Fisher information standard errors, profile likelihood confidence intervals tend to work better than Fisher information confidence intervals when <span class="math inline">\(N\)</span> is not so large—particularly when the log-likelihood function is not close to quadratic near its maximum.</li>
</ul>
</section>
</section>
<section id="geometry-of-the-likelihood-function" class="level1">
<h1>Geometry of the likelihood function</h1>
<section id="the-likelihood-surface" class="level2">
<h2 class="anchored" data-anchor-id="the-likelihood-surface">The likelihood surface</h2>
<ul>
<li>It is extremely useful to visualize the geometric surface defined by the likelihood function.</li>
<li>If <span class="math inline">\(\Theta\)</span> is two-dimensional, then the surface <span class="math inline">\(\loglik(\theta)\)</span> has features like a landscape.</li>
<li>Local maxima of <span class="math inline">\(\loglik(\theta)\)</span> are peaks.</li>
<li>Local minima are valleys.</li>
<li>Peaks may be separated by a valley or may be joined by a ridge. If you go along the ridge, you may be able to go from one peak to the other without losing much elevation. Narrow ridges can be easy to fall off, and hard to get back on to.</li>
<li>In higher dimensions, one can still think of peaks and valleys and ridges. However, as the dimension increases it quickly becomes hard to imagine the surface.</li>
</ul>
</section>
<section id="exploring-the-likelihood-surface-slices" class="level2">
<h2 class="anchored" data-anchor-id="exploring-the-likelihood-surface-slices">Exploring the likelihood surface: slices</h2>
<ul>
<li><p>To get an idea of what the likelihood surface looks like in the neighborhood of a point in parameter space, we can construct some likelihood <em>slices</em>.</p></li>
<li><p>A likelihood slice is a cross-section through the likelihood surface.</p></li>
<li><p>We’ll make slices for our Consett measles POMP model, in the <span class="math inline">\(\beta\)</span> and <span class="math inline">\(\mu_{IR}\)</span> directions.</p></li>
<li><p>Both slices will pass through our current candidate parameter vector, stored in the <code>pomp</code> model object.</p></li>
</ul>
<p><strong>Questions</strong>:</p>
<ol type="1">
<li><p>What is the difference between a likelihood slice and a profile?</p></li>
<li><p>What is the consequence of this difference for the statistical interpretation of these plots?</p></li>
<li><p>How should you decide whether to compute a profile or a slice?</p></li>
</ol>
<p><a href="./Q_slice.html">Worked solution to the Exercise</a></p>
</section>
<section id="slicing-the-measles-sir-likelihood" class="level2 allowframebreaks">
<h2 class="allowframebreaks anchored" data-anchor-id="slicing-the-measles-sir-likelihood">Slicing the measles SIR likelihood</h2>
<ul>
<li>We first construct a data frame to explore the parameter slice, with <span class="math inline">\(40\times 3+40\times 3=240\)</span> rows:</li>
</ul>
<div class="cell" data-layout-align="center">
<div class="sourceCode cell-code" id="cb10"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb10-1"><a href="#cb10-1" aria-hidden="true" tabindex="-1"></a><span class="fu">slice_design</span>(</span>
<span id="cb10-2"><a href="#cb10-2" aria-hidden="true" tabindex="-1"></a>  <span class="at">center =</span> <span class="fu">coef</span>(measSIR),</span>
<span id="cb10-3"><a href="#cb10-3" aria-hidden="true" tabindex="-1"></a>  <span class="at">Beta =</span> <span class="fu">rep</span>(<span class="fu">seq</span>(<span class="at">from=</span><span class="dv">5</span>,<span class="at">to=</span><span class="dv">30</span>,<span class="at">length=</span><span class="dv">40</span>),<span class="at">each=</span><span class="dv">3</span>),</span>
<span id="cb10-4"><a href="#cb10-4" aria-hidden="true" tabindex="-1"></a>  <span class="at">Gamma =</span> <span class="fu">rep</span>(<span class="fu">seq</span>(<span class="at">from=</span><span class="fl">0.2</span>,<span class="at">to=</span><span class="dv">2</span>,<span class="at">length=</span><span class="dv">40</span>),<span class="at">each=</span><span class="dv">3</span>)</span>
<span id="cb10-5"><a href="#cb10-5" aria-hidden="true" tabindex="-1"></a>) <span class="ot">-&gt;</span> param_slice</span>
<span id="cb10-6"><a href="#cb10-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-7"><a href="#cb10-7" aria-hidden="true" tabindex="-1"></a><span class="fu">dim</span>(param_slice)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[1] 240   7</code></pre>
</div>
</div>
<ul>
<li>We compute the likelihoods <span class="math inline">\(3\)</span> times for each combination (i.e., row) in <code>param_slice</code>:</li>
</ul>
<div class="cell" data-layout-align="center">
<div class="sourceCode cell-code" id="cb12"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb12-1"><a href="#cb12-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(iterators)</span>
<span id="cb12-2"><a href="#cb12-2" aria-hidden="true" tabindex="-1"></a><span class="fu">plan</span>(multisession)</span>
<span id="cb12-3"><a href="#cb12-3" aria-hidden="true" tabindex="-1"></a><span class="fu">foreach</span> (<span class="at">theta=</span><span class="fu">iter</span>(param_slice,<span class="st">"row"</span>), <span class="at">.combine=</span>rbind, </span>
<span id="cb12-4"><a href="#cb12-4" aria-hidden="true" tabindex="-1"></a>  <span class="at">.options.future=</span><span class="fu">list</span>(<span class="at">seed=</span><span class="dv">108028909</span>)) <span class="sc">%dofuture%</span> {</span>
<span id="cb12-5"><a href="#cb12-5" aria-hidden="true" tabindex="-1"></a>  measSIR <span class="sc">|&gt;</span> <span class="fu">pfilter</span>(<span class="at">params=</span>theta,<span class="at">Np=</span><span class="dv">5000</span>) <span class="ot">-&gt;</span> pf</span>
<span id="cb12-6"><a href="#cb12-6" aria-hidden="true" tabindex="-1"></a>  theta<span class="sc">$</span>loglik <span class="ot">&lt;-</span> <span class="fu">logLik</span>(pf)</span>
<span id="cb12-7"><a href="#cb12-7" aria-hidden="true" tabindex="-1"></a>  theta</span>
<span id="cb12-8"><a href="#cb12-8" aria-hidden="true" tabindex="-1"></a>} <span class="ot">-&gt;</span> lik_slice</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell" data-layout-align="center">
<div class="cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="tmp//figure/like-slice-plot-1.png" class="quarto-figure quarto-figure-center figure-img" style="width:5in;height:2in" data-fig-pos="h!"></p>
</figure>
</div>
</div>
</div>
<ul>
<li>Slices offer a very limited perspective on the geometry of the likelihood surface.</li>
<li>When there are only one or two unknown parameters, we can evaluate the likelihood at a grid of points and visualize the surface directly.</li>
</ul>
</section>
<section id="two-dimensional-likelihood-slice" class="level2 allowframebreaks">
<h2 class="allowframebreaks anchored" data-anchor-id="two-dimensional-likelihood-slice">Two-dimensional likelihood slice</h2>
<ul>
<li>We first construct the parameters grid data frame <code>param_grid</code>, with <span class="math inline">\(40\times 3 \times 40\times 3=14,400\)</span> rows:</li>
</ul>
<div class="cell" data-layout-align="center">
<div class="sourceCode cell-code" id="cb13"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb13-1"><a href="#cb13-1" aria-hidden="true" tabindex="-1"></a><span class="fu">expand.grid</span>(</span>
<span id="cb13-2"><a href="#cb13-2" aria-hidden="true" tabindex="-1"></a>  <span class="at">Beta =</span> <span class="fu">rep</span>(<span class="fu">seq</span>(<span class="at">from=</span><span class="dv">10</span>,<span class="at">to=</span><span class="dv">30</span>,<span class="at">length=</span><span class="dv">40</span>), <span class="at">each=</span><span class="dv">3</span>),</span>
<span id="cb13-3"><a href="#cb13-3" aria-hidden="true" tabindex="-1"></a>  <span class="at">Gamma =</span> <span class="fu">rep</span>(<span class="fu">seq</span>(<span class="at">from=</span><span class="fl">0.4</span>,<span class="at">to=</span><span class="fl">1.5</span>,<span class="at">length=</span><span class="dv">40</span>), <span class="at">each=</span><span class="dv">3</span>),</span>
<span id="cb13-4"><a href="#cb13-4" aria-hidden="true" tabindex="-1"></a>  <span class="at">Rho =</span> <span class="fl">0.5</span>, <span class="at">k=</span><span class="dv">10</span>, <span class="at">Eta=</span><span class="fl">0.06</span>, <span class="at">N=</span><span class="dv">38000</span></span>
<span id="cb13-5"><a href="#cb13-5" aria-hidden="true" tabindex="-1"></a>) <span class="ot">-&gt;</span> param_grid</span>
<span id="cb13-6"><a href="#cb13-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-7"><a href="#cb13-7" aria-hidden="true" tabindex="-1"></a><span class="fu">dim</span>(param_grid)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[1] 14400     6</code></pre>
</div>
</div>
<ul>
<li>We then compute likelihoods for each of the combinations:</li>
</ul>
<div class="cell" data-layout-align="center">
<div class="sourceCode cell-code" id="cb15"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb15-1"><a href="#cb15-1" aria-hidden="true" tabindex="-1"></a><span class="fu">plan</span>(multisession)</span>
<span id="cb15-2"><a href="#cb15-2" aria-hidden="true" tabindex="-1"></a><span class="fu">foreach</span> (<span class="at">theta=</span><span class="fu">iter</span>(param_grid,<span class="st">"row"</span>), <span class="at">.combine=</span>rbind,</span>
<span id="cb15-3"><a href="#cb15-3" aria-hidden="true" tabindex="-1"></a>  <span class="at">.options.future=</span><span class="fu">list</span>(<span class="at">seed=</span><span class="dv">421776444</span>)) <span class="sc">%dofuture%</span> {</span>
<span id="cb15-4"><a href="#cb15-4" aria-hidden="true" tabindex="-1"></a>  measSIR <span class="sc">|&gt;</span> <span class="fu">pfilter</span>(<span class="at">params=</span>theta,<span class="at">Np=</span><span class="dv">5000</span>) <span class="ot">-&gt;</span> pf</span>
<span id="cb15-5"><a href="#cb15-5" aria-hidden="true" tabindex="-1"></a>  theta<span class="sc">$</span>loglik <span class="ot">&lt;-</span> <span class="fu">logLik</span>(pf)</span>
<span id="cb15-6"><a href="#cb15-6" aria-hidden="true" tabindex="-1"></a>  theta</span>
<span id="cb15-7"><a href="#cb15-7" aria-hidden="true" tabindex="-1"></a>} <span class="ot">-&gt;</span> lik_grid</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell" data-layout-align="center">
<div class="cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="tmp//figure/pfilter-grid1-plot-1.png" class="quarto-figure quarto-figure-center figure-img" style="width:4.8in;height:3.2in" data-fig-pos="h!"></p>
</figure>
</div>
</div>
</div>
<p>In the above, all points with log-likelihoods less than 25 units below the maximum are shown in grey.</p>
<ul>
<li><p>Notice some features of the log-likelihood surface, and its estimate from the particle filter, that can cause difficulties for numerical methods:</p>
<ol type="1">
<li>The surface is wedge-shaped, so its curvature varies considerably. By contrast, asymptotic theory predicts a parabolic surface that has constant curvature.</li>
<li>Monte Carlo noise in the likelihood evaluation makes it hard to pick out exactly where the likelihood is maximized. Nevertheless, the major features of the likelihood surface are evident despite the noise.</li>
</ol></li>
<li><p>Wedge-shaped relationships between parameters, and nonlinear relationships, are common features of epidemiological dynamic models. We’ll see this in the case studies.</p></li>
</ul>
</section>
</section>
<section id="exercises" class="level1">
<h1>Exercises</h1>
<section id="cost-of-a-particle-filter-calculation" class="level2">
<h2 class="anchored" data-anchor-id="cost-of-a-particle-filter-calculation">Cost of a particle-filter calculation</h2>
<ul>
<li>How much computer processing time does a particle filter take?</li>
<li>How does this scale with the number of particles?</li>
</ul>
<p>Form a conjecture based upon your understanding of the algorithm. Test your conjecture by running a sequence of particle filter operations, with increasing numbers of particles (<code>Np</code>), measuring the time taken for each one using <code>system.time</code>. Plot and interpret your results.</p>
<p><a href="./expense.html">Worked solution to the Exercise</a></p>
</section>
<section id="log-likelihood-estimation" class="level2 allowframebreaks">
<h2 class="allowframebreaks anchored" data-anchor-id="log-likelihood-estimation">log-likelihood estimation</h2>
<p>Here are some desiderata for a Monte Carlo log-likelihood approximation:</p>
<ul>
<li>It should have low Monte Carlo bias and variance.</li>
<li>It should be presented together with estimates of the bias and variance so that we know the extent of Monte Carlo uncertainty in our results.</li>
<li>It should be computed in a length of time appropriate for the circumstances.</li>
</ul>
<p>Set up a likelihood evaluation for the measles model, choosing the numbers of particles and replications so that your evaluation takes approximately one minute on your machine.</p>
<ul>
<li>Provide a Monte Carlo standard error for your estimate.</li>
<li>Comment on the bias of your estimate.</li>
<li>Use doFuture to take advantage of multiple cores on your computer to improve your estimate.</li>
</ul>
<p><a href="./loglikest.html">Worked solution to the Exercise</a></p>
</section>
<section id="exercises-1" class="level2">
<h2 class="anchored" data-anchor-id="exercises-1">Exercises</h2>
<ol type="1">
<li><p><strong>One-dimensional likelihood slice</strong>: Compute several likelihood slices in the <span class="math inline">\(\eta\)</span> direction.</p></li>
<li><p><strong>Two-dimensional likelihood slice</strong>: Compute a slice of the likelihood in the <span class="math inline">\(\beta\)</span>-<span class="math inline">\(\eta\)</span> plane.</p></li>
</ol>
</section>
</section>
<section id="more-on-likelihood-based-inference" class="level1">
<h1>More on likelihood-based inference</h1>
</section>
<section id="maximizing-the-likelihood" class="level1">
<h1>Maximizing the likelihood</h1>
<section id="maximizing-the-particle-filter-likelihood" class="level2">
<h2 class="anchored" data-anchor-id="maximizing-the-particle-filter-likelihood">Maximizing the particle filter likelihood</h2>
<ul>
<li>Likelihood maximization is key to profile intervals, likelihood ratio tests and AIC as well as the computation of the MLE.</li>
<li>An initial approach to likelihood maximization might be to stick the particle filter log-likelihood estimate into a standard numerical optimizer, such as the Nelder-Mead algorithm.</li>
<li>In practice this approach is unsatisfactory on all but the smallest POMP models. Standard numerical optimizers are not designed to maximize noisy and computationally expensive Monte Carlo functions.</li>
<li>Further investigation into this approach is available as a <a href="pf-in-Nelder-Mead.html">supplement</a>.</li>
<li>We’ll present an <em>iterated filtering algorithm</em> for maximizing the likelihood in a way that takes advantage of the structure of POMP models and the particle filter.</li>
<li>First, let’s think a bit about some practical considerations in interpreting the MLE for a POMP.</li>
</ul>
</section>
<section id="likelihood-based-model-selection-and-model-diagnostics" class="level2">
<h2 class="anchored" data-anchor-id="likelihood-based-model-selection-and-model-diagnostics">Likelihood-based model selection and model diagnostics</h2>
<ul>
<li>For nested hypotheses, we can carry out model selection by likelihood ratio tests.</li>
<li>For non-nested hypotheses, likelihoods can be compared using Akaike’s information criterion (AIC) or related methods.</li>
</ul>
</section>
</section>
<section id="likelihood-ratio-test" class="level1">
<h1>Likelihood ratio test</h1>
<section id="likelihood-ratio-tests-for-nested-hypotheses" class="level2 allowframebreaks">
<h2 class="allowframebreaks anchored" data-anchor-id="likelihood-ratio-tests-for-nested-hypotheses">Likelihood ratio tests for nested hypotheses</h2>
<ul>
<li>The whole parameter space on which the model is defined is <span class="math inline">\(\Theta\subset\R^D\)</span>.</li>
<li>Suppose we have two <strong>nested</strong> hypotheses <span class="math display">\[\begin{equation*}
  \begin{aligned}
    H^{\langle 0\rangle} &amp;: \theta\in \Theta^{\langle 0\rangle}, \\
    H^{\langle 1\rangle} &amp;: \theta\in \Theta^{\langle 1\rangle},
  \end{aligned}
\end{equation*}\]</span> defined via two nested parameter subspaces, <span class="math inline">\(\Theta^{\langle 0\rangle}\subset \Theta^{\langle 1\rangle}\)</span>, with respective dimensions <span class="math inline">\(D^{\langle 0\rangle}&lt; D^{\langle 1\rangle}\le D\)</span>.</li>
<li>We consider the log-likelihood maximized over each of the hypotheses, <span class="math display">\[
  \begin{aligned}
    \ell^{\langle 0\rangle} &amp;= \sup_{\theta\in \Theta^{\langle 0\rangle}} \ell(\theta), \\
    \ell^{\langle 1\rangle} &amp;= \sup_{\theta\in \Theta^{\langle 1\rangle}} \ell(\theta).
  \end{aligned}
\]</span></li>
<li><strong>Wilks approximation</strong>: under the hypothesis <span class="math inline">\(H^{\langle 0\rangle}\)</span>, <span class="math display">\[
  \ell^{\langle 1\rangle} - \ell^{\langle 0\rangle} \approx \tfrac{1}{2}\,\chi^2_{D^{\langle 1\rangle}- D^{\langle 0\rangle}},
\]</span> where <span class="math inline">\(\chi^2_d\)</span> is a chi-squared random variable on <span class="math inline">\(d\)</span> degrees of freedom and <span class="math inline">\(\approx\)</span> means “is approximately distributed as”.</li>
<li>The Wilks approximation can be used to construct a hypothesis test of the null hypothesis <span class="math inline">\(H^{\langle 0\rangle}\)</span> against the alternative <span class="math inline">\(H^{\langle 1\rangle}\)</span>.</li>
<li>This is called a <strong>likelihood ratio test</strong> since a difference of log-likelihoods corresponds to a ratio of likelihoods.</li>
<li>When the data are IID, <span class="math inline">\(N\to\infty\)</span>, and the hypotheses satisfy suitable regularity conditions, this approximation can be derived mathematically and is known as <strong>Wilks’ theorem</strong>.</li>
<li>The chi-squared approximation to the likelihood ratio statistic may be useful, and can be assessed empirically by a simulation study, even in situations that do not formally satisfy any known theorem.</li>
</ul>
</section>
<section id="wilks-theorem-and-profile-likelihood" class="level2 allowframebreaks">
<h2 class="allowframebreaks anchored" data-anchor-id="wilks-theorem-and-profile-likelihood">Wilks’ theorem and profile likelihood</h2>
<ul>
<li>Suppose we have an MLE, written <span class="math inline">\(\hat\theta=(\hat\phi,\hat\psi)\)</span>, and a profile log-likelihood for <span class="math inline">\(\phi\)</span>, given by <span class="math inline">\(\profileloglik{{}}(\phi)\)</span>.</li>
<li>Consider the likelihood ratio test for the nested hypotheses <span class="math display">\[\begin{equation*}
  \begin{aligned}
    H^{\langle 0\rangle} &amp;: \phi = \phi_0, \\
    H^{\langle 1\rangle} &amp;: \text{$\phi$ unconstrained}.
  \end{aligned}
\end{equation*}\]</span></li>
<li>We can compute the 95%-ile for a chi-squared distribution with one degree of freedom: <code>qchisq(0.95,df=1)</code><span class="math inline">\(=3.841\)</span>.</li>
<li>Wilks’ theorem then gives us a hypothesis test with approximate size <span class="math inline">\(5\%\)</span> that rejects <span class="math inline">\(H^{\langle 0\rangle}\)</span> if <span class="math inline">\(\profileloglik{{}}(\hat\phi)-\profileloglik{{}}(\phi_0)&lt;3.84/2\)</span>.</li>
<li>It follows that, with probability <span class="math inline">\(95\%\)</span>, the true value of <span class="math inline">\(\phi\)</span> falls in the set <span class="math display">\[\begin{equation*}
  \big\{\phi: \profileloglik{{}}(\hat\phi)-\profileloglik{{}}(\phi)&lt;1.92\big\}.
\end{equation*}\]</span> So, we have constructed a profile likelihood confidence interval, consisting of the set of points on the profile likelihood within <span class="math inline">\(1.92\)</span> log units of the maximum.</li>
<li>This is an example of <a href="https://www.stat.nus.edu.sg/~wloh/lecture17.pdf">a general duality between confidence intervals and hypothesis tests</a>.</li>
</ul>
</section>
</section>
<section id="information-criteria" class="level1">
<h1>Information criteria</h1>
<section id="akaikes-information-criterion-aic" class="level2 allowframebreaks">
<h2 class="allowframebreaks anchored" data-anchor-id="akaikes-information-criterion-aic">Akaike’s information criterion (AIC)</h2>
<ul>
<li>Likelihood ratio tests provide an approach to model selection for nested hypotheses, but what do we do when models are not nested?</li>
<li>A more general approach is to compare likelihoods of different models by penalizing the likelihood of each model by a measure of its complexity.</li>
<li>Akaike’s information criterion <strong>AIC</strong> is given by <span class="math display">\[\begin{equation*}
  \mathrm{AIC} = -2\,\loglik(\hat{\theta}) + 2\,D
\end{equation*}\]</span> “Minus twice the maximized log-likelihood plus twice the number of parameters.”</li>
<li>We are invited to select the model with the lowest AIC score.</li>
<li>AIC was derived as an approach to minimizing prediction error. Increasing the number of parameters leads to additional <strong>overfitting</strong> which can decrease predictive skill of the fitted model.</li>
<li>Viewed as a hypothesis test, AIC may have weak statistical properties. It can be a mistake to interpret AIC by making a claim that the favored model has been shown to provide a superior explanation of the data. However, viewed as a way to select a model with reasonable predictive skill from a range of possibilities, it is often useful.</li>
<li>AIC does not penalize model complexity beyond the consequence of reduced predictive skill due to overfitting. One can penalize complexity by incorporating a more severe penalty than the <span class="math inline">\(2D\)</span> term above, such as via <a href="https://en.wikipedia.org/wiki/Bayesian_information_criterion">BIC</a>.</li>
<li>A practical approach is to use AIC, while taking care to view it as a procedure to select a reasonable predictive model and not as a formal hypothesis test.</li>
</ul>
</section>
</section>
<section id="references" class="level1">
<h1>References</h1>
<section id="references-1" class="level2">
<h2 class="anchored" data-anchor-id="references-1">References</h2>
<div id="refs" class="references csl-bib-body hanging-indent" data-entry-spacing="0" role="list">
<div id="ref-Arulampalam2002" class="csl-entry" role="listitem">
Arulampalam, M. S., S. Maskell, N. Gordon, and T. Clapp. 2002. <span>“A Tutorial on Particle Filters for Online Nonlinear, Non-<span>Gaussian</span> <span>Bayesian</span> Tracking.”</span> <em>IEEE Trans Signal Process</em> 50: 174–88. <a href="https://doi.org/10.1109/78.978374">https://doi.org/10.1109/78.978374</a>.
</div>
<div id="ref-Doucet2001" class="csl-entry" role="listitem">
Doucet, Arnaud, Nando de Freitas, and Neil Gordon, eds. 2001. <em>Sequential <span>Monte</span> <span>Carlo</span> Methods in Practice</em>. New York: Springer-Verlag.
</div>
<div id="ref-Fisher1922" class="csl-entry" role="listitem">
Fisher, R. A. 1922. <span>“On the Mathematical Foundations of Theoretical Statistics.”</span> <em>Philos Trans R Soc London A</em> 222: 309–68. <a href="https://doi.org/10.1098/rsta.1922.0009">https://doi.org/10.1098/rsta.1922.0009</a>.
</div>
<div id="ref-King2016" class="csl-entry" role="listitem">
King, Aaron A., Dao Nguyen, and Edward L. Ionides. 2016. <span>“Statistical Inference for Partially Observed <span>Markov</span> Processes via the <span>R</span> Package Pomp.”</span> <em>J Stat Softw</em> 69 (12): 1–43. <a href="https://doi.org/10.18637/jss.v069.i12">https://doi.org/10.18637/jss.v069.i12</a>.
</div>
<div id="ref-Kitagawa1987" class="csl-entry" role="listitem">
Kitagawa, Genshiro. 1987. <span>“Non-<span>Gauss</span>ian State-Space Modeling of Nonstationary Time Series.”</span> <em>J Am Stat Assoc</em> 82 (400): 1032–41. <a href="https://doi.org/10.1080/01621459.1987.10478534">https://doi.org/10.1080/01621459.1987.10478534</a>.
</div>
<div id="ref-Pawitan2001" class="csl-entry" role="listitem">
Pawitan, Yudi. 2001. <em><span>I</span>n <span>A</span>ll <span>L</span>ikelihood: <span>S</span>tatistical <span>M</span>odelling and <span>I</span>nference <span>U</span>sing <span>L</span>ikelihood</em>. Oxford: Clarendon Press.
</div>
</div>
</section>
<section id="license-acknowledgments-and-links" class="level2">
<h2 class="anchored" data-anchor-id="license-acknowledgments-and-links">License, acknowledgments, and links</h2>
<ul>
<li><p>This lesson is prepared for the <a href="https://rubbislam.quarto.pub/episim/">Simulation-based Inference for Epidemiological Dynamics</a> module at the Summer Institute in Statistics and Modeling in Infectious Diseases, <a href="https://sph.emory.edu/SISMID/index.html">SISMID</a>.</p></li>
<li><p>The materials build on <a href="../acknowledge.html">previous versions of this course and related courses</a>.</p></li>
<li><p>Licensed under the <a href="https://creativecommons.org/licenses/by-nc/4.0/">Creative Commons Attribution-NonCommercial license</a>. Please share and remix non-commercially, mentioning its origin. </p></li>
<li><p>Produced with R version 4.4.0 and pomp version 5.9.</p></li>
<li><p>Compiled on 2024-07-24.</p></li>
</ul>
<p><a href="index.html">Back to Lesson</a></p>
<p><a href="./main.R"><code>R</code> code for this lesson</a></p>


</section>
</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const onCopySuccess = function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  }
  const getTextToCopy = function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
    text: getTextToCopy
  });
  clipboard.on('success', onCopySuccess);
  if (window.document.getElementById('quarto-embedded-source-code-modal')) {
    // For code content inside modals, clipBoardJS needs to be initialized with a container option
    // TODO: Check when it could be a function (https://github.com/zenorocha/clipboard.js/issues/860)
    const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
      text: getTextToCopy,
      container: window.document.getElementById('quarto-embedded-source-code-modal')
    });
    clipboardModal.on('success', onCopySuccess);
  }
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp('/' + window.location.host + '/');
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      if (note) {
        return note.innerHTML;
      } else {
        return "";
      }
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      // TODO in 1.5, we should make sure this works without a callout special case
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
</div> <!-- /content -->




</body></html>